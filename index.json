[{"content":"简单来说，本文会讲清楚我是如何使用DDD方法来进行供应链服务拆分的。\n前言 本文尝试归纳整理作者前几个月负责的供应链项目中使用到的DDD战略设计方法，大概包含了以下部分：\n理解问题 明确业务价值 分析业务需求 建立业务概念模型 识别限界上下文 业务边界 应用边界 领域建模 领域分析建模 领域设计建模 模型实现 微服务架构 应用集成、交互 其中架构层面，主要做了微服务拆分。\n开始前简单对齐一下领域驱动设计的基本信息。\n领域驱动设计简述 领域驱动设计，即DDD（Domain Driven Design）是一种软件设计方法，适用于复杂的业务建模场景。\n简单概括DDD：通过领域建模的方式来落地软件设计过程以及结果。\n那么如何进行领域建模？\n我们可以拆解为两部分：\n战略设计 划分限界上下文：根据业务功能和职责，将整个业务系统划分为不同的限界上下文，明确每个限界上下文的边界和职责。 确定上下文映射关系：分析不同限界上下文之间的关系，如共享内核、客户 - 供应商、遵奉者等关系，定义它们之间的交互方式和协作机制。 战术设计 识别领域对象：从业务需求和流程中识别出实体、值对象、聚合等领域对象，并确定它们的属性和行为。 设计领域模型：根据领域对象之间的关系，构建领域模型，包括实体之间的关联、聚合的组成、领域服务的定义等。 实现领域逻辑：通过编写代码来实现领域模型中的业务逻辑，包括实体的状态变化、领域服务的具体操作等，同时使用仓储来处理数据的持久化和查询。 本文我们聚焦于理解问题、识别限界上下文这两步。\n工具、方法论只是辅助工程师去解决问题、输出方案，核心、本质还是要理解业务。所以我们先看看业务背景。\n业务背景 我们团队核心能力：\n为上层业务提供智能设备实物； 设备管理能力（B端以及C端使用设备的能力）； 售后交付服务； 24年之前，设备直接从供应商采购、交付，业务最简闭环流程： 24年开始，公司决定自研自营设备，业务最简闭环流程： 从图中可以看到，初期使用供应商现成的服务，生产制造、仓储对于平台是透明的。\n业务开始自营后，供应链的预算、合同、采购、交付、仓储这些环节都需要自己负责，任何环节纯人工，成本巨大且低效。\n同时，初期平台针对业务方侧已经建设了完善的销售、售后能力，24年亟需补充大后方的供应链能力。\n从采买供应商服务到自营设备，团队职责与视角发生了巨大改变。\n视角转变 职责上：增加了供应链的预算计划、生产采购、交付发货、仓储，每个环节都有巨大的效率提升空间。 视角上：自营供应链，需要自负盈亏、关注经营效率、品质。\n业务上职责、视角变了，对应到系统上，关注的重点自然也就变了，规划与动作自然也得变。\n职责变化 经过上面的梳理、分析，我们看看供应链ERP系统核心职责：\n进（贝壳平台从供应商侧进货） 销（贝壳平台向上层业务方售卖设备、服务） 存（贝壳平台管理自研自营设备库存）（库存管理、供应链） ERP业务核心是为贝壳IoT自研自营智能锁服务，将整个生产、进货、销售、售后、仓储流程数字化。 在之前采买供应商设备与服务的模式下，这些环节都是不需要我们负责的。\n核心目标 经过上面的步步分析，我们明确一下供应链ERP的核心目标：\n经营提效； 用更少的人、成本向上层提供服务 财务数字化： 快速算清账 系统化预算 系统化操盘 数字化沉淀业务规则； 流程运转 风控 ERP系统，核心是经营。领域名词都可以增加一个「经营」作为定语。 由于ERP业务处于客户视角的大后方，因此也可以增加「供应链」作为定语。\n至此，我相信读者对业务背景心中有数了。\n上面花了大量篇幅来说明业务背景，侧面说明业务背景其实是非常重要的。\n磨刀不误砍柴工。\n落地领域驱动战略设计 DDD的概念、学习成本、心智负担可以很大，也可以简化。我尝试简化了一版，对于战略设计，其实只需要三步：\n建立统一语言； 分解问题域； 关联业务与系统； 落地的完整全景： 如何上手？我们继续逐步拆解。\n理解业务问题 想要输出方案，首先第一步需要理解需求、问题。\n我一般通过沟通的方式，从业务侧获取业务知识、信息。 沟通的时候，抓住几个关键要素即可：\n人 事 资产 背后的本质就是：搞清楚谁负责什么事，什么是业务的核心指标。\n这一步，可以使用用户故事、用例分析、四色建模等方式操作，也可以使用一些通用的战略分析的工具，比如5W1H、PEST、SWOT、价值链分析。\n使用什么工具、方法我认为都是次要的，核心是要找对人找对信息的口子。\n这点可以参考：迅速熟悉一摊子事：顺藤摸瓜\n用例分析 经过与业务同学的沟通，我根据获得的信息，梳理了供应链运作的核心流程，如下： 梳理核心概念模型 根据上述的核心用例流程，其实已经可以梳理出核心的概念模型、领域实体：\n划分上下文 有了概念模型，我们就可以划分领域、梳理领域间的协作关系，进一步产出限界上下文，同时根据语义相关性、功能相关性将关联性较强的归到同一个上下文。\n统一语言 关于这里的核心域、支持域，我个人认为不是绝对的，比如下方的客户域、供应商域，在不同阶段不同角色来看，重要性是会变化的。\n因此这种划分为核心域是合理的（没有这块业务无法运转），划分为支撑域也是合理的（相比交易结算来说，对业务的成败影响没那么大）。\n根据子域初步划分限界上下文 核心的上下文就这几个： 梳理上下文协作关系 同样使用用例分析的方法，可以梳理评估一下上下文间的协作关系，进而产出中间层的上下文。\n上下文本质表达的是业务的范围边界，同时也能作为技术上工程服务划分的重要依据。当中间层的职责变大、业务变重之后，就可以考虑将中间层上下文独立出去。\n目前我们的供应链处于起步阶段，中间层业务并不复杂庞大，因此可以将中间层的业务维护在主上下文中。\n明确核心领域模型 经过上面的逐步分析，终于可以把核心的领域模型给敲定了：\n确定工程架构 经过上面的一系列过程，我们明确了核心的领域模型，并且划分了限界上下文。\n到此，我们可以根据领域模型、上下文创建对应的代码工程了，结构如下：\n这里部署只申请了两个工程资源，原因是目前还不清楚后续业务体量、复杂度会变得多大，因此暂时使用两个工程+子模块的方式进行代码工程架构拆分。同工程内的子模块间使用RPC子模块进行防腐交互，也为后续拆分预留了口子。\n总结 本文主要使用文字+图例的方式介绍了我24年投入较多精力的供应链项目，包含了整体设计以及使用DDD 划分服务边界拆分服务的部分。\n文中尽量屏蔽DDD 的复杂性，尽量让读者专注于业务的理解、整体的建模、拆分思路。\n最后小结一下，使用DDD 方法论建模、拆分服务，我们经历了以下核心过程：\n理解业务问题 划分上下文 明确核心领域模型 最后基于模型与上下文定义工程架构 其中最为重要的，我个人认为是要对业务背景理解透彻，要抓住一切能用的方式接收足够的业务信息。 其次才是使用工具、方法去解决问题。\n以上，欢迎讨论。\n","permalink":"https://redolog.github.io/posts/rd/ddd/strategy-design/supply-chain-case/","summary":"\u003cp\u003e简单来说，本文会讲清楚我是如何使用DDD方法来进行供应链服务拆分的。\u003c/p\u003e","title":"DDD战略设计之供应链服务拆分"},{"content":"本文是我摸索实践了几年的眼部护理最佳实践，涵盖了用药、辅疗、休息、日照、运动等方面，具体内容请直接看表。\n","permalink":"https://redolog.github.io/posts/note/eye-care-best-practice/","summary":"\u003cp\u003e本文是我摸索实践了几年的眼部护理最佳实践，涵盖了用药、辅疗、休息、日照、运动等方面，具体内容请直接看表。\u003c/p\u003e","title":"眼部护理最佳实践"},{"content":"缓存高并发场景下，常见的问题包括不限于：\n缓存击穿； 缓存穿透； 缓存雪崩； 本文，我们一起从概念以及场景入手，理解这类问题，并且提供可行的解决方案。\n缓存击穿 缓存穿透 缓存雪崩 小结 缓存是C端大流量场景必用的优化手段，针对高并发场景，常见问题无非就是上述三类：\n缓存击穿； 缓存穿透； 缓存雪崩； 本文我们通过图例，辅助理解了每一类问题的背景、原理，并且简要输出了解决方案。希望对读者有所帮助。\n以上。\n","permalink":"https://redolog.github.io/posts/rd/design/cache-concurrency/","summary":"\u003cp\u003e缓存高并发场景下，常见的问题包括不限于：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e缓存击穿；\u003c/li\u003e\n\u003cli\u003e缓存穿透；\u003c/li\u003e\n\u003cli\u003e缓存雪崩；\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e本文，我们一起从概念以及场景入手，理解这类问题，并且提供可行的解决方案。\u003c/p\u003e","title":"缓存高并发常见问题及方案"},{"content":"我们从顶层视角逐步拆解一下RocketMQ的架构组件设计。\nRocketMQ是当下工业界使用最广泛、可靠性最强、性能较高的一款分布式消息队列，其架构设计、源码有很多值得我们学习的地方，本文，我们先来了解一下它顶层的组件。\n最基础的组件 最主流的MQ像Kafka/RocketMQ都采用了这种基础设计，即：\nProducer负责生产消息； Broker 负责存储消息； Consumer 负责消费消息； Broker集群化 Broker作为MQ的核心角色，其肩负了一款分布式存储的「高性能」「高可用」等核心指标，对于上图中的单点，很明显存在单点故障的瓶颈。\n因此，RocketMQ通过集群化Broker的方案来解决服务端的单点问题。\n通过Topic组织消息分布 Broker核心负责存储消息、服务端高可用。对于消息的组织、分布，Kafka/RocketMQ都通过Topic来管理。\n即一条消息属于谁，一定能找到对应的一个Topic。\n而一个Broker内，存在许多的Topic。\n通过MessageQueue提高Topic扩展性 如果一个Topic负责所有的消息生产、存储、消费，很容易产生瓶颈：\n首先是生产与消费完全是单通道的； 其次是对于多个Topic在Broker上的分布是不均衡的，即多个Topic存储会存在数据倾斜「data skew」的问题； 对于上述问题，Kafka/RocketMQ都使用了同样的方案，即引入一个中间层进行解耦、扩展。\nDavid Wheeler 金句：「计算机科学领域的任何问题，都可以增加一个间接的中间层来解决。」\nKafka中的这一层叫做Partition分区，RocketMQ中的这一层叫做MessageQueue队列。分区更具有通用性，因此图例中我使用Partition，读者理解这里的含义即可。\n通过增加中间层，在进行消息的管理时，就可以进行多个分区间的负载均衡、存储均衡。\n通过主从提高Broker可用性 上面的架构，依然存在会丢消息的问题，即使引入了分区，单个Broker挂掉的时候，多个Topic的同一分区数据都会丢失（如果强制持久化就不会丢失）。\n请记住，数据不丢失的可用性问题一般解决的方案就两个：\n数据持久化； 数据冗余+复制； 这里我们着重想强调的是数据复制，即RocketMQ低版本中使用的方案是对Broker进行主从复制： 通过Dleger Raft架构提高Broker可用性 主从架构其实足够简洁，并且在大多数情况下，其可用性是非常高的，并且成本非常可控。\n但是公司大了之后，集群资源过多，并且业务场景也更加复杂。这时候主从架构有一个缺点，就是主节点遇到故障，Broker集群无法自动故障切换。\n这时候就轮到分布式共识协议登场了，在RocketMQ中使用了名叫Dleger的架构，其本质上是实现了一套Raft共识协议。\n大部分大型分布式存储集群，都是通过 Raft 这种协议来实现集群自治的。比如我多年前学习的 ElasticSearch 也是使用了同样的方案。\n这种共识协议定义了集群中节点的角色，更重要的是，其定义了主节点挂掉后自动选举新主节点的机制。\n具体的机制流程本文不做赘述。\nDleger架构如下图，核心优势是相较于主从复制架构增加了集群自选主自治，故障切换可以做到自动化（大大减少我们SRE on-call的工作量）。\n通过注册中心管理Broker集群 前面我们通过Broker集群解决了单点问题，同时增加了主从复制来提高数据的可用性。\n但是我们生产消息与消费消息的时候，程序与哪个Broker交互呢？\n答案是引入一个注册中心，来协调分布式的Broker集群。 注册中心维护了整个RocketMQ集群的核心元数据：\n集群的Topic列表； Topic的各分区（MessageQueue or Partition）与各Broker节点的关系； Broker节点的存活状态； 当业务服务端与RocketMQ交互时，仅需关心Producer、Consumer API即可。\nNameServer集群化 上面的NameServer同样存在单点问题，无他，还是通过集群的方式来提高可用性。\n小结 以上，我们渐进式了解了RocketMQ的核心组件设计，读完本文，对这些组件、解决方案有个大致印象即可。\n理解每个组件的设计用意才是关键。\n","permalink":"https://redolog.github.io/posts/rd/design/rocketmq/architecture/","summary":"\u003cp\u003e我们从顶层视角逐步拆解一下RocketMQ的架构组件设计。\u003c/p\u003e","title":"理解RocketMQ架构"},{"content":"今年进入新的方向后，有幸负责了一部分团队的管理，借以本文记录我的一些认知与实践。\n一、背景 此前在主R项目进度时，对于团队管理的思考尚不成体系。作为项目主负责人，只需做好整体技术方案设计、合理分配任务、紧密跟进进度以及处理临时问题，精力主要集中于确保事情顺利完成。\n而在人员管理方面，一方面缺乏足够的授权，另一方面也缺乏关注人员相关问题的意识与能力，自然也就无法有效解决相应问题。\n今年，我负责管理团队小方向的两位研发同事，有了相对完整的思考与实践。我认为这些方法适用于 2 - 7 人规模的团队管理。\n本文会持续迭代。\n二、基础认知 个人对于管理的认知：管理是一种工种，本质是要为团队产出、效能负责，要有为团队负责、服务的心态，一切以目标、解决问题为导向。\n三、实践方法 对于2-7人的技术团队管理，可以有以下实践方法： 1.信息输出/流转 拉齐团队、项目目标； 明确团队分工； 拉齐协作、沟通方式（及时汇报进度与风险）； 定时与各节点沟通，尽可能抹平信息差，及时给予以及获取反馈； 2.任务分工 洞察、明确各人的能力边界，根据能力模型分配任务； 3.团队成长辅导 如果通过TL的作用，一线同学有真的成长（能力、收入、认知等），团队向心力势必提升（士气提升），进一步对团队产生积极作用。\n识别并指出问题，提供解法（方法、工具）； 高P要能解决技术、沟通等核心高难度问题； 洞察人的问题，通过一些管理工具、方法解决人的问题（1v1沟通、绩效、成长计划、团建）； 4.机制\u0026amp;文化 针对工作流建立SOP，通过标准化提升团队效能； 明确团队文化，明确鼓励与抵制的品质； 结合绩效、奖金等管理工具，实体化上述文化倡导与抵制动作，通过机制将文化落地，进一步帮助团队成长； 5.精力分配 时间精力从一线的开发任务中释放出来，回归到上述管理重点； 梳理高优事项，为团队争取高优项目，打回低优先级低质量事项； 推动团队不断迭代、不断变强； 四、踩坑 从一线研发转变为虚线的管理，大部分人都会踩坑，下面是我踩的一些关键坑。\n1.精力分配问题 初期我还是没有接受自己的身份转变，同时我对自己的技术设计以及进度把控更加自信，所以我会给自己分配比较多的开发任务。\n在某些时候，这不是个问题，但是一旦碰上有其他机动的事需要处理，我就忙不过来了，这时候，我就成为了团队的瓶颈。\n在后来的反思中，我想到了一点，这种任务分配的方案，一定程度上也会损害团队内成员的积极性。\n他们的内心OS可能是：活都你干了，那我就顺势划水摸鱼了。\n因此在经历了几次这样的「紧张」时刻之后，我调整了精力分配的方案：只给自己分配较少的任务、通用框架的任务。如果有一件事只有我能解决，这种任务也分配给我。\n如果资源不够，考虑其他方案补充人力资源。\n分配、调度好自己的精力非常重要，分配不好，相当于管理者在团队中消失了，长此以往，团队将走向混沌。\n2.冲突处理不当问题 这里的冲突并不是Git分支的冲突，而是沟通中出现了意见分歧。冲突处理不当，会非常影响团队氛围、士气。\n工作中对于方案大家有不同的想法非常正常，每种方案都有各自的优点。有那么一次，我跟团队内同学就聊岔了，没有达成一致。\n后来我找这位同学1v1沟通了一次，本着坦诚清晰的原则，就事论事。表明我只是想帮助他解决问题，没有指责的意思。过程中我们也就近期碰到的问题进行了探讨，最后我也提了一些要求，给了一些成长的建议。\n第一次发生冲突的时候，其实当下没有处理好。第二次单独沟通，才化解了一些潜在的矛盾。\n在这次经历后，我也意识到了「坦诚沟通」的重要性，坦诚沟通可以帮助我发现人相关的问题，进而帮助团队解决这类问题。\n做管理后，能够优雅地解决冲突是非常重要的能力，有两个点：\n鼓励团队发生良性冲突，对事情有不同看法、见解，提升团队活性； 杜绝、减少恶性冲突，对人不要有恶意，对人不要武断评价； 一个可能比较片面的观察，挺多人其实比较缺乏冲突处理的能力，这个局限性来自于从小的家庭教育、交友经历；\n3.只关注事不关注人 不管做实线的TL还是虚线的小组长，跟做项目的owner的一大区别，我认为就体现在「是否关注人」上。\n每个人都不是完美的，我们也不追求完美，但是我们追求进步。\n想要追求进步，管理者就需要对团队内的成员成长负责。\n想让大家成长，第一点就需要「关注人」。\n而这，也是我之前犯的一个错误。\n很幸运的是，TL在会议上指出了我的这个问题，我及时做了反思与调整。\n现在，我除了做好大家的任务分工，也会关注大家的需求、状态。\n4.只关注内部少关注外部 做了管理之后，职责变大了，负责的团队盘子变大了。\n这种情况下，就不能只关注当下。换句话说，不能只关注自己，需要关注如何让自己的团队做得更好 + 有更大收获。\n仅仅关注团队内部，做到最好，也只是普普通通，说直白点，绩效最多是350。\n如何做到更好是一个系统性复杂的问题，我们这里只谈一点，就是管理者需要走出去。\n走出去获得关键信息，信息可能来自组织，可能来自其他团队，可能来自其他职能的同事。\n有了关键信息，可以帮助我们更好地决策，避免团队落后。\n什么是落后？\n在我看来，就是避免团队横向对标起来进入「看不懂、学不会、追不上」的困境。\n这部分，也是离开工位之后的工作之一。\n五、总结 总结一下，团队管理对于一线研发来说，最大的挑战就是思维需要转变。思维转变之后，也具备清晰的认知和有效的实践方法。\n本文概述了我的一些实践，主要体现在这些方面：\n信息流转 任务分工 团队成长辅导 机制\u0026amp;文化 精力分配 通过多方面较为完整的方法实践，目前能够解决我实际遇到的大部分问题。\n在未来的工作中，我还需要不断探索和完善这些方法。\n以上，共勉。\n","permalink":"https://redolog.github.io/posts/methodology/work/how-to-become-manager/","summary":"\u003cp\u003e今年进入新的方向后，有幸负责了一部分团队的管理，借以本文记录我的一些认知与实践。\u003c/p\u003e","title":"我的管理工作实践"},{"content":"踩坑一则复杂对象（领域实体）序列化失败问题。\n归因：测试环境Redis连接挂掉，同时对DDD领域实体序列化，JSON.toJSON() 封装了异常，导致关键异常栈丢失，后经过正向逆向分析找到了问题原因与解法。\n背景 11.14号，QA给我反馈了一则问题：\n测试环境，添加指纹密码时，会报tojson error： 分析 根据trace日志，可以直接定位到报错的源码位置：\n1 com/company/domain/lock/polymorphism/encoder/impl/SampleEncoder.java:294 其中报错代码示例，password是这个服务的一个核心领域实体：\n1 log.info(JSON.toJSON(password)); 但是这里的异常栈并没有把最原始的异常打印出来，原因是JSON.toJSON中对异常进行了包装：\n现在可以确定的一点是，在 SampleEncoder 中，对领域实体 Password 使用fastjson库序列化为字符串时，发生了报错。\n我们顺着 Password entity 的结构分析一下： Password领域实体内持有Lock领域实体，而Lock领域实体内持有HardwareInfo领域实体。\n在 getHardwareInfo() 方法中，存在查库查缓存行为。\n当序列化框架实现序列化方法 toJSON 时，会通过反射调用JavaBean中的属性方法，即各个get方法。\n凑巧的是，当时我们的测试环境redis连接挂了，导致调用 getHardwareInfo() 直接抛错。\n而在 JSON.toJSON 中，对异常进行了包装，丢失了一部分异常栈，因此在日志中看不到这块信息。\n我是怎么发现redis连接挂掉了呢？\n是因为QA同学反馈了其他的问题，其他的操作也无法操作redis，在这些地方异常栈被打印了出来。 两类问题结合分析，得出了上面的正向、反向的通路。\n而在线上环境我们可以直接通过监控报警观测到这种异常，不用兜这么大圈子。\n小结 问题根因：测试环境的redis连接挂了，导致领域对象的get方法抛错，导致toJSON直接失败。\n解决：联系DBA修复redis。\n教训：在使用了DDD架构的代码上，其实不建议对领域对象进行序列化操作，这种操作，无异于MVC架构中，对service进行序列化操作。做类似操作前，应该仔细评估+做好提前设计。\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/serialization/ddd-entity-redis-down/","summary":"\u003cp\u003e踩坑一则复杂对象（领域实体）序列化失败问题。\u003c/p\u003e\n\u003cp\u003e归因：测试环境\u003ccode\u003eRedis\u003c/code\u003e连接挂掉，同时对\u003ccode\u003eDDD\u003c/code\u003e领域实体序列化，\u003ccode\u003eJSON.toJSON()\u003c/code\u003e 封装了异常，导致关键异常栈丢失，后经过正向逆向分析找到了问题原因与解法。\u003c/p\u003e","title":"复杂对象序列化失败问题一例"},{"content":"LC 501. 二叉搜索树中的众数，在计数解法的基础上，利用二叉搜索树中序遍历有序的特性优化空间复杂度。\n题目 501. 二叉搜索树中的众数 https://leetcode.cn/problems/find-mode-in-binary-search-tree/description/\n给你一个含重复值的二叉搜索树（BST）的根节点 root ，找出并返回 BST 中的所有 众数（即，出现频率最高的元素）。\n如果树中有不止一个众数，可以按 任意顺序 返回。\n假定 BST 满足如下定义：\n结点左子树中所含节点的值 小于等于 当前节点的值 结点右子树中所含节点的值 大于等于 当前节点的值 左子树和右子树都是二叉搜索树\nMap计数版本 思路 看到这类找众数的题目，大脑里蹦出的第一个思路就是直接使用map计数，针对最后计数等于最大计数的元素，收集到一个答案数组即可。\n由于这个题目求的可能有多个众数，也无法用「摩尔投票」优化。\n思路非常直给，下面展示这部分代码：\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public static class MapCntSolution { // 维护每个元素对应的出现次数 Map\u0026lt;Integer, Integer\u0026gt; val2Cnt; // 维护最多的次数 int maxCnt; public int[] findMode(TreeNode root) { val2Cnt = new HashMap\u0026lt;\u0026gt;(); maxCnt = 0; dfs(root); List\u0026lt;Integer\u0026gt; majorityList = new ArrayList\u0026lt;\u0026gt;(); for (Map.Entry\u0026lt;Integer, Integer\u0026gt; entry : val2Cnt.entrySet()) { if (entry.getValue() == maxCnt) { majorityList.add(entry.getKey()); } } int[] ans = new int[majorityList.size()]; int i = 0; for (int majority : majorityList) { ans[i++] = majority; } return ans; } private void dfs(TreeNode node) { if (node == null) { return; } int val = node.val; int newCnt = val2Cnt.getOrDefault(val, 0) + 1; maxCnt = Math.max(maxCnt, newCnt); val2Cnt.put(val, newCnt); dfs(node.left); dfs(node.right); } } 复杂度分析 时间 需要完整遍历入参的BST，时间复杂度为O(n)。\n空间 使用额外的Map维护计数，空间复杂度O(n)。\nBST中序遍历版本 思路 BST与普通二叉树的最大区别就是：BST中元素是有序的。\n我们使用中序遍历，即可有序遍历到每个节点。\n当序列有序时，我们就可以逐个段维护对应的计数，同时到下一个段时，可以及时更新最大计数。\n这样可以优化掉上个版本中的map，省掉了额外的空间开销。\n具体的步骤参见下方代码中的注释：\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 public static class InOrderBSTSolution { // 利用BST中序遍历有序的特性，进行有序遍历 // 维护每个段的元素值，segmentVal int segmentVal; // 维护每个段的元素出现次数，cnt int cnt; // 维护所有端元素的最大出现次数，maxCnt int maxCnt; // 使用一个列表维护答案，当最大出现次数更新时，重置列表 List\u0026lt;Integer\u0026gt; ans; public int[] findMode(TreeNode root) { segmentVal = root.val; cnt = 0; maxCnt = 0; ans = new LinkedList\u0026lt;\u0026gt;(); inOrderDFS(root); int[] ansArr = new int[ans.size()]; for (int i = 0; i \u0026lt; ans.size(); i++) { ansArr[i] = ans.get(i); } return ansArr; } // 中序遍历，中间每个节点真实的判断与处理一个有序的数组元素一样 private void inOrderDFS(TreeNode node) { if (node == null) { return; } inOrderDFS(node.left); handlePerNode(node); inOrderDFS(node.right); } // 有序遍历时，处理每个元素 private void handlePerNode(TreeNode node) { int nodeVal = node.val; if (nodeVal == segmentVal) { // 连续的段，更新计数 cnt++; } else { // 更新为一个新的连续的段 segmentVal = nodeVal; cnt = 1; } // 更新出现最多的元素 if (cnt == maxCnt) { ans.add(nodeVal); } // 重置最大计数 if (cnt \u0026gt; maxCnt) { maxCnt = cnt; ans.clear(); ans.add(nodeVal); } } } 复杂度分析 时间 依然需要完整遍历入参的BST，时间复杂度为O(n)。\n空间 利用了BST中序遍历有序的特性，无需额外维护计数map，空间复杂度O(1)。\n小结 很多时候算法题会给出一些题干、设定，其中会隐含一些关键条件，本题中的BST有序就是关键条件。\n往往利用好这些关键条件，就能提升算法性能。\n这种思路同样适用于工作中的系统设计，一般业务场景、团队诉求会有一定的倾向性，将业务倾向性与技术方案结合，往往就是最合适的方案。\n","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/find-mode-in-binary-search-tree/","summary":"\u003cp\u003eLC 501. 二叉搜索树中的众数，在计数解法的基础上，利用二叉搜索树中序遍历有序的特性优化空间复杂度。\u003c/p\u003e","title":"LC 501. 二叉搜索树中的众数"},{"content":"RocketMQ事务消息是柔性分布式事务的一种具体实现方案，本文，我们来理解一下，为什么事务消息能够保障分布式节点间数据的一致性。\n背景 在 聊聊月租酒店项目中的交易系统设计 中，在关闭租约的场景下，我们使用了RocketMQ的事务消息作为可靠消息最终一致性的落地方案。\n同样作为另一种可靠消息最终一致性的方案，在支付成功下外部订单的场景中，我们又使用了本地消息表的方式。\n本文，我们就来研究一下可靠消息最终一致性，同时深度拆解一下RocketMQ事务消息是如何设计+实现的。\n什么是不可靠消息？ 我们看几段伪代码：\n1 2 3 4 5 @Transactional public void a(){ saveDB(); sendMsg(); } 这段代码有几个问题：\n当发送消息碰到网络超时，a方法捕获到超时异常，此时本地事务回滚，但是消息却发到了MQ，此时本地数据与下游产生了不一致； 当发消息耗时比较久时，会延长事务的时间，影响数据库性能； 我们可以尝试使用异步发送消息的方式来改善a方法：\n1 2 3 4 5 @Transactional public void a2(){ saveDB(); sendMsgAsync(); } 或者也可以尝试把发消息放到事务外：\n1 2 3 4 5 public void b(){ @Transactional saveDB(); sendMsg(); } a2方法、b方法引入了新的问题，当事务执行成功，而发消息却失败了，那么下游就无法与上游事务保持一致。这种case就是我要讲的「不可靠消息」。普通消息跟本地事务无法保证事务的原子性。\n这也是分布式事务中一致性最弱的一种落地方案：最大努力通知型。\n这种方案虽然看起来「弱鸡」，实际也是一种常用的解法，适用场景：\n分布式事务横跨多个系统，其中包括了外部系统（不可控、协调改造成本高）； 对最终一致的时间敏感度极低； 事务参与方处理结果不需要影响事务发起方（即事务不存在后置失败回滚）； 一种典型的场景就是签约、支付完成后，系统发短信通知给用户。此时通知消息可以容忍丢失。 另一种典型场景就是合同签约后，需要同步到GR的备案系统，此时的备案消息也可以容忍丢失。\n一般实践中，我们可以在事务发起方预留状态查询接口，供事务参与方回查，也可以使用别的补偿方式，比如聊聊月租酒店项目中的交易系统设计 中超时关单的场景下，我们使用被动关单的方式补偿了低概率下的发消息失败的case。\n当然了，有不可靠消息，也就有可靠消息。\n什么是可靠消息？ 可靠消息最终一致性的方案是典型的柔性事务解决方案，其适用场景是一致性要求不需要严格实时，允许有一定程度的延迟，同时不能接受消息丢失，业务效果表现为最终一致。\n其基本的运作过程：\n事务发起方执行完本地事务； 发送消息（不能丢失），这一步就是「可靠消息」的意思； 事务参与方（消费者）成功接受消息，并消费成功； 可靠消息强调的点：只要事务发起方将消息发出去，事务参与方就可以成功执行，整个分布式链路上能达到事务的最终一致。\n如何实现可靠消息？ 本地消息表 如 聊聊月租酒店项目中的交易系统设计 支付成功下外部订单的场景中我们实现的重试任务表就是一个典型的本地消息表。不过我们的重试任务表缺少了一个消息生产消费的链路，但是核心思想是一样的。\n我画了一个本地消息表的基本设计图： 这种方案的优点是比较成熟，通过本地消息表的方式持久化了消息数据，保障了消息不丢失，同时通过MQ消息解耦了多个事务，从而避免了分布式事务的问题。\n但同时也有一些缺点：\n消息表耦合到了事务发起方的业务库中，对业务存储有干扰，同时需要业务服务实现重试等逻辑； 事务参与方需要保证消费的幂等（这个问题不大）； 如果消息重试后失败，需要其他兜底手段； 服务职责耦合，不利于复用、拓展； 除了这种可靠消息方案，还有RocketMQ实现的更佳的方案。\nRocketMQ事务消息 本地消息表方案有部分缺点已经被RocketMQ事务消息解决掉了。\n比如通过使用RocketMQ事务消息，多个服务使用可靠消息机制时，不需要单独实现重试、本地消息存储的逻辑，起到了复用、拓展的作用。\n设计原理 我们根据 RocketMQ官方文档：事务消息 理解一下事务消息的运作过程，我翻译了一下官方的流程图： 其实一上来理解事务消息，可能有一点难度，但是经过上面从不可靠消息到本地消息表的分析，相信到这一步，事务消息的实现原理已经一目了然了：\n半消息的发送、存储，其实就相当于是本地消息表中的存储，经过第二步的响应，RocketMQ可以保证消息一定会存储到broker服务端（这一步提高可用性无非就是持久化+副本复制等操作）； 第五六七步其实就是RocketMQ实现的定时回查机制，保证能够处理首次事务决议没收到结果的异常情况； 基于上面两方面的设计，RocketMQ可以保证一阶段的结果，要么是成功要么是失败的，一阶段成功才继续二阶段的消息发送消费； 整个流程，RocketMQ使用如下的状态机保证流程运转： 在具体使用上，基本就以下几个要点：\n创建MessageType 为 Transaction 的topic； 在事务发起方的业务方法中发送消息； 实现org.apache.rocketmq.client.producer.TransactionListener下的接口： executeLocalTransaction 执行本地事务； checkLocalTransaction 回查本地事务执行结果； RocketMQ官方文档：事务消息 对于实践也有一些建议，可以参考一下。\n源码简析 经过上面的设计分析，我们可以看到RocketMQ与本地消息表最大的区别就是本地消息持久化与事务状态回查的逻辑实现在了通用MQ中。\n这块简单看下源码，不做过多源码解读。 我这里用的版本：\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-stream-rocketmq\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2023.0.0.0-RC1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.rocketmq\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rocketmq-broker\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 第一步：发送事务消息流程 从时序图不难看出，首先我们的服务端属于producer，主要用来触发生产事务消息的动作，在接到RocketMQ存储服务端响应后，尝试执行二阶段的事务执行，结束整体流程。\nRocketMQ存储服务端也就是broker，在发送消息这一步主要负责消息的存储，针对事务半消息会使用一个单独的RMQ_SYS_TRANS_HALF_TOPIC存储维护。\n第二步：回查事务处理状态 这一步涉及的代码稍微多一点。读懂这块可以了解一些RocketMQ大致的设计：\nBrokerController是所有核心组件的总控，跟Spring/SpringBoot设计类似，所有的组件都托管，同时对应有一个Bootstrap启动类用于启动核心组件、一些任务； RocketMQ网络通信借助了Netty的能力，在接口层面设计了一个RemotingCommand表示请求，而对应不同的流程类型有不同的Processor； 回查涉及到我们应用的服务端会注册对应的CheckListener，最终broker回调到我们的服务时，在钩子函数中触发我们业务的回查逻辑； 小结一下就是这里无非就是实现两大块：\n将事务半消息单独存储，我们的服务发消息给mq服务端； mq服务端启动任务回查我们的事务执行状态，成功则二阶段提交，否则就执行回滚； 总结 通过本文，我首先从上篇文章实际应用的案例入手，然后通过几段粗糙的伪代码过了一下不可靠消息的适用场景以及问题。\n接着作为对比，我们引入了可靠消息的概念，首先分析了可靠消息的适用场景以及设计实现的要点，然后我们分析了一下其经典实现「本地消息表」方案的设计思路。\n通过本地消息表的缺点，最后引入了本文的重点：RocketMQ事务消息，其解决了本地消息表的一些缺陷。我们通过设计流程、核心状态机了解了其设计原理，对涉及到的源码也做了时序调用的简析。\n最后我们小结一下，可靠消息作为最终一致性满足BASE理论的方案，其适用的场景：\n消息独立、可靠存储； 通过三方中间件解耦业务系统； 对数据一致性时间相对敏感； 其设计的核心能力：\n半消息发送、存储； 事务发起执行； 事务状态回查； 二阶段消息发送； 以上，欢迎指导。\nRef RocketMQ官方文档：事务消息 ","permalink":"https://redolog.github.io/posts/rd/design/trx-message/","summary":"\u003cp\u003e\u003ccode\u003eRocketMQ\u003c/code\u003e事务消息是柔性分布式事务的一种具体实现方案，本文，我们来理解一下，为什么事务消息能够保障分布式节点间数据的一致性。\u003c/p\u003e","title":"RocketMQ事务消息原理解析"},{"content":"本文我会尝试聊一下去年我主R做月租酒店项目时的一些核心设计要点。\n酒店项目是我来贝壳后主R的第一个比较完整的从零到一的项目，涵盖了对外交互、房源、签约、交易、结算等诸多领域，项目中有比较多有意思且有挑战的设计，对于当时的我来说，能够全链路承接整个项目也是一大挑战。\n本文聚焦的重点是其中的签约、支付、外部下单模块，我统称这些模块为交易模块。\n背景 先简单给读者介绍一下项目背景。\n贝壳租赁有多种业务模式，其中公寓是B2B2C的三方商家模式，比如自如、自营公寓（海盐）均是公寓模式下的大KA商家。商家通过签约入驻与贝壳达成合作协议，之后便可以使用公寓完善的系统进行管房、管钱、流量商业化等能力。平台对城市、商家也有完善的运营管理能力。\n而月租酒店是对公寓模式的一种房源供给以及交易模式的补充。\n前期运营同学使用公寓系统已经跑通了业务模式：录房、上架、签约去化，验证了模式的有效性。但是使用原系统录房只有两种途径：\n通过贝壳管房系统手动录 通过贝壳的OpenApi调接口录 对于酒店方来说，并没有动力针对贝壳这类外部渠道开发对接OpenApi，因此想要提升录房效率，需要贝壳侧进行适配对接。\n像抖音、快手这类渠道，与贝壳类似，都是针对酒店侧提供的接口进行适配对接。\n让管家联系酒店下单，效率也很低。因此除了对接房源，订单对接也很有必要。\n除此之外，我们也建设了结算对账线上化能力，帮助平台与酒店商家高效分账。\n由于我们是直接对接了外部的房源接口，同时订单也是直接调外部接口创建的，从系统交互上来看，我们跟外部做了直连，因此月租酒店项目，也叫做直连酒店。\n房源直连，提升录房、发房效率\n订单直连，提升下单效率\n月租直连酒店整体的运作过程： 系统设计 了解了大致背景，下面看下我们是怎么设计这块系统的。\n整体设计 首先简单看下整体的设计。\n还是看一下全景架构图： 相信读者看完背景介绍后，能够理解我们要做什么。简单来说我们做了三块建设：\n针对酒店供应商的接口，我们做了适配对接，其中包括房源、订单的对接； 针对租客侧的签约、下单、支付等签中流程，我们建设了完善的交易能力； 针对平台与商家的结算，我们也做了系统功能； 其中有些链路沿用了公寓、C端已有的能力，其中商家与贝壳的签约，则沿用了公寓的CRM能力。\n在系统的层面看，我们新增了三个服务：\nhotel-adator：负责适配外部接口，屏蔽不同供应商的差异； hotel-house：负责维护酒店房源模型，对外接入adaptor的酒店、房源，对内同步数据到公寓的房源链路； hotel-zuwu：负责维护酒店交易模型，流转租约、账单等核心状态机，对接内部支付、对账中台； 经过评估，我们的结算业务并没有太复杂，因此我们将结算跟签约交易统一放到了租务项目中，租务是租赁业务中负责租中事务的领域。 到这里，读者能对这块业务、系统有一个整体感知即可。\n交易模块设计 到这部分，其实才是我想写这篇文章的重点。\n接下来我们来看看，交易整个大的模块内，到底有哪些点是需要我们仔细考虑的。\n数据模型设计 设计模型的前提是，要对业务流程、业务动作、业务角色有掌控力。经过前期调研、充分沟通、流程梳理，我们明确了月租酒店的业务流程与迭代的规划。\n对于外部的订单交互来说，我们只需要做这几件事： 在内部的订单模型中，我们需要关联外部订单。内部的订单主模型，我们叫做租约。\n对于内部的签约、交易、结算以及营销，我们需要实现以下核心能力： 根据以上对业务流程、业务模式、核心动作的讨论、推断，我们最终的模型设计如下： 读到这里，读者对数据模型有大概的认知即可。\n分布式一致性设计 上面的模型只是为了方便读者理解业务跟系统，这部分的分布式一致性设计，其实才是真正的重难点。\n在月租直连酒店项目中，交易与结算模块都需要考虑接口的事务一致性、数据的完整性，由于交易涉及场景更多，本文我们就以交易为例，结算模块中的实践与考量实际大同小异。\n简单来说，我们为了保证一致性，引入了分布式事务的方案。\n行业内针对这类问题，通用实践的解法一般是可靠消息、最大努力通知、TCC、Seata框架。\n在我们的项目中，根据不同场景的诉求，我们主要引入了三类方案，以下是按照一致性保障强度从高到低的排序：\nSeata 的 AT 模式 可靠消息最终一致性 RocketMQ事务消息 本地消息表 最大努力通知 MQ普通消息（可能会丢消息）+接口补偿 我们从方案+场景的维度逐个分析。\n创建租约（Seata AT模式） 商机管家接到租客的咨询并且拿到租客的下单意向后，会在我们的B端系统创建租约，之后租客便可以签约、支付。\n创建租约的时序图如下： 可以看到这一步逻辑很多，同时交互的服务也很多。图中红框标出的是分布式事务的范围。\n设计1：\n首先我们在做协议的校验之前就调用了外部酒店的验单接口，如果这一步没有库存或者被其他规则拦住，则流程终止，过程中没有数据损失。\n设计2：\n其次协议的创建、盖章等动作并没有放入事务中，有这些考虑：\n首先协议中台的这些接口除了数据写入，同时还有文件操作，Seata的AT模式只支持MySQL的数据写入； 其次协议中台的这一系列接口，支持重入+数据无需回滚，如果前序某个接口调用失败了，我们会提示用户明确的文案，提示用户可以重试，这样数据、流程整体也是完整自洽的； 最后一点是因为，协议中台提供的SLA并不保障多个接口的连续调用性能，如果接入到分布式事务中，可能会导致事务耗时过长； 结合以上几点，我们评估在B端的场景下，协议交互的这几个调用可以不放到事务内。\n设计3：\n对于酒店B端租约、账单等数据的创建以及锁券，则需要使用分布式事务保障调用的一致性。\n我们看下主流分布式事务协议的对比： 结合我们这里的业务场景，要求尽可能保障事务的一致性，要求具备隔离性，同时事务参与的几方均为关系型数据库操作。Seata AT模式对业务代码基本无侵入性，同时公司内有基建团队维护，因此这里我们倾向于使用AT模式来保障一致性。\n在创建租约的场景里，Seata服务端就是TC（协调者），我们酒店的B端租务服务就是TM（管理者），而营销的服务就是RM（资源管理器）。一次分布式事务调用的过程大致如下：\nTM（酒店B端租务服务）调用TC创建分布式事务的记录，获得生成的XID； TM（酒店B端租务服务）通过RPC框架调用其他服务接口，同时会携带上一步生成的XID； RM（营销服务）通过其接收到的XID，将其所管理的资源且被该调用所使用到的资源注册为一个事务分支(Branch Transaction)； 当该请求的调用链全部结束时，TM（酒店B端租务服务）根据本次调用是否有失败的情况，如果所有调用都成功，则决议Commit，如果有超时或者失败，则决议Rollback； TM将结果同步给TC，TC根据结果协调各RM，进行事务二阶段的提交或回滚； 设计4：\n创建租约这一步，由于调用多耗时较长，我们针对前端的交互做了体验上的优化，点击提交按钮后，系统会弹窗提示用户目前进行到了哪一步来缓解用户体验上的焦虑（类似携程上买票的过程）。\n对比C端电商更通用的下单场景，其中的设计要点跟倾向性会跟B端很不一样，比如C端需要更多性能、可用性方面的设计。\n月租酒店项目中，除了这个场景，在以下场景也使用了AT模式保障了事务的实时一致性：\n签约完成：创建支付流水+支付单，其中创建流水、更新账单为本地数据库操作，创建支付单则需要调用支付中台； 外部下单成功：更新租约状态、同步核销券； 签约、支付超时关单（最大努力通知） 租约在创建后以及租客在支付后，服务端均需要通过RocketMQ的延迟消息来完成超时关闭的功能。我们的实现方案中，在对应的节点，会发送一条RocketMQ延迟消息，在消费逻辑中判断是否满足超时的条件，从而触发关闭逻辑。\n这里发送消息的时候，我们一般不会将发消息与本地事务放在一起，因此，消息发送无法保证事务的原子性，也就是说，消息可能会丢。\n这里我们评估了场景对消息的丢失容忍度，是可以接受丢消息的，这就是最大努力通知的一种应用场景。\n当然，为了提高业务的可用性、数据完整性，我们会在用户查询租约、账单的时候，做一个被动超时关单的操作，算是对最大努力通知的一种补偿。\n支付成功（本地消息表） 租客在支付完成之后，就进入了外部下单的流程，这一步我们主要是更新本地的账单状态，更新流水实付信息，同时触发外部下单的调用。\n支付成功的时序图如下： 这里我们主要有两个设计：\n设计1：\n首先我们设计了一个重试任务的表，放在酒店租务服务中，能保证只要本地事务成功，重试的任务就可以持久化下来。这解决了使用普通消息发送失败然后丢消息的问题。\n重试任务目前我们设计每分钟都会触发。\n这里为什么要考虑做这种设计呢？原因是比如如家提供的接口，有可能响应非常慢。在外部提供的SLA不够高的情况下，使用定时任务的设计能够提高流程整体的可靠性。同时与支付成功的节点做了一定的解耦。\n设计2：\n其次重试的任务中会记录状态与重试次数，重试次数消耗完之前，只要没有进入子流程，会一直重试。\n至于重试次数的配置，会根据不同的供应商提供的SLA进行调整。\n下单成功或者失败的子流程一致性则在子流程内部收敛。\n关闭租约（RocketMQ事务消息） 签约超时、支付超时、外部下单失败修需要关闭租约。\n而在关闭租约时除了作废租约、账单，还需要同步解锁券，券的操作就涉及到了营销中台的接口调用。\n设计考虑：\n这里我们同样可以考虑Seata的AT模式，但是在这个场景下，其实不需要强一致以及隔离性。\n由于券已经被锁定，并且传入了我们的业务标识（租约的唯一编码），因此资源其实已经在业务数据层面做了隔离。\n这个场景下，使用RocketMQ事务消息是一种更轻的方式，同时也能保障最终一致性。作为柔性事务的实现方式之一，事务消息满足BASE理论，即基本可用+最终一致。\n总结 本文主要给读者介绍了我去年主R月租直连酒店项目的整体设计以及交易核心涉及的一些分布式一致性设计要点。\n在B端交易的场景下，我们主要用到了三类方案来保障分布式环境下的数据一致性，即：\nSeata 的 AT 模式 可靠消息最终一致性 RocketMQ事务消息 本地消息表 最大努力通知 MQ普通消息（可能会丢消息）+接口补偿 在针对不同场景考虑同样的分布式一致性问题的时候，最后的方案可能不同，但是设计的思路其实是一致的，即：\n评估场景的侧重点、倾向性； 评估场景对一致性、隔离性、数据实时性等指标的容忍度； 评估方案与场景诉求的契合程度； 评估团队能力、基建现状； 系统设计出了需要梳理业务流程、明确业务规则，还需要做出取舍。除了一致性，在做系统设计的时候，也需要兼顾性能、可用性。同时技术方案需要做好兜底设计。\n最后，如果读者有疑问或者建议，欢迎评论区指出。\n以上。\n","permalink":"https://redolog.github.io/posts/rd/design/case/hotel-project/","summary":"\u003cp\u003e本文我会尝试聊一下去年我主R做月租酒店项目时的一些核心设计要点。\u003c/p\u003e\n\u003cp\u003e酒店项目是我来贝壳后主R的第一个比较完整的从零到一的项目，涵盖了对外交互、房源、签约、交易、结算等诸多领域，项目中有比较多有意思且有挑战的设计，对于当时的我来说，能够全链路承接整个项目也是一大挑战。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e本文聚焦的重点是其中的签约、支付、外部下单模块，我统称这些模块为交易模块。\u003c/strong\u003e\u003c/p\u003e","title":"聊聊月租酒店项目中的交易系统设计"},{"content":"本文尝试归纳整理作者所了解的AQS架构设计原理。首先从解决的问题场景出发，再到AQS核心设计方案，最后会引入一些底层能力的简要说明。\n开始前先看看管程模型的图：我个人觉得管程模型其实是理解锁、AQS的 抓手，对于我们理解AQS至关重要。\n当有一块新知识、新业务、新系统需要迅速上手时，我的一个思路就是顺藤摸瓜。那么找到这个藤就很关键。对于AQS来说，管程模型就是藤。同时管程模型还承载了核心设计的架构与流程，因此一张图可以简明扼要地帮助我们认识、理解AQS。\n下面开始正文。\n0.引子：实现一个简单的锁 对于多线程的环境，我们往往需要通过锁的机制来保证资源访问的正确性、数据的一致性。\n下面就是基于自旋实现的一个基本的锁：\n1 2 3 4 5 6 7 8 9 10 11 12 13 class SimpleLock { private AtomicBoolean locked = new AtomicBoolean(false); public void lock() { while (!locked.compareAndSet(false, true)) { // 自旋等待，直到获取锁成功 } } public void unlock() { locked.set(false); } } 我们可以在业务方法中使用上面实现的锁：\n1 2 3 4 5 6 7 8 9 10 public void service(){ // 使用锁来保护对共享资源的操作 SimpleLock lock = new SimpleLock(); lock.lock(); try { // 这里执行需要同步的业务逻辑 } finally { lock.unlock(); } } 1.AQS解决的问题 在SMP架构的CPU上，并且是多线程竞争激烈的情况下，执行上面的业务方法，每个线程对应核都需要频繁与总线bus交互，总线负责将当前最新的状态刷到主存，过程如下： 当竞争状态严重时，上面基于自旋实现的锁，会有一些问题：\nbus总线风暴 过多自旋浪费CPU资源 关于总线风暴的解释：CAS操作和volatile一样都需要CPU进行通过MESI协议各个内核的Cache一致性，会通过 CPU的BUS(总线)发送大量MESI协议相关的消息，产生Cache一致性流量。因为总线被设计为固定的通信能力，如果Cache一致性流量过大，总线将成为瓶颈，这就是所谓的总线风暴。\n而解决上面问题的常用手段有:\n分散操作热点 使用队列削峰 二者都是「空间换时间」的思路。\n解法1的典型案例就是juc中的LongAdder，相比AtomicLong，LongAdder将单个CAS热点 (value值)分散到了一个cells数组中，等到需要获取数据时才做加和运算。\n解法2的典型案例就是synchronized、AQS，他们的原理都是将发生CAS争用的线程加入一个队列中排队，降低CAS争用的激烈程度。\n这里的队列以及对共享变量的管理方式，我们叫它MESA管程模型。\n2.核心设计 AQS最为核心的设计，就是基于Java库实现了一套管程模型。甚至Java早期提供的synchronized锁机制，也是管程模型。\nAQS、synchronized二者基本流程对比如图： 2.1管程模型 管程模型的一些基本规则：\n持有共享变量； 控制共享变量的操作； 线程操作共享变量的前提是必须获得锁才能进入管程内部； 获取锁失败，则进入同步队列阻塞等待； 进入管程内部，则判断条件是否满足，不满足则进入条件队列阻塞等待； 条件达成后，重新进入同步队列排队； 这里的流程规则可以结合上面的流程图来理解。\n2.2AQS核心组件 基于上面的模型，java.util.concurrent.locks.AbstractQueuedSynchronizer中有以下核心组件：\n同步状态标记： volatile int state 抢锁同步队列（双向链表）： volatile Node head volatile Node tail 条件等待队列（单向链表），实际使用条件队列时，通过持有对应的ConditionObject获取条件队列的操作方法： ConditionObject Node fistWaiter Node lastWaiter 2.3AQS核心流程 有了组件，我们还需要定义如何使用这些组件、在哪一步用对应的组件，这就是AQS作为一个抽象类通过模板方法设计模式定义的主流程。\n2.3.1资源共享方式 AQS中设计了两套资源共享的方式：\nExclusive(独享锁) 只有一个线程能占有锁资源，如ReentrantLock。独享锁又可分为公平锁和非公平锁。 Share(共享锁) 多个线程可同时占有锁资源，如Semaphore、CountDownLatch、CyclicBarrier、 ReadWriteLock的Read锁。 独占与共享模式的主要区别有两点：\n资源占用模式不同：简单来说就是独占的state通常为0-1，而共享的state通常大于1。 独占模式下同一时刻只能允许一个线程获得锁，其他线程此时只能进入队列阻塞等待，典型实现案例是ReentrantLock； 共享模式下同一时刻允许多个线程获得锁，典型实现例子是Semaphore/CountDownLatch； 线程等待与唤醒策略不同：简单来说就是独占模式一次只唤醒一个线程，而共享模式则根据state数量一次唤醒多个线程。 2.3.2AQS主模板流程 针对不同的资源共享方式，有不同的主流程，即对应共享锁、独享锁模板流程。\n除了模板中已定义的核心主流程，具体的子类实现可以通过钩子方法自定义想要的核心节点能力。\n独占锁可以自定义：\ntryAcquire(int)，尝试获取资源。若成功则返回true，若失败则返回false。 tryRelease(int)，尝试释放资源。若成功则返回true，若失败则返回false。 isHeldExclusively()，判断该线程是否正在独占资源。只有用到condition条件队列时才需要去实现它。 共享锁可以自定义：\ntryAcquireShared(int)，尝试获取资源，负数表示失败，0表示成功，但没有剩余可用资源，正数表示成功，且有剩余资源。 tryReleaseShared(int)，尝试释放资源，若成功则返回true，若失败则返回false。 我们先以独占锁为例，用泳道图梳理一下AQS中获取锁与释放锁的主流程与钩子方法的调用时机： 可以看到，在AQS主流程中主要实现节点入队出队、尝试获取锁、线程启停、节点状态机流转等逻辑。而钩子方法中主要实现同步状态的更新、获取。\n共享模式的主流程与钩子方法与独占模式大同小异，本文暂不做过多描述。\n3.底层能力的支持 对于AQS实现用到的一些底层能力，主要是一些我之前学习时候的答疑。\nQ：如何保证状态更新的一致性？\nA：使用CAS能力即可，LockSupport帮我们封装了cmpxchg指令，从CPU维度提供了原子更新能力。\nQ：如何实现线程的挂起唤醒能力？\nA：使用LockSupport.park/LockSupport.unpark即可，同样是用了unsafenative的能力，能够用来通过Java方法调用操作系统操作线程的状态的能力。 对应状态机可以参考： 4.小结 最后用一张图总结本文的内容： 以上。\n","permalink":"https://redolog.github.io/posts/rd/design/aqs/architecture/","summary":"\u003cp\u003e本文尝试归纳整理作者所了解的\u003ccode\u003eAQS\u003c/code\u003e架构设计原理。首先从解决的问题场景出发，再到\u003ccode\u003eAQS\u003c/code\u003e核心设计方案，最后会引入一些底层能力的简要说明。\u003c/p\u003e","title":"理解AQS架构"},{"content":"本文记录最近与朋友聊过的算法实例：给定一堆带重量的货物，计算最少需要的货车数量。\n业务算法系列：收录业务系统开发中真实遇到的算法\n背景 题目描述：\n给定一个int[]，weights [100,300]范围，每个元素表示货物重量的数组\n求装完给定货物最少的车辆数\n限制：每个货物最多能装300的货物\n货物重量数组长度为n，货物重量区间为m\n思路 过了两天已经想不起我最开始的错误思路了，所以本文就聚焦来看正确思路吧。\n关于这道题跟湧哥聊了一轮，他给了我一个解题的窍门：针对这种题目，对于入参有常数级的限制，因此可以考虑一些特定的做法，比如对数据排序可以考虑桶排序，对于数据查找可以粗暴遍历（因为复杂度是常数级）。\n正确的解题思路： 对货物按照重量排序； 针对每辆车，尽可能找当前剩余最重且当前车能装的下的货物（贪心）； 重复这个过程，一直每辆车都装满或者装不下剩余货物中最轻的； 第一步：排序 对于排序，最粗暴的方式就是使用快排，针对本题，由于货物重量区间为[100,300]，是一个常量区间，因此可以使用桶排序，使用一个201长度的数组即可统计货物重量以及数量，同时一次桶排序仅需遍历一次入参数组，时间复杂度为O(n)。\n在排序这一步，桶排序优于快排。\n第二步：找当前装的下最重的货物 本题中一辆车最多能装三个100，因此查找当前装得下最重的货物可以粗暴遍历，从当前车辆剩余容量开始逐个遍历即可，最多遍历201次。\n拓展：如果入参可以很大，这里的查找可以考虑二分法，这样可以保证较大数据集的时候，查找效率最高。\n第三步：重复装车 这一步只要维护好边界，能够及时跳出循环即可。\n代码实现 思路描述完了，下面展示我两种思路下的代码实现：\n桶排序+遍历查找 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 /** * 观察题目给定的条件，发现货物重量区间为 [100, 300]，是一个常数级别的区间，因此我们可以尝试两个操作： * 1. 给定的重量排序，可以使用桶排序，排序的复杂度能达到O(n)，要优于快排这种方案； * 2. 每辆车去找当前能装得下的最大货物时，可直接遍历查找（因为逐个遍历也就是O(201)的复杂度，是一个常数级复杂度）； */ public static class BucketSortSolution { public int findMinimumQuantityOfCars(int[] weights) { // 排序后的桶，维护货物重量-\u0026gt;货物数量 // 复杂度：O(n) // 一共n个货物 int bucketN = 300 - 100 + 1; int[] weight2Cnt = new int[bucketN]; for (int weight : weights) { weight2Cnt[weight - 100]++; } int ans = 0; while (true) { // 标记当前车辆有无货物可装，一辆车最多装3个货 int carWeightCnt = 3; // 当前车辆剩余可装容量 int remainWeight = 300; // 每辆车尽可能装满 // 从大的货物开始装 for (int i = remainWeight - 100; i \u0026gt;= 0; i--) { int currWeight = i + 100; if (remainWeight \u0026lt; currWeight || weight2Cnt[i] == 0) { // 当前货物太重了，看下一个比较轻的 // 当前重量没有货物 continue; } while (remainWeight \u0026gt;= currWeight \u0026amp;\u0026amp; weight2Cnt[i] \u0026gt; 0) { remainWeight -= currWeight; weight2Cnt[i]--; carWeightCnt--; } } if (carWeightCnt == 3) { // 已经没有货物了 break; } ans++; } return ans; } } 复杂度 空间复杂度，使用了一个额外的数组维护重量对应数量，复杂度O(m)。\n时间复杂度，排序遍历一次入参数组，每辆车查找遍历x次排序数组（x为一个常量），时间复杂度最大为O(m*n)。\n红黑树（或者可以考虑java中的跳表结构） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 /** * 使用有序集合（树、跳表）的方案思路： * 1. 使用红黑树记录每个重量对应的货物数量； * 2. 每辆车从当前剩余 */ public static class BinarySearchCollectionSolution { /** * 整体复杂度：O(n*logn) * 限制：每辆车可以装载最大300的货物 * * @param weights [100,300]范围，每个元素表示货物重量的数组 * @return 求装完给定货物最少的车辆数 */ public int findMinimumQuantityOfCars(int[] weights) { int ans = 0; // 每个重量的货 对应 数量 TreeMap\u0026lt;Integer, Integer\u0026gt; weight2Count = new TreeMap\u0026lt;\u0026gt;(Comparator.naturalOrder()); // 树形成复杂度 O(n*logn) for (int weight : weights) { weight2Count.put(weight, weight2Count.getOrDefault(weight, 0) + 1); } // 每辆车先选中一个当前最大的货，重量x，然后去找小于且最接近(当前剩余容量-x)的货，如果此时(当前剩余容量-x)小于100则装下一辆 // 每一轮选中某个货物后，从treemap有序结构中移除货物计数 while (true) { if (weight2Count.size() == 0) { break; } // 从最重的货物开始装 // 获取最大值复杂度 O(1) Integer maxWeight = weight2Count.lastKey(); // 更新最重货物量 updateTargetWeightCount(weight2Count, maxWeight); if (weight2Count.size() == 0) { break; } int remainWeight = 300 - maxWeight; while (weight2Count.size() \u0026gt; 0 \u0026amp;\u0026amp; weight2Count.firstKey() \u0026lt;= remainWeight) { // 这里使用树形结构查询，二分法复杂度 O(logn) Integer maxLowerWeight = weight2Count.floorKey(remainWeight); // 更新 小于且最接近(当前剩余容量-x)的货量 updateTargetWeightCount(weight2Count, maxLowerWeight); remainWeight -= maxLowerWeight; } ans++; } return ans; } // 更新某个key O(1) private void updateTargetWeightCount(TreeMap\u0026lt;Integer, Integer\u0026gt; weight2Count, Integer maxLowerWeight) { Integer maxLowerWeightCount = weight2Count.get(maxLowerWeight); if (maxLowerWeightCount - 1 \u0026gt; 0) { weight2Count.put(maxLowerWeight, maxLowerWeightCount - 1); } else { weight2Count.remove(maxLowerWeight); } } } 复杂度 空间复杂度，使用了一个额外的树形map维护货物重量，复杂度O(n)。\n时间复杂度，每次查找通过二分法找到小于等于特定重量的元素，时间复杂度O(logn)，这个过程最多查找n次，因此整体时间复杂度O(n*logn)。\n小结 对于题目给定的较小数据集，桶排序+遍历查找是更加直观的解法，同时湧哥几分钟内就实现了一版，他能够快速找到这道题给定条件的突破点并且实现，这点给了我启发：对于特定条件要有条件反射，小数据集有对应的套路。\n而对于更大或者更通用的数据集，则可以按照我实现的树结构、跳表结构来解题。\n两种算法各有优劣，对于本题货车装货的实际场景，桶排序的方式更高效，只有当m n变量变的更大时，二分的解法才更高效。\n","permalink":"https://redolog.github.io/posts/rd/algo/case/find-minimum-quantity-of-cars/","summary":"\u003cp\u003e本文记录最近与朋友聊过的算法实例：给定一堆带重量的货物，计算最少需要的货车数量。\u003c/p\u003e","title":"算法案例：给定一堆货物，计算最少货车数"},{"content":"如何迅速熟悉一摊子新的事？我目前总结、实践下来就四个字：顺藤摸瓜。\n全文一张图： ","permalink":"https://redolog.github.io/posts/methodology/work/how-to-get-familiar-with-new-thing/","summary":"\u003cp\u003e如何迅速熟悉一摊子新的事？我目前总结、实践下来就四个字：顺藤摸瓜。\u003c/p\u003e","title":"迅速熟悉一摊子事：顺藤摸瓜"},{"content":"针对近期处理的两例并发场景引发的「互斥性」「幂等性」问题进行过程分析、输出系统方案。\n阅读本文前，我想重申一个观点： 在相对复杂的工业代码中（绕来绕去，不是简单的一个函数调用），排查「简单问题」也不是一件简单的事。\n背景 近期处理租赁合同工单、报警，发现有些接口在并发、调用重试、用户连击等情况下的存在幂等性、数据一致性问题。\n本文借由一些问题实例，来系统性分析解决幂等性、数据一致性问题，同时针对这类问题做一下系统性的梳理。\n问题案例 案例 影响 问题1：已签约合同状态瞬时回退到起草状态 下游服务收到消息乱序。 问题2：重复写入 biz_record 合同详情报错，获取合同完整数据处操作完全阻塞。 具体的排查、分析过程放在内网工单处理过程wiki。\n本文我们核心目标是从上往下审视系统设计，以此来完成「理论指导实践」的落地，具体的代码细节不做过多展示（脱敏）。\n首先对问题逐个进行问题核心原因的分析。\n问题1 剖析 以问题2为例，租赁合同场景，系统主要用户：\n经纪人 业客（业主+租客） 签约过程的核心状态机： 其中盖章由经纪人在我们PC后台、App端完成。业客完成签字后，合同状态从盖章流转到签约状态。\n此时由于业客已经完成了签字动作，合同状态正常只能往后流转，即过户（物业交割）。\n真实场景下，如果合同确实存在变更（业客变更、条款变更），则应走变更流程。变更单提交并且通过审核后，合同状态才会回到起草，从头开始作业。\n而问题1案例中，根据下游服务的RD反馈，他们连续收到了两条消息：\n2024-03-04 19:31:27.296 合同已签约 2024-03-04 19:31:27.467 合同起草 经过细致的对日志+源码进行排查分析，我最终成功还原了当时并发的场景，如图： 简单描述下当时的情景：\n首先业客首先完成了签署； 状态更新的时刻：Mar 4, 2024 @ 19:31:27.290 随后事务提交； 随后分布式锁释放； 然后在稍后的同一时刻，经纪人修改了合同； 接口进入时刻：Mar 4, 2024 @ 19:31:27.277 此时业客完成签署的状态事务还未提交； 此时首先开启事务并查询了合同状态，查到的状态为旧的【已盖章】； 接着进行了一段较耗时的dubbo调用，调用收到响应的时刻 2024-03-04T19:31:27.361 ，对比上面的时间，可以发现此时业客签署已经处理完毕； 接着获取分布式锁（由于业客签署的接口已经执行完释放了锁，因此此处可正常获取到锁）； 在锁范围内进行一系列业务校验、业务更新； 前人此处考虑到了多个业务流间保证操作的互斥性，但是部分代码迭代后没处理好锁的范围。\n前人想做到的效果图例： 一句话总结问题：使用了分布式锁，意图想控制业客签署、修改合同两个业务流的互斥访问，但是在锁外先做了状态判断，导致锁内更新数据错误。\n此处使用分布式锁不当，导致两个应该互斥访问的操作隔离性不足。\n问题1核心是未保证多个互斥接口间的并发访问，从而导致数据一致性出错。\n解决 问题核心原因是在获取锁之前就判断了业务状态，两个接口间未保证同步性（访问互斥性）。\n仅需保证先获取锁，在锁内进行数据状态的获取与判断，问题即可解决。\n解决口诀：一锁、二判、三更新。\n问题2 剖析 问题2是一个典型的幂等性问题。\n由于代码脱敏原因，针对并发场景，我们使用图例还原下并发错误时的过程：\n一句话总结下问题：有分布式锁，但是没锁全。导致两个先后到达的请求可并发进入内部方法 sendMessageToUser，并发时，此方法使用事务无法保证幂等性。\n因此最后写入了多条 biz_record 数据，正常情况一个合同应该只有一条 biz_record 。\n解决 问题核心是分布式锁有，但是没锁全整个过程。\n因此只要保证锁住整个方法，问题即可解决。\n解决口诀：一锁、二判、三更新。\n总结 处理完上述两个问题，我们总结下共性。\n问题1：多个应当保证访问互斥的接口间，使用了分布式锁，但是在锁外先判断了业务状态，导致锁内拿到的其实是过期数据。\n问题2：接口应当保证幂等性，使用了分布式锁，但是未锁住需保证幂等的完整范围。\n可以看到，在这两类问题中，我们都只要做到：在需要保证访问互斥、幂等的接口上使用悲观锁对完整的代码范围加锁，同时需遵循【先加锁后做判断与更新】原则，问题都能解决。\n使用分布式锁确保业务互斥性的正反例对比： 使用分布式锁确保接口幂等性的正反例对比： )\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/distributed-lock/exlusiveness-and-idempotence-case01/","summary":"\u003cp\u003e针对近期处理的两例并发场景引发的「互斥性」「幂等性」问题进行过程分析、输出系统方案。\u003c/p\u003e","title":"使用分布式锁解决接口互斥性、幂等性问题两例"},{"content":"需求分支首先通过MR合并到了master分支，部署后发现有问题，于是回滚代码，操作了MR处的revert，后面修复了问题，使用原功能分支提新的MR时，发现变更处是空的，本文来解决此类无法合并情形的问题。\n首先说结论：对原MR的revert再次revert，后续即可使用原功能分支合并到主干分支。\n分析过程以及原因详见下文。\n背景 20240311晚，有个需求上线后发现有问题，于是回滚了部署资源，同时需要回滚代码。\n回滚代码使用的是gitlab MR处的revert： 上线范围包含了两个功能分支：\na b 其中a分支的内容需要回滚，b分支原先合并到了a分支。需要将b分支的内容重新合到master，进行部署上线。\n使用b分支提MR到master时，发现不显示一行变更，即MR无法正常操作。\nrevert操作后，会提交一个新的commit，用来表示前面的mr被回退。\n该团队该项目git协作使用的是功能分支完备后merge到主干分支的方式。\n分析 操作关键步骤 我们对上面的问题进行简化，在本地环境开启新的分支进行模拟：\ntest-mr-revert 模拟功能分支； master-bak-20240312 模拟主干分支； 1. 开发功能 在 test-mr-revert 上提交一些commits。\n2. 使用MR合并 test-mr-revert 到 master-bak-20240312 MR会生成一个新的标识merge的commit。\n3. 进入刚刚的MR，点revert revert 后创建了一个分支，其中新增了一个commit，回退了之前的MR，之后会提交一个新的MR，将revert对应commit合并到master-bak。\n4. 再次合并 test-mr-revert 到 master-bak-20240312 此时复现了我们上线时的情形： 找根因 Google搜索「git revert merge and remerge」，找到了StackOverflow上的一个问答： https://stackoverflow.com/questions/1078146/re-doing-a-reverted-merge-in-git\n08、09年的时候就有人提出了这种情况的疑问。\n对应官方 git-scm 回退一个有问题的merge文档：https://mirrors.edge.kernel.org/pub/software/scm/git/docs/howto/revert-a-faulty-merge.txt\n我贴一下官方文档中的关键解释： 小结一下这种情况的原因：\nrevert操作后，会提交一个新的commit，用来表示前面的mr被回退。 产生的影响就是，生成了新的revert commit，保留了原来功能分支上的commit。由于这个revert记录了操作了某些commit的回退，所以用原始功能分支上的commit操作merge，会没有变更。 因此问题根因就是，针对MR的revert，会将功能分支的所有commit标记为回退，因此再往主干分支合并时会不生效。\n针对整个过程画了个图，帮助读者更好理解： 解决方案 官方文档给出了解法：对前面的revert，进行一次revert操作。这种解法有一个前提是，功能分支问题已修复。\n针对我们11号上线的情况，由于我们其实是把两个功能分支合并到了一起，所以提了一个统一的MR，针对MR操作revert后，两个功能分支上的commit其实都会被标记为回退。因此针对此类情况，之后可以不合并两个分支，多个分支可以多次提交MR，这样针对MR回退时，多个分支间不受影响。\n另外在对commit的管理足够规范前提下，如果commit提交足够原子性，并且数量可控，在对一个融合的MR，可以考虑使用cherry-pick，这样可以完成我们一个MR中仅保留部分commit的目的。\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/git/revert-mr-and-merge-again/","summary":"\u003cp\u003e需求分支首先通过MR合并到了\u003ccode\u003emaster\u003c/code\u003e分支，部署后发现有问题，于是回滚代码，操作了\u003ccode\u003eMR\u003c/code\u003e处的\u003ccode\u003erevert\u003c/code\u003e，后面修复了问题，使用原功能分支提新的\u003ccode\u003eMR\u003c/code\u003e时，发现变更处是空的，本文来解决此类无法合并情形的问题。\u003c/p\u003e","title":"gitlab MR被revert后，功能分支无法再次merge到主干分支，怎么办？"},{"content":"年后负责一个比较老的服务，报警不少，今天排查的这个问题也只有在老版本MyBatis下才有机会复现，值此宝贵的线上问题机会，记录一番。\n低版本MyBatis中，在高并发情况下，有概率遇到解析private内部类集合解析抛错的情况。\n我通过分析异常栈+搜索+分析源码的方式解决了此问题。\n背景 直接上我们的异常栈：\n2024-03-01 18:45:28.503 [38728506_441_358] [TID: N/A] ERROR c.l.c.c.t.TestController.batisConcurrentBug$4:1076 - {\u0026#34;bltag\u0026#34;:\u0026#34;log_error\u0026#34;,\u0026#34;errmsg\u0026#34;:\u0026#34;selectByOrderIdsAndContractTypesAndContractStatus::class org.mybatis.spring.MyBatisSystemException:nested exception is org.apache.ibatis.builder.BuilderException: Error evaluating expression \u0026#39;orderIds != null and orderIds.size() \u0026gt; 0\u0026#39;. Cause: org.apache.ibatis.ognl.MethodFailedException: Method \\\u0026#34;size\\\u0026#34; failed for object [690] [java.lang.IllegalAccessException: Class org.apache.ibatis.ognl.OgnlRuntime can not access a member of class java.util.Arrays$ArrayList with modifiers \\\u0026#34;public\\\u0026#34;]\\norg.mybatis.spring.MyBatisExceptionTranslator.translateExceptionIfPossible(MyBatisExceptionTranslator.java:75)\\norg.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:371)\\ncom.sun.proxy.$Proxy69.selectList(Unknown Source)\\norg.mybatis.spring.SqlSessionTemplate.selectList(SqlSessionTemplate.java:198)\\norg.apache.ibatis.binding.MapperMethod.executeForMany(MapperMethod.java:119)\\norg.apache.ibatis.binding.MapperMethod.execute(MapperMethod.java:63)\\norg.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:52)\\ncom.sun.proxy.$Proxy135.selectByOrderIdsAndContractTypesAndContractStatus(Unknown Source)\\nsun.reflect.GeneratedMethodAccessor361.invoke(Unknown Source)\\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\njava.lang.reflect.Method.invoke(Method.java:498)\\norg.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:302)\\norg.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\\norg.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\\ncom.company.fn.mybatis.encrypt.DaoEncryptAspect.invoke(DaoEncryptAspect.java:47)\\norg.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)\\norg.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:208)\\ncom.sun.proxy.$Proxy136.selectByOrderIdsAndContractTypesAndContractStatus(Unknown Source)\\ncom.company.contract.controller.test.TestController.lambda$testMybatisConcurrentBug$4(TestController.java:1073)\\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\\njava.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)\\njava.util.concurrent.FutureTask.run(FutureTask.java)\\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\njava.lang.Thread.run(Thread.java:750)\\n\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;10.33.74.11\u0026#34;,\u0026#34;port\u0026#34;:\u0026#34;8080\u0026#34;,\u0026#34;segmentid\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;line\u0026#34;:0} org.mybatis.spring.MyBatisSystemException: nested exception is org.apache.ibatis.builder.BuilderException: Error evaluating expression \u0026#39;orderIds != null and orderIds.size() \u0026gt; 0\u0026#39;. Cause: org.apache.ibatis.ognl.MethodFailedException: Method \u0026#34;size\u0026#34; failed for object [690] [java.lang.IllegalAccessException: Class org.apache.ibatis.ognl.OgnlRuntime can not access a member of class java.util.Arrays$ArrayList with modifiers \u0026#34;public\u0026#34;] at org.mybatis.spring.MyBatisExceptionTranslator.translateExceptionIfPossible(MyBatisExceptionTranslator.java:75) ~[mybatis-spring-1.2.2.jar:1.2.2] at org.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:371) ~[mybatis-spring-1.2.2.jar:1.2.2] at com.sun.proxy.$Proxy69.selectList(Unknown Source) ~[?:?] at org.mybatis.spring.SqlSessionTemplate.selectList(SqlSessionTemplate.java:198) ~[mybatis-spring-1.2.2.jar:1.2.2] at org.apache.ibatis.binding.MapperMethod.executeForMany(MapperMethod.java:119) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.binding.MapperMethod.execute(MapperMethod.java:63) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:52) ~[mybatis-3.2.8.jar:3.2.8] at com.sun.proxy.$Proxy135.selectByOrderIdsAndContractTypesAndContractStatus(Unknown Source) ~[?:?] at sun.reflect.GeneratedMethodAccessor361.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362] at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:302) ~[spring-aop-4.2.4.RELEASE.jar:4.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190) ~[spring-aop-4.2.4.RELEASE.jar:4.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) ~[spring-aop-4.2.4.RELEASE.jar:4.2.4.RELEASE] at com.company.fn.mybatis.encrypt.DaoEncryptAspect.invoke(DaoEncryptAspect.java:47) ~[fn-commons-utils-1.4.4-SNAPSHOT.jar:?] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) ~[spring-aop-4.2.4.RELEASE.jar:4.2.4.RELEASE] at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:208) ~[spring-aop-4.2.4.RELEASE.jar:4.2.4.RELEASE] at com.sun.proxy.$Proxy136.selectByOrderIdsAndContractTypesAndContractStatus(Unknown Source) ~[?:?] at com.company.contract.controller.test.TestController.lambda$testMybatisConcurrentBug$4(TestController.java:1073) ~[classes/:?] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_362] at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) [?:1.8.0_362] at java.util.concurrent.FutureTask.run(FutureTask.java) [?:1.8.0_362] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_362] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_362] at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362] Caused by: org.apache.ibatis.builder.BuilderException: Error evaluating expression \u0026#39;orderIds != null and orderIds.size() \u0026gt; 0\u0026#39;. Cause: org.apache.ibatis.ognl.MethodFailedException: Method \u0026#34;size\u0026#34; failed for object [690] [java.lang.IllegalAccessException: Class org.apache.ibatis.ognl.OgnlRuntime can not access a member of class java.util.Arrays$ArrayList with modifiers \u0026#34;public\u0026#34;] at org.apache.ibatis.scripting.xmltags.OgnlCache.getValue(OgnlCache.java:47) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.ExpressionEvaluator.evaluateBoolean(ExpressionEvaluator.java:32) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.IfSqlNode.apply(IfSqlNode.java:33) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.MixedSqlNode.apply(MixedSqlNode.java:32) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.DynamicSqlSource.getBoundSql(DynamicSqlSource.java:40) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.mapping.MappedStatement.getBoundSql(MappedStatement.java:278) ~[mybatis-3.2.8.jar:3.2.8] at com.company.hawkeye.client.mybitis.CatMybatisPlugin.intercept(CatMybatisPlugin.java:70) ~[hawk-client-1.5.6.jar:?] at org.apache.ibatis.plugin.Plugin.invoke(Plugin.java:60) ~[mybatis-3.2.8.jar:3.2.8] at com.sun.proxy.$Proxy296.query(Unknown Source) ~[?:?] at sun.reflect.GeneratedMethodAccessor475.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362] at org.apache.ibatis.plugin.Invocation.proceed(Invocation.java:49) ~[mybatis-3.2.8.jar:3.2.8] at com.company.ctt.metrics.core.instrument.mybatis.MyBatisMetricsInterceptor.recordRequest(MyBatisMetricsInterceptor.java:55) ~[metrics-dog-core-2.1.9.31.jar:2.1.9.31] at com.company.ctt.metrics.core.instrument.mybatis.MyBatisMetricsInterceptor.intercept(MyBatisMetricsInterceptor.java:33) ~[metrics-dog-core-2.1.9.31.jar:2.1.9.31] at org.apache.ibatis.plugin.Plugin.invoke(Plugin.java:60) ~[mybatis-3.2.8.jar:3.2.8] at com.sun.proxy.$Proxy296.query(Unknown Source) ~[?:?] at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:108) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:102) ~[mybatis-3.2.8.jar:3.2.8] at sun.reflect.GeneratedMethodAccessor405.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362] at org.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:358) ~[mybatis-spring-1.2.2.jar:1.2.2] ... 23 more Caused by: org.apache.ibatis.ognl.MethodFailedException: Method \u0026#34;size\u0026#34; failed for object [690] at org.apache.ibatis.ognl.OgnlRuntime.callAppropriateMethod(OgnlRuntime.java:837) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.ObjectMethodAccessor.callMethod(ObjectMethodAccessor.java:61) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.OgnlRuntime.callMethod(OgnlRuntime.java:860) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.ASTMethod.getValueBody(ASTMethod.java:73) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:170) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.SimpleNode.getValue(SimpleNode.java:210) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.ASTChain.getValueBody(ASTChain.java:109) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:170) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.SimpleNode.getValue(SimpleNode.java:210) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.ASTGreater.getValueBody(ASTGreater.java:49) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:170) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.SimpleNode.getValue(SimpleNode.java:210) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.ASTAnd.getValueBody(ASTAnd.java:56) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:170) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.SimpleNode.getValue(SimpleNode.java:210) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.Ognl.getValue(Ognl.java:333) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.ognl.Ognl.getValue(Ognl.java:310) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.OgnlCache.getValue(OgnlCache.java:45) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.ExpressionEvaluator.evaluateBoolean(ExpressionEvaluator.java:32) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.IfSqlNode.apply(IfSqlNode.java:33) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.MixedSqlNode.apply(MixedSqlNode.java:32) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.scripting.xmltags.DynamicSqlSource.getBoundSql(DynamicSqlSource.java:40) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.mapping.MappedStatement.getBoundSql(MappedStatement.java:278) ~[mybatis-3.2.8.jar:3.2.8] at com.company.hawkeye.client.mybitis.CatMybatisPlugin.intercept(CatMybatisPlugin.java:70) ~[hawk-client-1.5.6.jar:?] at org.apache.ibatis.plugin.Plugin.invoke(Plugin.java:60) ~[mybatis-3.2.8.jar:3.2.8] at com.sun.proxy.$Proxy296.query(Unknown Source) ~[?:?] at sun.reflect.GeneratedMethodAccessor475.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362] at org.apache.ibatis.plugin.Invocation.proceed(Invocation.java:49) ~[mybatis-3.2.8.jar:3.2.8] at com.company.ctt.metrics.core.instrument.mybatis.MyBatisMetricsInterceptor.recordRequest(MyBatisMetricsInterceptor.java:55) ~[metrics-dog-core-2.1.9.31.jar:2.1.9.31] at com.company.ctt.metrics.core.instrument.mybatis.MyBatisMetricsInterceptor.intercept(MyBatisMetricsInterceptor.java:33) ~[metrics-dog-core-2.1.9.31.jar:2.1.9.31] at org.apache.ibatis.plugin.Plugin.invoke(Plugin.java:60) ~[mybatis-3.2.8.jar:3.2.8] at com.sun.proxy.$Proxy296.query(Unknown Source) ~[?:?] at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:108) ~[mybatis-3.2.8.jar:3.2.8] at org.apache.ibatis.session.defaults.DefaultSqlSession.selectList(DefaultSqlSession.java:102) ~[mybatis-3.2.8.jar:3.2.8] at sun.reflect.GeneratedMethodAccessor405.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362] at org.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:358) ~[mybatis-spring-1.2.2.jar:1.2.2] ... 23 more at org.mybatis.spring.SqlSessionTemplate$SqlSessionInterceptor.invoke(SqlSessionTemplate.java:358) ... 23 more Caused by: org.apache.ibatis.ognl.MethodFailedException: Method \u0026#34;size\u0026#34; failed for object [690] [java.lang.IllegalAccessException: Class org.apache.ibatis.ognl.OgnlRuntime can not access a member of class java.util.Arrays$ArrayList with modifiers \u0026#34;public\u0026#34;] at org.apache.ibatis.ognl.OgnlRuntime.callAppropriateMethod(OgnlRuntime.java:837) at org.apache.ibatis.ognl.ObjectMethodAccessor.callMethod(ObjectMethodAccessor.java:61) at org.apache.ibatis.ognl.OgnlRuntime.callMethod(OgnlRuntime.java:860) at org.apache.ibatis.ognl.ASTMethod.getValueBody(ASTMethod.java:73) at org.apache.ibatis.ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:170) at org.apache.ibatis.ognl.SimpleNode.getValue(SimpleNode.java:210) at org.apache.ibatis.ognl.ASTChain.getValueBody(ASTChain.java:109) at org.apache.ibatis.ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:170) at org.apache.ibatis.ognl.SimpleNode.getValue(SimpleNode.java:210) at org.apache.ibatis.ognl.ASTGreater.getValueBody(ASTGreater.java:49) at org.apache.ibatis.ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:170) at org.apache.ibatis.ognl.SimpleNode.getValue(SimpleNode.java:210) at org.apache.ibatis.ognl.ASTAnd.getValueBody(ASTAnd.java:56) at org.apache.ibatis.ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:170) at org.apache.ibatis.ognl.SimpleNode.getValue(SimpleNode.java:210) at org.apache.ibatis.ognl.Ognl.getValue(Ognl.java:333) at org.apache.ibatis.ognl.Ognl.getValue(Ognl.java:310) at org.apache.ibatis.scripting.xmltags.OgnlCache.getValue(OgnlCache.java:45) ... 45 more 分析 通过异常栈，可以快速定位到报异常的源码。加以搜索报错关键字，不难定位到MyBatis官方issue： https://github.com/mybatis/mybatis-3/pull/384\n在stackoverflow中已经有人给出了解法： https://stackoverflow.com/questions/36961953/mybatis-error-org-apache-ibatis-ognl-ognlruntime\n简要概括下问题： 此bug为OGNL 2.7 + MyBatis 3.2.X 版本的问题，当给Mapper传入private内部类型的集合时，调用private内部类中的方法由于并发，会出现反射调用方法的问题。\n详细的解释有一位网友写的不错：MyBatis居然也有并发问题，我这里就不再赘述细节。\n针对问题，我也做了一版分析，画图说明下问题发生的过程： 图中标示了核心的类以及调用链路，更详细的类大家可以参考异常栈。\n在第三个流程中，OgnlRuntime用来实现反射调用入参类型的属性方法。由于传入的Method在OgnlRuntime中是内部缓存中的同一个引用，当多个线程并发执行时，会出现多线程同时修改同一Method同一属性的时序问题，如图：\n对应源码： 问题复现 分析清楚原因，我尝试在本地复现了一下这种线上低频出现的并发问题：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @RequestMapping(\u0026#34;testMybatisConcurrentBug\u0026#34;) @ResponseBody public Object testMybatisConcurrentBug() { ExecutorService executor = new ThreadPoolExecutor(10, 10, 100L, TimeUnit.MILLISECONDS, new SynchronousQueue\u0026lt;\u0026gt;(), new ThreadPoolExecutor.AbortPolicy()); for (int j = 0; j \u0026lt; 100; j++) { executor.submit(() -\u0026gt; { try { // 此处传入 Arrays.asList Collections.singletonList 这类`private`内部类式的容器映射均可 List\u0026lt;DO\u0026gt; retList = yourMapper.selectBylist(Arrays.asList(690L), Arrays.asList(ContractTypeEnum.HT.getValue()), Arrays.asList(ContractStatusEnum.GZ.getValue())); System.out.println(retList); } catch (Exception e) { logger.error(\u0026#34;selectBylist::\u0026#34;, e); e.printStackTrace(); } }); } return \u0026#34;testMybatisConcurrentBug 启动\u0026#34;; } 还可以本地确认一下反射类的修饰符、方法访问性：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public static void main(String[] args) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException { // 创建一个Arrays.asList返回的List实例 List\u0026lt;Long\u0026gt; list = Arrays.asList(666L); // 获取ArrayList实例的Class对象 Class\u0026lt;?\u0026gt; clazz = list.getClass(); // 获取size()方法 Method method = clazz.getMethod(\u0026#34;size\u0026#34;); // 检查这个方法确实是存在于ArrayList或者其超类中的 if (List.class.isAssignableFrom(clazz)) { if (!Modifier.isPublic(method.getDeclaringClass().getModifiers())) { System.out.println(\u0026#34;目标类的修饰符不是public！！！\u0026#34;); } if (!method.isAccessible()) { System.out.println(\u0026#34;目标方法不可直接访问！！！\u0026#34;); method.setAccessible(true); } // 调用size()方法 int size = (Integer) method.invoke(list); System.out.println(\u0026#34;The size of the list is: \u0026#34; + size); method.setAccessible(false); } else { System.out.println(\u0026#34;The provided object is not a List.\u0026#34;); } } 使用如上测试脚本可以稳定复现，佐证了我们前面的分析正确性。\n解决 问题只要分析、复现清楚，解决方案就很简单：\n可以将项目MyBatis版本升级到3.3.X； 由此引入依赖的问题，可考虑多环境灰度观察进行平稳上线； 也可以将涉及到报错的Mapper处入参，修改为java.util.ArrayList类型； 梳理了下，光这一个mapper方法就涉及20+调用处； 经过沟通权衡，我们选择了升级MyBatis版本（改动小、版本差异不大）。升级后的OgnlRuntime： 可以看到修改method的可访问性增加了synchronized，保证其多线程操作下的原子性、结果一致性。\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/mybatis/ognl-2.7-concurrent-bug/","summary":"\u003cp\u003e年后负责一个比较老的服务，报警不少，今天排查的这个问题也只有在老版本\u003ccode\u003eMyBatis\u003c/code\u003e下才有机会复现，值此宝贵的线上问题机会，记录一番。\u003c/p\u003e\n\u003cp\u003e低版本\u003ccode\u003eMyBatis\u003c/code\u003e中，在高并发情况下，有概率遇到解析\u003ccode\u003eprivate\u003c/code\u003e内部类集合解析抛错的情况。\u003c/p\u003e\n\u003cp\u003e我通过分析异常栈+搜索+分析源码的方式解决了此问题。\u003c/p\u003e","title":"排查mybatis ognl解析参数值并发设置字段访问性报错问题一例"},{"content":"排查一则生产环境报 ConcurrentModificationException 的问题。\n在相对复杂的工业代码中（绕来绕去，不是简单的一个函数调用），排查「简单问题」也不是一件简单的事。\n记录本文的原因：\n个人觉得有点意思（头一回线上碰到这个异常）； 此类排查过程对个人有些许收获； 背景 线上报警，对应异常栈：\n1 2 3 4 5 6 7 8 nullclass java.util.ConcurrentModificationException:null java.util.HashMap$HashIterator.nextNode(HashMap.java:1442) java.util.HashMap$EntryIterator.next(HashMap.java:1476) java.util.HashMap$EntryIterator.next(HashMap.java:1474) SMSServiceImpl.send(SMSServiceImpl.java:101) SendSmsEmailService.sendSmsWithTemplate(SendSmsEmailService.java:55) VerifyCodeService.sendVerifyCode(VerifyCodeService.java:103) ContractController.sendVerifyCodeForSignPermission 分析 ConcurrentModificationException 表示对集合的操作出现了并发修改异常【我在遍历集合元素的时候，有人改了集合元素，导致我遍历获取下个元素出错】。\n既然异常栈都给出来了，那么我们去看下代码。这里跳过代码详细截图，下图是我梳理出的脱敏流程以及关键代码： 简单解释下这个过程： 在 VerifyCodeService.sendVerifyCode 方法中首先构建了一个 vars map，然后丢进了一个线程池中发邮件，同时主线程开始发送短信。 在发邮件的操作中，经过层层透传，map引用被传到了Velocity模板中，其中对map有 put remove 等增删操作。\n问题很明显了，两个线程，一个遍历读，一个并发写，自然就会抛 ConcurrentModificationException。\n解决 解决方式想到了两种：\nMap 传入子方法时，拷贝一个新的对象实例； Map使用 ConcurrentHashMap 类型； 对比之下，此处的场景中我倾向于在向线程池传入map时拷贝一个新的对象，优点如下：\n开销不会太大（map仅承载了邮件、短信的消息、收发人信息）； 数据能否确保多线程间的数据一致性，异步流程仅使用快照，对主线程无任何影响； 语义明确，此时我就是要在主线程主流程代码中能看清楚，往线程池传的是一个快照； 使用 ConcurrentHashMap 也可以修复此问题，但我觉得有如下缺点：\n此场景无需使用 ConcurrentHashMap 对并发操作的实时性，异步线程改了数据其实在主线程中完全用不到； Api 相较 HashMap 复杂，容易踩坑新的问题； 踩的坑 排查这个问题最开始我陷入了第一个子流程中的代码（对应上图中的红圈1），盯着发短信的代码中遍历的片段迟迟不得其解。\n直到我出去上了个厕所，点出去看了下 VerifyCodeService.sendVerifyCode 中的整体流程，才发现发短信使用了异步的线程池。此时才打开了正确的大门。 当然进入这个方法，问题也没有那么快解开，正如我图中所画，第二个子流程（对应上图中的红圈2）中代码嵌套很深，层层进入分析了片刻，才找到了修改原map引用的具体位置。\n这里对我的启发：\n排查问题切不可陷入细节，要先有全局的视野，再有细节； 总结 此并发修改异常源于我们的问题代码中使用了线程池负责发短信子流程，其中对入参的map进行了并发修改，导致主线程中遍历map时出现了不一致的情况。\n问题也好修复，如上所说，我推荐传入线程池时拷贝一个新的map实例。\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/java/concurrent-modification-exception/","summary":"\u003cp\u003e排查一则生产环境报 \u003ccode\u003eConcurrentModificationException\u003c/code\u003e 的问题。\u003c/p\u003e\n\u003cp\u003e在相对复杂的工业代码中（绕来绕去，不是简单的一个函数调用），排查「简单问题」也不是一件简单的事。\u003c/p\u003e","title":"排查 ConcurrentModificationException 问题一例"},{"content":"本文尝试归纳整理作者所了解的IO基础模式、IO架构通用模式，尽可能一篇文章提供IO相关的全景图以及方案优化对比。\n本文使用黑夜模式阅读体验更佳。\n如无特殊说明，本文使用的术语遵从Linux、Java的习惯。\nIO基础概念 Buffer Buffer 是字节流在内存中的表示，是一种数据结构。\n针对读写模式的不同，Java中使用同样的字段不同的使用方式。如图： Channel Channel 则是负责读写IO动作的组件，对接OS层面的文件、网络模块，数据读写到内存中则用Buffer表示。\nScattering Read \u0026amp; Gathering Write 分散读：一次读调用，从多个IO源中获取数据。\n聚集写：一次写调用，完成多个IO目的地的写入。\nSelector Selector组件是IO多路复用算法的核心组件，单线程（或多线程）单个Selector组件即可完成多个IO数据就绪的监听。\nPipe Pipe表示两个Channel间的单向连接。\nIO基础模式 介绍模式之前，强调一些基础背景：\n我们的服务端程序属于用户态； 操作系统属于内核态； 当程序向操作系统发起IO时，操作系统会经历以下阶段： 数据准备； 从物理设备，通过DMA，复制到内核缓冲区（Kernel Buffer）； 数据就绪； 数据复制（从内核空间复制到用户空间）； 从内核缓冲区，通过CPU，复制到用户空间缓冲区； 数据准备与复制两个阶段的核心作用如图： IO的基础模式解决的是统一抽象了单个IO的流程模式，基础模式是高级模式的基础。\n同步阻塞IO 对于同步阻塞IO来说，从应用程序发起IO调用（图中的read()）后会一直等待，直到操作系统完成数据准备、就绪、复制等动作。\n同步非阻塞IO 对于同步非阻塞IO来说，从应用程序发起IO调用（图中的read()）后，如果此时数据还未就绪，则该干啥干啥去，稍后片刻后再发起read()调用检查数据是否就绪。\n当数据就绪后，应用程序发起的read()会阻塞获取到结果。\n同步非阻塞模式解决了同步阻塞模式下的一直阻塞等待数据准备结果的效率问题。\n异步IO 对于异步IO来说，应用程序发起read()后即时返回，后续数据就绪后，由操作系统通知应用程序结果。\n异步模式提高了同步非阻塞模式下需要多次检查数据准备结果的效率。\nIO多路复用 对于IO多路复用模式，使用一个Selector组件统一接管了多个IO请求的数据准备状态的检查工作。\nIO多路复用技术关键点：\n多条连接共用一个阻塞对象，无需轮询所有连接对象，常见实现方案有 select、epoll、kqueue； 当某条连接有数据响应时，OS通知进程，进而做业务处理； IO高性能模式 理解掌握了IO的基础模式，我们基本可以解决单个IO、简单场景IO的问题。\n但面对工业系统面临的问题时，比如系统承载连接变多、用户并发请求数升高、资源利用率，我们需要做更多的设计，因此需要基于基础模式进行高性能模式的探索。\nPPC: Process Per Connection 一个连接启动一个进程处理，是传统UNIX网络服务器最初始的模式。\n特点：\nfork开销大； 跨进程通信复杂； 支持的并发连接数有限； PPC: prefork 优化1：prefork 进程预热\n系统启动时预先创建好子进程。\n特点：\n通过提前创建子进程优化了原PPC模式下fork开销大的问题； 其他问题依然存在； TPC: Thread Per Connection 一个连接启动一个线程处理。\n特点：\n解决了PPC fork开销大的问题，线程创建开销更小； 但是创建线程的开销依然很大； 解决了跨进程通信复杂的问题，多线程共享了进程内存空间，通信效率较高； 但是线程通信也需要同步，引入了死锁的问题； 多线程间可能会互相影响，比如某个线程出现内存越界时，会导致整个进程退出； TPC: prethread 优化1：prethread 线程预热\n预先创建线程。\n特点：\n相比PPC的prefork更加灵活； 案例：Apache的MPM Worker，多进程+多线程模式，默认支持了 16*25=400 个并发处理线程； PPC、TPC的局限性分析 实现简单，但是无法支持高并发场景。\n问题分析： 进程、线程在不使用的情况下就要销毁，可以考虑使用池化的方式进行资源复用； 单进程处理多个连接，如果多个连接都进行阻塞监听，则有后续连接因前序连接阻塞无法处理的问题，可以考虑使用非阻塞轮询的方式改进，如果连接过多，则轮询的开销也会加大，进一步，可以考虑使用IO多路复用进行改进； 改进方案： 根据以上方面的分析，方案无非是如下三种：\n连接资源池化； 非阻塞IO； IO多路复用； 相比PPC、TPC，下方的Reactor、Proactor模式更擅长应对高并发。\nReactor模式 由上部分的PPC、TPC的局限性分析我们已经得出了改进的方案，而 IO多路复用 + 线程池 = Reactor模式（等同于Dispatcher）。\nReactor架构模式是主流中间件使用的方案！\nReactor我们可以细分为三种模式：\n单Reactor单进程、线程； 单Reactor多线程； 多Reactor多进程、线程； 我们选取其中常用的方案进行优缺点、结构分析。\n单Reactor单进程、线程 优点：\n实现简单； 没有跨进程通信； 没有跨进程资源竞争； 缺点：\n无法利用多核CPU的优势； Handler只有一个进程（线程），当前序任务处理过久时，handler会成为后续任务的瓶颈； 适用场景：业务场景处理快速的场景。\n使用案例：Redis。\n单Reactor多线程 优点：\n可充分利用多核CPU的优势； 缺点：\n多线程数据同步较复杂； 单个Reactor负责所有请求的处理，可能成为瓶颈； Proactor IO操作+业务操作完全异步化 = Proactor模式。\n优点：\n全链路异步化，更高效； 缺点：\n实现复杂； 目前仅Windows系统通过IOCP技术提供了实现，Linux下暂不支持全异步的AIO； 总结 本文介绍了IO的基础概念、基础模式、高性能模式三大板块。\n其中基础模式中，我们将空间明确区分为了内核空间、用户空间。当应用层发起IO调用后，操作系统需要处理数据准备、数据就绪、数据复制三个阶段的工作。 从图例中，可以清晰地判断何为同步、异步，何为阻塞、非阻塞。\n弄清楚基本概念与基础模式后，我们过了一遍主流中间件使用的高性能架构模式。其中Reactor模式最为重点，请重点关注。\n每一个架构模式的设计均是为了解决上一个模式的缺点。\n而解决问题的思路是相似的：\n资源预处理； 非阻塞IO（NIO）； 资源池化； IO多路复用； 异步化（AIO）； 学习、运用本文相关知识，我觉得两点最为关键：\n梳理过程要清晰，配合图例、流程图、文字可以更好地理解一块领域的知识； 解决问题的思路一般不需要重新发明，因此要了解业界、经典著作中的已有方案与解决问题的过程； 以上。\nRef Jakob Jenkov Java NIO 教程 李号双 深入拆解 Tomcat \u0026amp; Jetty 李运华 从 0 开始学架构 ","permalink":"https://redolog.github.io/posts/rd/design/io/all-about-io/","summary":"\u003cp\u003e本文尝试归纳整理作者所了解的\u003ccode\u003eIO\u003c/code\u003e基础模式、\u003ccode\u003eIO\u003c/code\u003e架构通用模式，尽可能一篇文章提供\u003ccode\u003eIO\u003c/code\u003e相关的全景图以及方案优化对比。\u003c/p\u003e","title":"关于IO我所知道的一切都在这里了"},{"content":" 我是谁？ 我叫宋惠龙，网名一般是DragonSong，我是一名政法学院毕业的后端工程师。\n生于山西平遥，求学于广东佛山，现在蚂蚁国际搬砖，毕业以来一直从事业务后端研发领域的工作。\n为什么写博客？ 写博客的初衷很简单：记录一些自己当前阶段的知识，帮助自己学习巩固知识，帮助自己思考。\n经过多次思考与调整，我现在对博客的要求：只要具备 [实用、思考、系统] 特点之一，均可输出。\n退一步讲，博客不应设定太多要求，只要我开心想写即可。\n我的人生战略 工作经历 蚂蚁集团\n努力搬砖中\n贝壳找房\n公寓直连酒店（房源+交易+营销）、普租合同、IoT后端工程师\n主要负责：核心研发、团队管理（IoT方向）、异地招聘\n小米集团\n新零售销服B2B商家、订单系统后端工程师\n主要负责：核心研发、团队组建\n图匠数据\nKA项目（蒙牛、达能、联合利华）、图像算法平台后端工程师\n主要负责：项目研发攻关、平台建设、老带新\n教育经历 佛山大学\n2013.9-2017.6\n管理学学士-公共事业管理\n能力认证 小米面试官认证\n从图匠时代开始，承蒙老板提供机会，我开始了做面试官的经历，在小米时期拿到了组织认定的专业认证，在贝壳时期也帮助成都团队招聘了部分新成员\n主要产出：小米公益平台团队组建、贝壳成都团队组建\n51talk口语水平认证\n主要产出：51 talk 口语 10 级水平\n兴趣爱好 爱猫人士 阿杜\n阿杜出生于深圳，后待过北京、杭州，见多识广\n七七\n七七出生于北京，后待过杭州，脸皮厚吃得开\n其他 业余骑行爱好者\n路线是广州南沙20km左右的一个自由路线，主要是平路，稍有爬升\n业余爬山徒步爱好者\n路线是杭州九溪十八涧\n如何找到我？ 您可以通过如下渠道找到我：\nGithub: https://github.com/redolog Wechat: dragonsong1024 ","permalink":"https://redolog.github.io/about/","summary":"我是谁？ 我叫宋惠龙，网名一般是DragonSong，我是一名政法学院毕业的后端工程师。 生于山西平遥，求学于广东佛山，现在蚂蚁国际搬砖，毕业以","title":"宋惠龙的自我介绍"},{"content":"此文统一整理操作系统零拷贝相关图例。\n其中红色箭头表示上下文切换动作。其余动作均有文字说明。\n针对 Scattering、Gathering 的附加说明： ","permalink":"https://redolog.github.io/posts/rd/linux/zero-copy/legend/","summary":"\u003cp\u003e此文统一整理操作系统零拷贝相关图例。\u003c/p\u003e","title":"Linux零拷贝图例一览"},{"content":"本文记录做酒店业务系统开发时遇到的算法实例：计算酒店产品月租价。\n业务算法系列：收录业务系统开发中真实遇到的算法\n背景 还是之前做酒店业务期间，我司酒店针对的是月租的场景，因此C端外展的价格是月租价。\n月租价定义：从当天入住开始计算，碰到第一个连续30天可开房的区间，则累加30天的价格，否则继续查找。\nB端供给侧拉到的ARI酒店商品数据格式是这样的：\nARI: 房价，房量，房态（Availability，Rate，Inventory） 针对酒店商品的具体售卖信息，包含价格Rate，库存Inventory，可用性Availability http://docs.huazhu.com/crs/api/distributor/docs/v1/common/contract\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 /** * 酒店商品 */ public class HotelAri { /** * 日期 */ private Date ariDay; /** * 价格 */ private BigDecimal price; /** * 库存 */ private Integer inventory; } 因此简化后的问题转化为了一个方法：\n1 public BigDecimal calculateMonthPrice(List\u0026lt;HotelAri\u0026gt; ariList) 我们这里定义 ariList 数据量为固定的60，并且保证输入的ariList是根据日期连续递增的序列。（无需再排序）\n求解 我们画图表达一下题目： 所求应该很清晰了：给定60个连续的库存、价格数据，求其中第一段连续30天有库存的价格和。\n粗暴解 首先来段粗暴解法，直接逐个展开每个子区间，判断30连续有库存是否成立。\n拿到入参，依次遍历每个ari，每次都往后找一个30天的区间，如果有库存0的ari，则判断下一个ari。 代码也很简单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public BigDecimal calculateMonthPrice(List\u0026lt;HotelAri\u0026gt; ariList) { int n = ariList.size(); for (int i = 0; i \u0026lt;= n - 30; i++) { BigDecimal monthPrice = BigDecimal.ZERO; for (int j = i; j \u0026lt; 30 + i; j++) { HotelAri currAri = ariList.get(j); if (currAri.getInventory() \u0026lt;= 0) { // 如果库存为0，跳出当前循环，寻找下一个连续30天的区间 break; } monthPrice = monthPrice.add(currAri.getPrice()); if (j == 29 + i) { // 如果连续30天都有库存，返回这30天的价格 return monthPrice; } } } // 如果没有找到连续30天有库存的区间，返回0 return BigDecimal.ZERO; } 等同于如下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public BigDecimal calculateMonthPrice3(List\u0026lt;HotelAri\u0026gt; ariList) { int n = ariList.size(); int monthDays = 30; for (int i = 0; i \u0026lt; n; i++) { int rangeCnt = 0; BigDecimal monthPrice = BigDecimal.ZERO; for (int j = i; j \u0026lt; n; j++) { rangeCnt++; HotelAri currAri = ariList.get(j); if (currAri.getInventory() \u0026lt;= 0) { break; } monthPrice = monthPrice.add(currAri.getPrice()); if (rangeCnt == monthDays) { return monthPrice; } } } return BigDecimal.ZERO; } 整体的时间复杂度为O(n^2)，原因是使用了双层嵌套循环。这个效率肯定是可以提升的。\n滑动窗口优化解 其实我们在每个子区间判断的时候，碰到30天内无库存的情况，可以直接跳跃到下一个有库存的地方继续，不需要重复的判断。\n姑且，我们将子区间叫做滑动窗口，窗口经过每个元素只处理一次。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public BigDecimal calculateMonthPrice2(List\u0026lt;HotelAri\u0026gt; ariList) { int n = ariList.size(); int monthDays = 30; int l = 0, r = l; for (; l \u0026lt; n; l++) { BigDecimal monthPrice = BigDecimal.ZERO; while (r \u0026lt; n \u0026amp;\u0026amp; r \u0026lt; l + monthDays) { HotelAri currAri = ariList.get(r); if (currAri.getInventory() \u0026lt;= 0) { l = r + 1; r = l; break; } // 此时r有库存 monthPrice = monthPrice.add(currAri.getPrice()); if (r - l + 1 == monthDays) { return monthPrice; } r++; } } // 如果没有找到连续30天有库存的区间，返回0 return BigDecimal.ZERO; } public BigDecimal calculateMonthPrice4(List\u0026lt;HotelAri\u0026gt; ariList) { int monthDays = 30; int validCnt = 0; BigDecimal monthPrice = BigDecimal.ZERO; for (HotelAri currAri : ariList) { if (currAri.getInventory() \u0026lt;= 0) { validCnt = 0; monthPrice = BigDecimal.ZERO; continue; } validCnt++; monthPrice = monthPrice.add(currAri.getPrice()); if (validCnt == monthDays) { return monthPrice; } } return BigDecimal.ZERO; } 每个元素仅一层循环遍历一次，时间复杂度从O(n^2)提升到了O(n)。\n小结 在酒店产品连续30天有库存的第一个段求解价格的和问题中，我们尝试了两种解法：\n暴力解； 滑动窗口解； 其中滑动窗口解法的核心也很简单：对于前序判断过的元素不做重复判断（此段不符合条件，下一段肯定也不符合，因此不需要重复判断）。\n可以看到业务研发中碰到的例子，算法并不复杂。\n另外在入参数据量较小的情况下，两种解法的运行效率并无差异。\n解决、记录此类问题，主要价值还是梳理问题、表达思路、培养算法思维（效率意识）。\n","permalink":"https://redolog.github.io/posts/rd/algo/case/calculate-hotel-month-price/","summary":"\u003cp\u003e本文记录做酒店业务系统开发时遇到的算法实例：计算酒店产品月租价。\u003c/p\u003e","title":"算法案例：计算酒店月租价"},{"content":"本文使用策略模式和模板方法模式来优化处理支付回调的代码结构，记录设计模式应用实例。\n背景 hotel-zuwu 服务负责酒店订单履约、租约账单流转、支付、合同签约、分账等领域业务。\n其中与支付中台（集团金融平台）交互的流程如下：\n可以看到当收到支付中台回调后，需要做一系列的判断逻辑。\n项目最初上线时，仅需考虑用户付款即平台收入的场景，因此回调接口中在一个service方法中处理，高效且清晰。\n随着需求迭代，这里引入了用户退款即平台支出以及其他的场景，同时这块代码有初步开发完就交接的情况，因此代码结构、逻辑变得稍显混乱。\n优化思路 1. 先分析优化前的代码 回调入口：\n支付成功与取消入口： 可以看到，判断逻辑主要两个大分支：\n收入或支出 支付成功或取消 这里的问题在于，在每个具体处理的方法中，可能都需要判断彼此两个分支。在内部某个方法中，很可能遗漏某段逻辑。\n整理下这里的分支，其实也就四种情况：\n收入支付成功 收入支付取消 支出支付成功 支出支付失败 2. 再思考多分支情况的优化解法 针对分支多的情况，如果分支是一些离散的值，可以使用hashmap来替代分支语句。\n如果分支在代码中表示的是不同的处理逻辑，可以考虑策略模式来优化代码间的协作结构。\n因此，我们尝试使用策略模式来优化。上述四种情况正好对应四种策略。\n策略模式是一种行为型设计模式，主要解决的问题是如何在运行时根据不同情况选择算法或行为。它允许定义一组算法（策略），将它们封装成对象，然后在运行时根据需要动态地选择要执行的策略。这种模式有助于减少大量的条件语句和分支结构，提高代码的可维护性和可扩展性。\n每个策略中除了处理回调逻辑，还可以维护分支判断的状态，比如「收入支付成功」「收入支付取消」，这样我们可以直接用一个 List 注入所有策略类，增加一个 support 方法用于判断当前策略属于何种场景。\n动手改造 定义通用骨架 我们首先使用模板方法模式来定义通用的骨架，抽取上述状态判断、运行逻辑：\n实现具体策略 然后实现前面分析到的四种分支对应的Handler，support中定义分支的条件，realHandle中则定义分支下的具体处理逻辑。\n以收入成功的分支为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class PaymentSuccessHandler extends AbstractPaymentCallbackHandler { @Resource private PaymentFacade paymentFacade; @Override protected boolean support(CommonCallbackReq callbackReq, RentPaymentBill paymentBill) { return 支付类型为收入 \u0026amp;\u0026amp; 回调状态为成功; } @Override protected void realHandle(CommonCallbackReq callbackReq, RentPaymentBill paymentBill) { // 更新流水 // 更新账单 // 外部酒店供应商下单 } } 新代码接入 由于回调入口的地方已经足够清晰，因此维持原状，这里我们仅将优化好的策略模式插入到回调处：\n效果 最后看下改造完的结构：\n可以看到，收入、支出、支付成功、支付取消 一目了然。增加逻辑再也不用担心会漏哪个场景了。\n小结 本文，我们探讨了一个与支付回调流程相关的代码优化问题。\n面对代码的些许混乱，我通过如下操作进行了改进：\n首先分析项目背景、业务流程以及代码逻辑； 针对较为明显且核心的问题，思考应对其较合适的解法； 然后尝试编码改造； 最终评估效果：代码结构更加清晰、后续迭代的效率也得到了提升【如果这一步效果不佳，回到第二步继续】； 本案例对应的 code base 是一个较新的代码工程，因此历史债不重，优化工作相对容易。\n但价值是明显的：\n改造对工程代码质量有实际提升； 思考、实践经验可复用； ","permalink":"https://redolog.github.io/posts/rd/design/pattern/case/pay-callback/","summary":"\u003cp\u003e本文使用策略模式和模板方法模式来优化处理支付回调的代码结构，记录设计模式应用实例。\u003c/p\u003e","title":"使用策略模式优化支付回调代码一例"},{"content":"协助同事排查处理了一则aviator使用不当导致元空间内存泄漏的问题，个人觉得是一则很有价值的案例，因此观摩学习一波。\n使用aviator不启用表达式缓存的情况下，会分别在编译、解析阶段根据时间戳使用asm动态生成class，生成过多class就会撑爆元空间，也就是内存泄漏。\n除此之外，在分析过程中还发现了aviator低版本的另一个泄漏问题，非常适合用来巩固jvm相关知识。\n本文结构请参照目录。\n背景 报警 起因是12号的时候有个报警，显示发生了OOM，同时携带了metaspace元空间关键字： 看下对应服务元空间监控趋势： 这里可以观察到元空间在一定周期内上涨，同时也有一些无周期规律的上涨趋势。\njvm参数 对应服务使用了jdk 1.8\n服务启动参数：\n-XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m 分析 前面可以看到元空间 meta space 内存占用呈上涨趋势；\n因为元空间存的是类的描述、元信息，本身我们的类基本是固定的，除此之外就是一些框架动态生成的类，但是这部分增长不应该这么快，因此这里很可疑！！！\n通过增加jvm 启动参数-XX:+TraceClassLoading -XX:+TraceClassUnloading，可以查看类加载的情况，比如加载了多少类，同时可以看类加载的变化趋势。\n在日志中捕捉到了 Script_前缀 这种可疑的类（生成了好多）。\n如图： 通过类能定位到对应的三方库：\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.googlecode.aviator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aviator\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.2.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 可以看到，这里加载了很多前缀相同的类。加载的类多，元空间中类的元信息也暴增，同时堆内对象也暴增，因此服务内存紧张，触发fullgc，当gc后依然生成超多类时，jvm撑不住了，触发OOM。\n问题基本定位到了，接下来就需要研究下问题在aviator是如何发生的，以及怎么解决问题。\nScript_前缀类 加载很多的原因，我们需要一步步debug分析下源码：\n问题1：未使用缓存，每次调用均生成新代码 业务代码中只有一个地方调用了aviaotr，因此入口很好找，定位到调用aviator的位置：\ncompile继续往里走：\ninnerCompile负责编译、解析新的表达式，如果这一步复用缓存，则从缓存中取前序已经处理好的表达式。\n可以看到这里一共做了：\n词法分析； 动态代码生成； 这里会动态生成 Script_ 前缀的脚本类； 表达式解析； 这里会动态生成 Lambda_ 前缀的匿名内部类； 一直debug会到executeModule这一步： 可以看到这里维护了lambda类的一些描述信息，具体生成过程可参看下方折叠内容，总结下就是先生成Script_ 前缀的脚本类，然后脚本类中动态生成Lambda_ 前缀的匿名内部类，这些类正常情况下在后面不用的时候需要释放掉对应内存。\n具体的调用链路折叠在此处，需要可以细看： Object executeDirectly(final Map\u0026lt;String, Object\u0026gt; env) LambdaFunction newLambda(final Env env, final String name) LambdaFunction newInstance(final Env env)\n上述路径需在 最后一步断点，不断回滚，才能找到进入路径。因为真实进入的代码是生成的Script_调用的。\n通过断点可以观察到，使用缓存后，这里使用的 LambdaFuntion 是同一个，不会重复生成新的类。（defineClass） 小结： 这里我们已经可以得出aviator在不使用缓存的情况下，每次调用都会生成新的类。 在编译、解析表达式的过程中，会使用asm生成代码，在动态代码生成阶段生成 Script_ 前缀的脚本类，在表达式解析阶段生成 Lambda_ 前缀的匿名内部类； 如果不使用缓存，则每次调用都生成新的代码；\n问题2：ThreadLocal缓存泄漏 这里我debug使用的aviator源码版本是5.3.3。\n低于5.3.3版本，还有一个ThreadLocal强引用无法回收的问题，作者已经在 issues/494 中进行了修复。\n使用如下脚本压测5.2.7版本：\n1 2 3 4 5 6 7 8 9 for (int i = 0; i \u0026lt; 10000; i++) { Expression expression = AviatorEvaluator.compile(express); Map\u0026lt;String, Object\u0026gt; env = new HashMap\u0026lt;\u0026gt;(); env.put(\u0026#34;a\u0026#34;, 100.3); env.put(\u0026#34;b\u0026#34;, 45); env.put(\u0026#34;c\u0026#34;, -199.100); Object result = expression.execute(env); System.out.println(\u0026#34;表达式结果：\u0026#34; + result); } jvm最终会报metaspace不足导致的OOM：\ncom.googlecode.aviator.exception.CompileExpressionErrorException: define class error Caused by: java.lang.OutOfMemoryError: Metaspace 对应元空间趋势： 可以看到频繁调用的时候，元空间内存占用会持续上涨。\n这个问题的原因，我们可以主要关注ThreadLocal缓存 LambdaFunction的地方：\nThreadLocal\u0026lt;LambdaFunction\u0026gt; 调整为了ThreadLocal\u0026lt;Reference\u0026lt;LambdaFunction\u0026gt;\u0026gt;，作者意图：当jvm内存紧张时，可回收 ThreadLocal 中持有的LambdaFunction。 通过断点可以看到，此处的 fnLocal.get() 仅在 newInstance 中调用，当发生了 soft reference 在内存紧张的时候被回收时，此处就会进入下方 new LambdaFunction 的流程，即缓存中没有就创建一个新的对象。\n我们再使用5.3.3新版本进行压测： 可以对比新旧版本的元空间增长趋势，内存泄漏的问题在新版本中解决了。\n总结 综上，Script_前缀的类加载过多的原因是，业务代码中调用aviator解析表达式没有使用缓存。\n最后，彻底解决aviator内存泄漏的坑需要两步走：\n预编译阶段使用缓存，复用重复的表达式； 升级aviator版本，这样jvm垃圾回收器在内存不够的情况下才能回收编译解析阶段生成的 lambda 匿名类实例； 由于对应服务中大部分表达式是一样的，因此我们主要通过使用预编译缓存来解决此次问题，修复问题后我们的服务元空间的趋势： 可以看到持续几天，元空间基本无增长。问题解决！！！\n名词解释 名词 解释 aviator Aviator是一种高性能、轻量级的Java语言实现的表达式求值引擎，用于动态求值表达式。它的实现思路与其他轻量级的求值器不同，不是通过解释的方式运行，而是将表达式直接编译成Java字节码，交给JVM执行。Aviator的定位是介于重量级脚本语言（如Groovy）和轻量级表达式引擎（如IKExpression）之间。 asm ASM（全称为\u0026quot;Abstract Syntax Tree Manipulation\u0026quot;）是一个用于在Java字节码层面进行操作和转换的框架。它提供了一组API，可以读取、修改和生成Java字节码。 meta space 元空间 元空间是jdk1.8引入的一块内存区域，使用系统本地内存，用于存储类的元信息，属于堆外内存 Hard (Strong) References 我们常规用到的引用类型。 Weak References 垃圾回收时，不管对象是否可达，都回收使用 weak 引用标识的对象，同时不阻止垃圾回收器回收被引用的对象。当我们需要缓存一些对象，但又不希望这些对象影响垃圾回收的时候，可以使用Weak Reference来实现。当对象的强引用被清除后，垃圾回收器会自动回收这些被缓存的对象。 Soft References jvm内存不够用时，垃圾回收时回收 soft 引用标识的对象。使用场景类似weak reference，区别点仅限于垃圾回收的时机，soft reference仅在应用内存紧张的时候回收。 Phantom References phantom引用标识的对象在触发垃圾回收后，会进入一个队列，只有完成我们出队的逻辑，垃圾对象才会被回收。常用于一些触发回收后的逻辑控制，比如记录回收、排查内存泄漏。 Ref 由「Metaspace容量不足触发CMS GC」从而引发的思考 【Aviator】（三）缓存引起的Full GC解决 aviator对大量生成匿名类占用元空间说明的issue java引用类型 深入理解堆外内存 Metaspace ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/jvm/memory-leak/aviator/","summary":"\u003cp\u003e协助同事排查处理了一则\u003ccode\u003eaviator\u003c/code\u003e使用不当导致元空间内存泄漏的问题，个人觉得是一则很有价值的案例，因此观摩学习一波。\u003c/p\u003e\n\u003cp\u003e使用\u003ccode\u003eaviator\u003c/code\u003e不启用表达式缓存的情况下，会分别在编译、解析阶段根据时间戳使用\u003ccode\u003easm\u003c/code\u003e动态生成\u003ccode\u003eclass\u003c/code\u003e，生成过多\u003ccode\u003eclass\u003c/code\u003e就会撑爆元空间，也就是内存泄漏。\u003c/p\u003e\n\u003cp\u003e除此之外，在分析过程中还发现了\u003ccode\u003eaviator\u003c/code\u003e低版本的另一个泄漏问题，非常适合用来巩固\u003ccode\u003ejvm\u003c/code\u003e相关知识。\u003c/p\u003e","title":"aviator动态创建类过多导致元空间内存泄漏 问题一例"},{"content":"排查了一则MySQL中datetime写入 诡异进位 的问题，通过查阅官方文档的方式找到了问题原因。顺带巩固下基础知识。\n背景： 问题由最近QA同事给我提的一个工单引起。\n相关表结构：\n1 2 3 4 5 CREATE TABLE `t_test` ( `c1` datetime DEFAULT NULL, `c2` datetime(3) DEFAULT NULL, `c3` datetime(6) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 其中我们业务时间字段类型为 c1 datetime 类型。\n过程：\n前端传入 某天最后毫秒的13位时间戳 后端通过 LocalDateUtils.convertMilliToLocalDateTime(timestamp) 传到 dao，进行sql写入 示例时间戳： 1691769599999 2023-08-11T23:59:59.999\n问题表现： 写入到mysql后，数据变成了 2023-08-12T00:00:00。\n分析： 最佳实践应该是找mysql官方文档，根据我们使用的5.7 找到了mysql关于datetime类型的描述： https://dev.mysql.com/doc/refman/5.7/en/fractional-seconds.html\n而我们表中字段设置为datetime，未保留毫秒精度，因此传入 2023-08-11T23:59:59.999 时四舍五入进位，变成了 2023-08-12T00:00:00 。\n顺便复习下rdbms中各种数据类型后面可指定长度、精度的情况：\n整数类型： TINYINT：可选长度参数指定显示宽度。 SMALLINT：可选长度参数指定显示宽度。 MEDIUMINT：可选长度参数指定显示宽度。 INT：可选长度参数指定显示宽度。 BIGINT：可选长度参数指定显示宽度。 浮点数类型： FLOAT：可选精度参数指定小数位数。 DOUBLE：可选精度参数指定小数位数。 定点数类型： DECIMAL：必需的精度和标度参数分别指定总位数和小数位数。 日期和时间类型： DATE：无需额外参数。 TIME：可选的小数秒精度参数指定秒的小数位数。 DATETIME：可选的小数秒精度参数指定秒的小数位数。 TIMESTAMP：可选的小数秒精度参数指定秒的小数位数。 字符串类型： CHAR：必需的长度参数指定字符串的固定长度。 VARCHAR：必需的长度参数指定字符串的最大长度。 BINARY：必需的长度参数指定二进制字符串的固定长度。 VARBINARY：必需的长度参数指定二进制字符串的最大长度。 ENUM：必需的值列表参数指定列允许的值。 SET：必需的值列表参数指定列允许的值。 TEXT：无需额外参数。 BLOB：无需额外参数。 其他类型： BOOLEAN：无需额外参数。 JSON：无需额外参数。 解决： 方案1： 数据库字段设置为 datetime(3)，保留毫秒三位精度。\n方案2： 前端或后端将时间戳进行特殊处理，比如 -1s 。\n推荐使用方案1。\nRef https://dev.mysql.com/doc/refman/5.7/en/fractional-seconds.html ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/mysql/fractional-seconds/","summary":"\u003cp\u003e排查了一则\u003ccode\u003eMySQL\u003c/code\u003e中\u003ccode\u003edatetime\u003c/code\u003e写入 \u003cstrong\u003e诡异进位\u003c/strong\u003e 的问题，通过查阅官方文档的方式找到了问题原因。顺带巩固下基础知识。\u003c/p\u003e","title":"mysql datetime 秒精度问题一例"},{"content":"Since 3.4.1 (less or more), mybatis-plus bring in a bug: when parsing extra whitespaces in SQL, it throws net.sf.jsqlparser.parser.ParseException. I solved the problem by finding relevant discussions and resources to gain insights into the issue.\nBackground Mybatis-Plus is a powerful Java framework that provides enhanced functionalities on top of Mybatis, simplifying data access operations.\nIn a project named service-a, we encountered the problem when parsing SQL statements that contain whitespace lines. The problem lead to unexpected errors during query execution, resulting in frustration and delays in the development process.\nRelated error log:\nnet.sf.jsqlparser.parser.ParseException: Encountered unexpected token: \u0026#34;\\n\\n\\n\u0026#34; \u0026lt;ST_SEMICOLON\u0026gt; Troubleshooting First, I analyzed the exception stack trace, the log tells me the error was thrown when dao framework parsing sql which contains whitespaces. The log highlights an unexpected token consisting of three consecutive newlines encountered while attempting to parse the SQL statement.\nAnd then I asked my colleagues if they had encountered similar error messages. It turns out no one I asked have seen this problem before.\nSo I just google the log message.\nGoogle leads me to related issues that shed light on potential solutions:\nhttps://github.com/JSQLParser/JSqlParser/issues/1756 https://github.com/baomidou/mybatis-plus/issues/5345 These discussions provided crucial information and suggestions from the community, showcasing different approaches to tackle the problem.\nSolution Temporary By adjusting the mybatis-plus configuration, specifically the shrink-whitespaces-in-sql property, we were able to mitigate the problem.\n1 2 3 mybatis-plus: configuration: shrink-whitespaces-in-sql: true Implementing this temporary solution offered relief, enabling successful parsing of SQL statements despite the presence of whitespace lines.\nLong-term While the temporary solution resolves the immediate issue, it\u0026rsquo;s important to track the progress of the related issues and stay updated on potential permanent fixes provided by the mybatis-plus and JSqlParser communities.\nAt the same time, I submitted feedback to the internal infrastructure team of the company(Keboot) to help them resolve this issue through iterative updates.\nConclusion In this blog post, we explored a issue related to mybatis-plus and the parsing of SQL statements containing whitespace lines. By identifying the error message, researching related discussions, and implementing a temporary solution, we were able to overcome the problem and continue with the development process.\nIn the long run, we need to stay engaged with the community, keep an eye on related issues, and apply upcoming fixes to ensure a robust and seamless experience with mybatis-plus (rely on the infrastructure team).\nRef https://github.com/JSQLParser/JSqlParser/issues/1756 https://github.com/baomidou/mybatis-plus/issues/5345 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/mybatis-plus/whitespaces_in_sql_err/","summary":"\u003cp\u003eSince 3.4.1 (less or more), mybatis-plus bring in a bug: when parsing extra whitespaces in SQL, it throws \u003ccode\u003enet.sf.jsqlparser.parser.ParseException\u003c/code\u003e. I solved the problem by finding relevant discussions and resources to gain insights into the issue.\u003c/p\u003e","title":"troubleshooting mybatis-plus parsing whitespaces in SQL"},{"content":"Find mathematical patterns through observing test cases.\nProblem 2544. Alternating Digit Sum https://leetcode.cn/problems/alternating-digit-sum/\nYou are given a positive integer n. Each digit of n has a sign according to the following rules:\nThe most significant digit is assigned a positive sign.\nEach other digit has an opposite sign to its adjacent digits.\nReturn the sum of all digits with their corresponding sign.\nIterate through string Solution thinking convert int param to string; iterate string char by char; accumulate sum with specified sign; update sign in loop; Code 1 2 3 4 5 6 7 8 9 func alternateDigitSum(n int) int { nStr := strconv.Itoa(n) sum, sign := 0, 1 for i := range nStr { sum += sign * int(nStr[i]-\u0026#39;0\u0026#39;) sign = -sign } return sum } Complexity analysis Time We need to iterate int n with log(n)\u0026rsquo;s time complexity;\nSpace Used an extra string with O(logn)\u0026rsquo;s length。\nOptimizaiton with math solution Solution thinking Closely examine the solution mentioned above, the reason why we used an extra string is we want to iterate the n from higher position(significant bit) to lower position.\nAnother way to iterate the n\u0026rsquo;s position is using the remainder (modulus) operation as an approach.\nBUT, this way leads to a new question: we need to determine the sign.\nBy observing the test cases, we can find the mathematical patterns: the sign differs between the oddness or evenness of the number of digits in a specific number n.\nAs illustrated in above picture, we can count the number of digits in n. And for even number, we can just calculate the opposite result sum.\nFor code simplicity, we can multiply -sign at the end.\nCode 1 2 3 4 5 6 7 8 9 10 11 func alternateDigitSum(n int) int { sum, sign := 0, 1 for n \u0026gt; 0 { sum += sign * (n % 10) n /= 10 sign = -sign } // n的位数为奇数时，从最低位开始到最高位sign是对称的 // n的位数为偶数时，从最低位开始到最高位sign 与 从最高位到最低位sign 相反，因此最后乘以-1 return sum * -sign } Complexity analysis Time Time complexity is O(log(n)).\nSpace In this solution, there is no extra variable. That is the optimization point.\nSpace complexity is O(1).\nSummary At this point, the question is solved.\nBy solving this problem, we find the mathematical patterns through observing test cases.\nodd n: sign to left == sign to right even n: sign to left == opposite(sign to right) Observing test cases is so IMPORTANT!\n","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/alternating-digit-sum/","summary":"\u003cp\u003eFind mathematical patterns through observing test cases.\u003c/p\u003e","title":"LeetCode 2544. Alternating Digit Sum"},{"content":"踩坑，Redis缓存使用Jackson2JsonRedisSerializer管理序列化，未设置disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)，上线中新代码写入了新的缓存字段结构，旧代码读到新结构报错UnrecognizedPropertyException。\n背景 a服务上线，在b表新增字段c。\nbDTO为缓存对象且对其他服务暴露。对象结构如下：\n1 2 3 4 5 { name string id int c int ```新增字段``` } 上线两台实例，发现有报错：\nCould not read JSON: Unrecognized field \u0026#34;c\u0026#34; nested exception is com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field \u0026#34;c\u0026#34; 第一步动作：先回滚。\n分析 第二步动作：查具体原因。\n可以看到报了一个jackson解析的异常。\n对应的查询接口为： /url/d\n迅速拉当时的rd了解一下接口背景：\n前两个月增加了此读缓存接口； 接口会在登录某系统处调用，影响范围较大； 调用缓存大概是这样的：\n1 2 3 4 @Cacheable(cacheNames = PREFIX_KEY, key = \u0026#34;#key\u0026#34;) public BDTO query(String key) { return mapper.query(key); } 接口通过@Cacheable加了一层缓存，逻辑为查b表信息。\n关联变更：本次上线 b 上新增了 c 字段。\n根据报错，看下jackson的设计： https://stackoverflow.com/questions/4486787/jackson-with-json-unrecognized-field-not-marked-as-ignorable\n所以问题是这样产生的：\n新的代码发布，缓存写入了带新字段的json； 旧的代码读缓存，报错 原因1：旧代码的b结构是旧的； 原因2：cacheable使用的redis序列化方式为 Jackson2JsonRedisSerializer ，对应objectMapper未设置 disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)； 解决 第三步动作：解决问题。\ncacheable redis 序列化设置，增加 disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)，反序列化时会忽略class中没有的属性； 作用是：防止以后报此类错； queryBusinessInfo 缓存key更名，防止发布中老代码读到新写入的数据； 作用是：防止发布中报此错； ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/serialization/jackson/unrecognized_property_exception/","summary":"\u003cp\u003e踩坑，\u003ccode\u003eRedis\u003c/code\u003e缓存使用\u003ccode\u003eJackson2JsonRedisSerializer\u003c/code\u003e管理序列化，未设置\u003ccode\u003edisable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)\u003c/code\u003e，上线中新代码写入了新的缓存字段结构，旧代码读到新结构报错\u003ccode\u003eUnrecognizedPropertyException\u003c/code\u003e。\u003c/p\u003e","title":"使用jackson反序列化未忽略未知字段时报错 `UnrecognizedPropertyException` 问题一例"},{"content":"LC 2671. 频率跟踪器，一道中规中矩的数据结构题目，解法平平无奇，却让我学到了「阅读理解」的重要性。\n题目 2671. 频率跟踪器 https://leetcode.cn/problems/frequency-tracker/\n实现 FrequencyTracker 类：\nFrequencyTracker()：使用一个空数组初始化 FrequencyTracker 对象。 void add(int number)：添加一个 number 到数据结构中。 void deleteOne(int number)：从数据结构中删除一个 number 。数据结构 可能不包含 number ，在这种情况下不删除任何内容。 bool hasFrequency(int frequency): 如果数据结构中存在出现 frequency 次的数字，则返回 true，否则返回 false。 Map+Set版本 思路 维护每个数据的频率 num2Freq； 同时维护频率对应数据集合 freq2Nums； 数据集合使用有序结构 or set，保证读写效率； 思路平平无奇。\n实现提交，发现1118个用例，卡在了第1024个上。\n百思不得其解，debug两遍也没想出原因。然后在地铁上看了下其他人的解法，发现了卡点：原来是我「阅读理解」出了问题。\n我理解错了 deleteOne 的含义，我开始理解的「从数据结构中删除一个 number」是将数据+频率全部删除。\n而实际上数据是要降低一次频率！\n搞明白错在哪里，代码修正就很快了，如下是修正后的代码。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 static class SetSolution { Map\u0026lt;Integer, Integer\u0026gt; num2Freq; Map\u0026lt;Integer, Set\u0026lt;Integer\u0026gt;\u0026gt; freq2Nums; public SetSolution() { num2Freq = new HashMap\u0026lt;\u0026gt;(); freq2Nums = new HashMap\u0026lt;\u0026gt;(); } public void add(int number) { int freq = num2Freq.getOrDefault(number, 0) + 1; num2Freq.put(number, freq); freq2Nums.computeIfAbsent(freq, k -\u0026gt; new HashSet\u0026lt;\u0026gt;()); freq2Nums.get(freq).add(number); if (freq2Nums.containsKey(freq - 1)) { // 移除较低频率中的当前数据 freq2Nums.get(freq - 1).remove(number); } } public void deleteOne(int number) { if (!num2Freq.containsKey(number)) { return; } int freq = num2Freq.get(number); num2Freq.remove(number); freq2Nums.get(freq).remove(number); if (freq - 1 \u0026gt; 0) { // 频率-1的情况 num2Freq.put(number, freq - 1); freq2Nums.computeIfAbsent(freq - 1, k -\u0026gt; new HashSet\u0026lt;\u0026gt;()); freq2Nums.get(freq - 1).add(number); } } public boolean hasFrequency(int frequency) { return freq2Nums.containsKey(frequency) \u0026amp;\u0026amp; !freq2Nums.get(frequency).isEmpty(); } } 复杂度分析 时间 增、改、查时间复杂度均为O(1)。\n空间 使用额外的Map+Set，空间复杂度O(n)。\n优化思路，只记录频率下数据量 思路 观察上面解法，同时再读题，会发现数据操作仅存在 add/delete，每一次频率内数据要么多一个少一个，不存在其他信息获取需求，解法1中freq2Nums维护了频率丢应所有原数据，实际上我们仅需要获取频率对应数据量（即频率的数据频率），因此仅维护频率对应数据量即可。\n优化后，空间可节省n个。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 static class CntSolution { // 数据元素对应频率 Map\u0026lt;Integer, Integer\u0026gt; num2Freq; // 频率对应数据量；由于数据操作仅存在 add/delete，每一次频率内数据要么多一个少一个，不存在其他信息获取需求，因此仅维护频率对应数据量即可 Map\u0026lt;Integer, Integer\u0026gt; freq2Cnt; public CntSolution() { num2Freq = new HashMap\u0026lt;\u0026gt;(); freq2Cnt = new HashMap\u0026lt;\u0026gt;(); } public void add(int number) { int oldFreq = num2Freq.getOrDefault(number, 0); int newFreq = oldFreq + 1; num2Freq.put(number, newFreq); if (oldFreq \u0026gt; 0) { freq2Cnt.put(oldFreq, freq2Cnt.get(oldFreq) - 1); } freq2Cnt.put(newFreq, freq2Cnt.getOrDefault(newFreq, 0) + 1); } public void deleteOne(int number) { if (!num2Freq.containsKey(number)) { return; } int oldFreq = num2Freq.get(number); int newFreq = oldFreq - 1; if (newFreq == 0) { num2Freq.remove(number); } else { num2Freq.put(number, newFreq); freq2Cnt.put(newFreq, freq2Cnt.getOrDefault(newFreq, 0) + 1); } freq2Cnt.put(oldFreq, freq2Cnt.get(oldFreq) - 1); } public boolean hasFrequency(int frequency) { return freq2Cnt.containsKey(frequency) \u0026amp;\u0026amp; freq2Cnt.get(frequency) \u0026gt; 0; } } 复杂度分析 同解法1，实际执行的空间效率更高。\n小结 本题中规中矩，很容易产出思路。\n但也正像平时与上下游同事沟通，很容易出现理解上的偏差。因此要重视「阅读理解」。\n","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/frequency-tracker/","summary":"\u003cp\u003eLC 2671. 频率跟踪器，一道中规中矩的数据结构题目，解法平平无奇，却让我学到了「阅读理解」的重要性。\u003c/p\u003e","title":"LC 2671. 频率跟踪器"},{"content":"记录一则macOS设置默认浏览器失效的例子。通过尝试解决了问题。\n问题描述 mbp设置中，设置了默认浏览器为Chrome，但是点击链接跳转到了Safari。\n设置如图：\n环境 macOS 版本：13.0 (22A8380)\n解决 找苹果客服，给我提供的方案是尝试重启、尝试进安全模式。 尝试上一步后无效，进入到Chrome的设置中，发现也有个默认浏览器设置，此处设置后生效。设置如图： Ref 如何在 Mac 上使用安全模式 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/macos/setting/default-browser/","summary":"\u003cp\u003e记录一则\u003ccode\u003emacOS\u003c/code\u003e设置默认浏览器失效的例子。通过尝试解决了问题。\u003c/p\u003e","title":"踩坑：macOS默认浏览器设置后跳转有误"},{"content":"踩坑，新机器装开发环境，因手动变更了已配置好的idea启动依赖的agent路径，导致jetbrains全系产品无法启动。\n背景 时间：2023.03.23\n换了新电脑。下载了 intellij idea。\n配置了开发环境。\n用了个把小时（中间会有若干配置操作），ide突然崩溃了，并且，无法重启。\n崩溃日志： 此时由于身体饥饿，脑子也不是很好用。\nmacos版本：macos 13.0 Ventura\n处理过程 尝试修复 第一反应是：该不会是苹果m2芯片跟idea版本不兼容的问题吧？\n于是下载了3个不同版本，依次安装尝试。\n结果均以失败告终。\n23号晚八九点，我放弃折腾，先回家吃饭了。\n找到问题所在 24号早上我休息好了，来到工位开始思考是哪里出了问题。\n首先ide经过了几小时的正常使用，说明不是版本兼容的问题。\n然后我突然想到！由于我在finder下创建了 /Users/dragonsong/Documents/dev-tool 目录，但是发现跟老电脑上的 /Users/dragonsong/Documents/dev-tools 不一致！\n所以我手动将目录名统一为 /Users/dragonsong/Documents/dev-tools 。\n而这个目录下，安装了ide启动依赖的agent。\n如图： 而这里的-javaagent下的目录是没更新的。\n解决 可以通过两种方式解决此问题：\n直接执行安装此agent的shell脚本 手动改此vmoptions下的路径 操作后，确实可以正常使用jetbrains产品了。\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/ide/jetbrains/boot-with-java-agent/crash/","summary":"\u003cp\u003e踩坑，新机器装开发环境，因手动变更了已配置好的\u003ccode\u003eidea\u003c/code\u003e启动依赖的\u003ccode\u003eagent\u003c/code\u003e路径，导致\u003ccode\u003ejetbrains\u003c/code\u003e全系产品无法启动。\u003c/p\u003e","title":"踩坑更改了javaagent路径导致idea无法启动问题一例"},{"content":"踩坑，spring-data-redis版本过低，导致并发获取数据为null问题一例，试用phind.comAI 搜索，快速定位到了问题原因。\n背景 组内维护了一个商户服务，本文我们可以叫它user-service，年前上的代码中加了一个 @Cacheable(\u0026quot;key#30\u0026quot;)的读缓存。\n从上周开始，以极低频率，偶发报查询NPE空指针。一周大概报错3-4次。\n伪代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 @Cacheable(cacheNames = \u0026#34;businessSimpleInfo#30\u0026#34;, key = \u0026#34;#businessCode\u0026#34;) public BusinessSimpleVo queryBusinessSimpleInfo(String businessCode) { BusinessSimpleVo result = new BusinessSimpleVo(); Business business = this.queryByBusinessCode(businessCode); if (business == null) { return result; } result.setBusinessTerminateStatus(business.getTerminalStatus()); result.setBusinessCode(business.getBusinessCode()); result.setSystemType(business.getSystemType()); return result; } 日志报错指向，调用queryBusinessSimpleInfo时，报NPE。\nuser-service使用的spring-data-redis版本为：\n1 org.springframework.data:spring-data-redis:jar:1.8.8.RELEASE:compile 分析 分析代码 可以看到，我们的result无论何时，都不可能为null。\n所以问题只可能出现在@Cacheable上。\n搜索答案 直接搜索 https://phind.com/ 。\n这里解释有点小问题，但是已经把核心问题说出来了：并发场景下，低版本的spring-data-redis获取数据时会获取到null，原因是RedisCache类get()方法中有非原子性操作。\n这里并发是指我们的服务线程与Redis过期线程同时操作数据。\n分析spring-data-redis源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /** * Return the value to which this cache maps the specified key. * * @param cacheKey the key whose associated value is to be returned via its binary representation. * @return the {@link RedisCacheElement} stored at given key or {@literal null} if no value found for key. * @since 1.5 */ public RedisCacheElement get(final RedisCacheKey cacheKey) { Assert.notNull(cacheKey, \u0026#34;CacheKey must not be null!\u0026#34;); Boolean exists = (Boolean) redisOperations.execute(new RedisCallback\u0026lt;Boolean\u0026gt;() { @Override public Boolean doInRedis(RedisConnection connection) throws DataAccessException { return connection.exists(cacheKey.getKeyBytes()); } }); if (!exists.booleanValue()) { return null; } return new RedisCacheElement(cacheKey, fromStoreValue(lookup(cacheKey))); } 可以看到上述24行，当请求线程（单个或多个）同时走到24行时，此时数据过期，此时fromStoreValue返回的就是null值。\n解决 捋清脉络，有两种解决办法：\n升级 spring-data-redis 版本到 1.8.11。 重写RedisCache.get方法，示例代码如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 public static class CustomizedRedisCache extends RedisCache { private RedisOperations redisOperations; public CustomizedRedisCache(String name, byte[] prefix, RedisOperations\u0026lt;?, ?\u0026gt; redisOperations, long expiration) { super(name, prefix, redisOperations, expiration); this.redisOperations = redisOperations; } // https://github.com/spring-projects/spring-data-redis/issues/1312 public RedisCacheElement get(final RedisCacheKey cacheKey) { Assert.notNull(cacheKey, \u0026#34;CacheKey must not be null!\u0026#34;); // 前置查数据，保证null都会fail-fast RedisCacheElement redisCacheElement = new RedisCacheElement(cacheKey, fromStoreValue(lookup(cacheKey))); Boolean exists = (Boolean) this.redisOperations.execute((RedisCallback\u0026lt;Boolean\u0026gt;) connection -\u0026gt; connection.exists(cacheKey.getKeyBytes())); // fail-fast if (!exists.booleanValue()) { return null; } // 要么为未过期的有效数据；要么为刚过期的数据； return redisCacheElement; } } public static class RedisCustomizedCacheManager extends RedisCacheManager { private RedisOperations redisOperations; private CacheableRedisProperties cacheableRedisProperties; public RedisCustomizedCacheManager(RedisOperations redisOperations, CacheableRedisProperties cacheableRedisProperties) { super(redisOperations); this.redisOperations = redisOperations; this.cacheableRedisProperties = cacheableRedisProperties; } public RedisCustomizedCacheManager(RedisOperations redisOperations, Collection\u0026lt;String\u0026gt; cacheNames) { super(redisOperations, cacheNames); this.redisOperations = redisOperations; } public RedisCustomizedCacheManager(RedisOperations redisOperations, Collection\u0026lt;String\u0026gt; cacheNames, boolean cacheNullValues) { super(redisOperations, cacheNames, cacheNullValues); this.redisOperations = redisOperations; } protected RedisCache createCache(String cacheName) { long expiration = computeExpiration(cacheName); return new CustomizedRedisCache(cacheName, (isUsePrefix() ? new CustomizedRedisCachePrefix(cacheableRedisProperties.getPrefix()).prefix(cacheName) : null), redisOperations, expiration); } protected boolean isUsePrefix() { return true; } } Ref Spring-data-redis cacheable并发导致的null问题 https://github.com/spring-projects/spring-data-redis/issues/1312 phind.com ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/redis/cacheable-anno/spring-data-redis/get_non_atomic/","summary":"\u003cp\u003e踩坑，\u003ccode\u003espring-data-redis\u003c/code\u003e版本过低，导致并发获取数据为\u003ccode\u003enull\u003c/code\u003e问题一例，试用\u003ccode\u003ephind.com\u003c/code\u003eAI 搜索，快速定位到了问题原因。\u003c/p\u003e","title":"spring-data-redis版本过低，导致并发获取数据为null问题一例"},{"content":"排查了一则MySQL中in查询传入270个入参则 索引失效 的问题，本文记录分析与解决的过程。\n背景： 上线了一个统计类的逻辑，发现有慢SQL。\n相关表结构：\n1 2 3 4 5 6 CREATE TABLE `t` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;id\u0026#39;, `repeat_hash` varchar(128) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`), KEY `idx_repeat_hash` (`repeat_hash`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 对应查询语句：\n1 2 3 select * from t where repeat_hash in (一堆入参) 线上入参传入了100-300个，查询耗时达到了 5000-50000ms，大概就是5s到50s级别。单个入参为固定的hash值，示例：\u0026ldquo;19445304e93d3a6134544458004d9170\u0026rdquo;。\n分析： explain一波： 找到问题症结了！\nin元素过多 or in元素在索引树上范围较大，导致扫了全表。\n将入参数量改到一百来个，减少入参数量： 此时type变为了range。\n根据我的经验值，一般 \u0026ldquo;19445304e93d3a6134544458004d9170\u0026rdquo; 这种入参批量in查询时，传入三五百个甚至一千个都不是问题。\n所以我们需要挖一下背后的原理。\n原理解析 mysql官网关于如何用索引的文档：\nhttps://dev.mysql.com/doc/refman/5.7/en/mysql-indexes.html\n翻译：\n小表上的索引不重要； 大表上，但是查询需要遍历到大部分的行，这种情况顺序扫描全表的开销反而比使用索引的开销小（顺序读减少了磁盘寻道）； 因此，当前慢查询的原因，我个人判断是传入的repeathash范围较大，mysql 优化器认为此时不如直接扫全表来的快。\n这里存疑：由于我们的编码值是一个固定位数的hash值，如果300个左右的编码比较分散，理论上来说100个编码同样也是分散的。但是根据后面实测，100个编码确实能走到索引上。\n解决思路 方案一 每批in的入参数量改成100\t。\n优点是改动较小，缺点是如果100个元素的范围较大，依然会比较慢。\n方案二 一批数据处理过程，每次处理一条数据时，根据对应的repeatHash查一次数据库。\n优点是每次查询都能保证是一个type==ref的单条命中索引的查询，性能预期较稳定\t，缺点则是jvm与mysql的io rt需要多次。\n经过后面的实测，选用方案一。\n实测 实际跑下来的数据表现是：\n每次批量100左右的in，可以走到索引，耗时几十ms级别； 单条for循环耗时稳定，300条左右耗时10s左右，100条左右耗时4s； 对比下来选用了方案一。 对应性能表现： 开启数据任务后，暂未出现慢查询。\nRef https://dev.mysql.com/doc/refman/5.7/en/mysql-indexes.html ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/mysql/index/in-ineffective/","summary":"\u003cp\u003e排查了一则\u003ccode\u003eMySQL\u003c/code\u003e中\u003ccode\u003ein\u003c/code\u003e查询传入270个入参则 \u003cstrong\u003e索引失效\u003c/strong\u003e 的问题，本文记录分析与解决的过程。\u003c/p\u003e","title":"mysql in 查询索引失效问题一例"},{"content":"踩坑redis混部，导致AOF写磁盘过多，导致影响其他实例。通过与sre协作、分析日志，解决了问题。\n背景 组内维护了一个较老的服务，从22年10月国庆开始，偶发出现了redis相关报错。\n报错示例：\n1 2 org.springframework.data.redis.cache.RedisCacheManager|-1|redis异常：key=[QueryParam(codeList=[3149904936829982727], codeType=2)|], exception=Could not get a resource from the pool; nested exception is redis.clients.jedis.exceptions.JedisException: Could not get a resource from the pool org.springframework.dao.InvalidDataAccessApiUsageException: Could not get a resource from the pool; nested exception is redis.clients.jedis.exceptions.JedisException: Could not get a resource from the pool 这个服务的这种报错，以假期的规律出现，比如国庆假期第一天上午，开始频繁报警。\n同周期出现的，还有另一个报错：\n1 org.springframework.data.redis.cache.RedisCacheManager|-1|redis异常：key=[XL18332|440300|], exception=java.net.SocketTimeoutException: Read timed out; nested exception is redis.clients.jedis.exceptions.JedisConnectionException: java.net.SocketTimeoutException: Read timed out\u0026#34;, 对应报错的接口是一个资质查询的缓存。一周维度的请求数据为：\n近一周总访问量 12,595,893，qps 352 。\n分析 分析日志 首先通过日志，可以看到报错给出两个信息：\n拿不到连接 可能的原因： Redis宕机了。 应用使用了比服务端连接池更多的连接。 应用使用完连接后，没及时释放连接资源。 连接超时 读取响应超时，可能的原因： Redis服务器的资源利用率过高或内存不足。 Redis连接池配置错误，导致连接过多或连接过少。 Redis客户端未启用自动断开连接功能，导致连接空闲时间过长。 Redis命令执行时间过长，例如在BLPOP操作时，阻塞等待响应时间超过了socket_timeout设置的时间。 Redis建立连接超时时间过短，例如socket_connect_timeout设置的时间过短。 排查应用流量 观察到报错随着假期（高峰期）的周期出现，于是通过日志平台与监控验证下我们服务的流量是否有高峰激增。\n日志、监控显示，我们的流量并没有激增太多。结合背景部分的具体数据，qps 352 对于Redis即使是单实例也完全形不成压力。\n所以问题流量并没经过我们这个服务。\n那么这就奇怪了！！！\n与sre协作 本服务内找不到问题，尝试找sre，看看有没有什么有效信息。\n一问，sre给出了关键日志：\n1 Asynchronous AOF fsync is taking too long (disk is busy?). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis. 如果熟悉Redis，可以get到，此时我们的Redis实例正被阻塞于AOF fsync写文件的操作上，但是由于磁盘IO过高，磁盘此时压力过大， fsync一直卡着。\n这也是导致我们的服务一直拿不到Redis连接的直接原因！\n接着我们验证了服务对应Redis的AOF写入量，发现并不多！这就奇怪了！\n再次询问sre，得知这台Redis与其他20+个实例混合部署在同一台机器上。\n此时，所有疑点都明朗了。\n破案 我们的Redis由于与其他20+个实例混部在同一台机器上，其他实例流量激增导致磁盘压力过大，导致我们的实例迟迟无法获取到IO权限。\n服务端这边也迟迟拿不到连接，看到的假象就是Redis停止服务！\n解决 捋清脉络，并且与相关干系人快速沟通后，我们决定切换Redis实例到独立的机器上。\n操作后，此报错在多个假期未出现，问题解决！\nRef Kaito-Redis常见问题答疑 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/redis/asynchronous_aof_fsync_is_taking_too_long/","summary":"\u003cp\u003e踩坑\u003ccode\u003eredis\u003c/code\u003e混部，导致\u003ccode\u003eAOF\u003c/code\u003e写磁盘过多，导致影响其他实例。通过与\u003ccode\u003esre\u003c/code\u003e协作、分析日志，解决了问题。\u003c/p\u003e","title":"混部redis写文件竞态严重导致实例获取不到连接问题一例"},{"content":"居家办公，使用备用机搬砖，发现iCloud有文件没同步过来，排查、咨询客服后通过进入安全模式解决了问题。\n背景 突然小区封控跟流调就来了，我只能居家办公。\n症状 家里这台mbp的Document目录放在iCloud同步目录下，但是我8号打开电脑想看obsidian的工作笔记就懵了。\n大概两个月的文件都没给同步过来。\n然后之前挺多文件都是有同步的。\n环境 我的工作环境是公司一台MacBook Pro (Retina, 15-inch, Mid 2015)。\n家里一台MacBook Pro (13-inch, 2016, Two Thunderbolt 3 ports)，系统版本是：11.2 (20D64)。\n思路 \u0026amp; 操作 先想 检查网络 这个可以迅速排除，因为虽然这台笔记本不常用，但是我周末在家一直是用这台写代码的，力扣也正常提交。 检查照片是否有同步 发现照片完全实时同步了（7号晚上的已经能看到最新的）。 检查iPhone上的文件是否同步 这个也迅速确认了，手机上的一些证件、工作目录都有，并且是最新的。 再查 接着上bing搜了下，找到了 苹果论坛：MAC里的icloud无法同步 。\n但是通过尝试，这里也被排除了。\n直接问客服 因为要赶紧搬砖，所以果断打了客服的电话。\n给客服交代了背景、尝试过的操作。客服给了我可行的方案：进安全模式。\n安全模式的步骤：\n关机； 按一下电源键； 然后长按shift，直到屏幕有画面； 之后会经过两次登录，即可进入安全模式； 进入安全模式会有些卡顿，是正常的。\n然后我看着Finder下的iCloud同步目录在以肉眼可见的变化从服务端拉取数据，等待两分钟，我的工作笔记基本就同步完成了。\n最后问了下客服，判断是被动同步的时候有卡住的情况，然后我机器一直没重启，一直没有触发过主动拉取。\n之后碰到类似情况可以尝试：\n重启机器，触发一些登录项、启动动作； 进入安全模式，触发同步拉取； End 至此，经过一些失败的想法、操作，最终在客服的支持下学到了安全模式的解决办法。\nRef 苹果论坛：MAC里的icloud无法同步 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/macos/icloud/sync-block/","summary":"\u003cp\u003e居家办公，使用备用机搬砖，发现iCloud有文件没同步过来，排查、咨询客服后通过进入安全模式解决了问题。\u003c/p\u003e","title":"解决iCloud同步卡住的问题"},{"content":"分析排查一则接口超时问题，定位为JVM Young区新生代过小引发GC频繁、触发STW停顿过多。\n背景 系统关系 b服务调用a服务接口x，接口超时。我负责a服务。 业务场景 其中b服务业务场景是B端营销活动场景，存在类似抢购的操作。 时间 2022-10-13 特征 调用端接口超时；偶现（大约一天出现两次）； 机器配置 传统化机器套餐：标准型S4X6(4 cores cpu 6G mem)；我们的服务配置的是4GB堆，500MB元空间； Linux版本 3.10.0-862.14.4.el7.x86_64 Java版本 1.8.0_181 服务QPS 低峰：20*9；高峰 70*9； 第一次简单排查 首先排查了一遍常规日志，没有明确结论。\n尝试在b服务侧增加了redis缓存。\n之后几天没有同样的报警。\n第二次排查 2022-11-01 同样的接口出现了超时。\n晚上八点出头，同事看到GC出现了1.79s的收集停顿时长。\n于是，2号上午开始埋头排查：一定要查出个究竟。\nJVM参数解读 完整参数：CommandLine flags: -XX:AutoBoxCacheMax=20000 -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:CompressedClassSpaceSize=528482304 -XX:+ExplicitGCInvokesConcurrent -XX:InitialHeapSize=4294967296 -XX:+ManagementServer -XX:MaxHeapSize=4294967296 -XX:MaxMetaspaceSize=536870912 -XX:MaxNewSize=1073741824 -XX:MaxTenuringThreshold=6 -XX:MetaspaceSize=536870912 -XX:NewSize=1073741824 -XX:OldPLABSize=16 -XX:-OmitStackTraceInFastThrow -XX:+PrintCommandLineFlags -XX:+PrintGC -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCCause -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintPromotionFailure -XX:-UseBiasedLocking -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseFastAccessorMethods -XX:+UseParNewGC :+UseParNewGC 使用了ParNew收集器 :MaxHeapSize=4294967296 堆最大4GB :MaxMetaspaceSize=536870912 元空间最大 0.54GB :MaxNewSize=1073741824 young区最大1GB :MaxTenuringThreshold=6 young区对象晋升到old区age阈值 :MetaspaceSize=536870912 元空间 0.54GB :NewSize=1073741824 young区1GB :OldPLABSize=16 上面提到停顿了一秒多的gc log 2022-11-01T20:08:44.272+0800: 49.429: [GC (Allocation Failure) 2022-11-01T20:08:44.272+0800: 49.429: [ParNew: 764866K-\u0026gt;43858K(943744K), 1.7194357 secs] 834581K-\u0026gt;377754K(4089472K), 1.7195128 secs] [Times: user=6.91 sys=0.20, real=1.72 secs] 2022-11-01T20:08:45.992+0800: 51.149: Total time for which application threads were stopped: 1.7198275 seconds, Stopping threads took: 0.0000787 seconds 定量分析 重新描述问题：2022-11-01T20:08:44.272+0800: 49.429 时间点发生了 1.7198275s 的STW停顿。\n同时，我跑到对应机器上把一秒前后的log拿下来，使用 gceasy 查看内存分布： 可以看到元空间与old区空间完全够，而young区接近满的状态。\n严谨起见，我们将监控时段拉到25号-3号，查看jvm old区内存峰值占用情况： old区谷值占用情况： 可看到所有实例堆最大配了4GB，其中1GB左右被young区使用，而old区内存最多分配2GB出头，gc回收后仅占用不到1GB。说明我们的实例对象大部分生命周期都不长。\n目前其实已经得出一些定量结论：我们服务中大部分对象的生命周期并不长。\n查看监控大盘上的jvm信息 本着严谨的态度，我们需要做一些定性分析。\n定性分析 大盘上对应出问题实例jvm内存： 这里的秒、毫秒级别略有偏差，原因可能是监控取的是近似值（统计值）。\n可以看到gc次数相对较稳定，但是在8、9、10分时略有上升，将时间跨度拉大后，这里就是一个突刺。 无fullGC，触发的都是youngGC，并且会有突刺、上升趋势。\n观察线程状态： 结合每分钟gc次数： 可以发现在监控 16:34 开始gc次数变多，曲线攀升一直持续到 16:37 ，此处从5升到了8.5/9次每分钟。\n定量分析 三四分钟的时间，每台机器上的gc次数都近乎翻了一倍，激增。（gc次数频繁） 疑点：到底是哪些线程变多？ 上面观察线程状态的趋势图，可以看到deamon thread变多。\n我们需要研究下这里的后台线程究竟是哪部分的。这里我用arthas观测了线程分布，以及配合分析了qps趋势。\n利用平台提供的arthas工具，看了下线程分布，可以看到大多数daemon线程为http-nio，即都是tomat在处理http请求。\n配合观察四点多时段的qps走势，可以得出结论：此时由于qps爬升，请求变多，tomat启动了更多了nio线程。\n这部分线程我最开始怀疑是gc线程，查阅资料后，发现可以确认，young区parnew回收器使用多线程复制算法，并且以STW的方式运行，其线程数可通过命令确认：\n1 java -XX:+PrintFlagsFinal | grep ParallelGCThreads 这里在我们服务机器上输出：\n1 uintx ParallelGCThreads = 33 {product} 所以增多的gc线程最多也只有33个，因此，这里线程数的提升主要是tomat nio线程。\n附一个知识点：\nParallel New (ParNew) Collector (invoked by \u0026ldquo;-XX:+UseConcMarkSweepGC\u0026rdquo; option) The ParNew collector for Young generation uses the \u0026ldquo;Copy (also called Scavenge)\u0026rdquo; algorithm parallelly using multiple CPU processors (multiple threads) in a stop-the-world fashion.\nThe ParNew collector for Young (new) generation uses the same algorithm as the PS collector, except that it has an internal \u0026lsquo;callback\u0026rsquo; that allows an old generation collector to operate on the objects it collects (really written to work with the concurrent collector), as described by Jack Shirazi in \u0026ldquo;Oracle JVM Garbage Collectors Available From JDK 1.7.0_04 And After\u0026rdquo;. The ParNew collector for Young generation can be identified in GC log messages with the \u0026ldquo;ParNew\u0026rdquo; label, as in this example: \u0026ldquo;ParNew: 1068K-\u0026gt;63K(1152K)\u0026rdquo;. In JVM 9 and older releases, the ParNew collector for Young generation can be invoked explicitly using the \u0026ldquo;-XX:+UseParNewGC\u0026rdquo; option. But this option has been removed from JVM 10.\n疑点：加内存为什么能解决问题？ 这里主要是需要扩大young区内存，前面分析得出，我们服务中的大部分对象生命周期都较短，因此需要扩充young区。\n一次Minor GC，主要分两步：\nT1(扫描新生代) T2(复制存活对象) 其中 T2\u0026raquo;T1，复制成本更高。\n当扩大了young区后，Minor GC频率会变低，触发Minor GC更晚后，此时需要复制、晋升的对象就变少了，因此节省了T2的成本。\n整体而言，这里会提升性能。\n具体细节可参考：从实际案例聊聊Java应用的GC优化 。\n疑点：使用G1的优势与条件？ 经过综合评估，我们最后将堆调成了8GB，同时切换CMS为G1。\n前提： 使用G1有一个前提：堆不能太小，堆如果太小，会出现频繁full GC。原理简单来说，就是实现G1算法需要额外的空间占用（维护关系、状态、对象年龄\u0026hellip;），如果堆太小，则对象迅速堆满堆空间触发full GC。\n优势：\n网格化，精细管控 G1对空间进行了网格化，每个网格叫region，默认是2MB，分配、回收空间在region的粒度上操作，将处理链路打散，各自处理各自内部的逻辑； 类比：就像存储领域的分区，数据根据路由key存到某个分区、分片上，存储系统整体的拓展性、性能、稳定性都得以提升；也很像疫情防控，出现阳性只封控对应楼，而不是一下子把整个区封掉，网格化管理更加灵活； 并行回收 多线程并行处理，充分利用多核机器的优势 年轻代动态调整 默认young区为整堆大小的5%，通过-XX:G1NewSizePercent设定 最大可调整到整堆大小的60%，通过-XX:GlMaxNewSizePercent设定。年轻代回收暂停期间，会计算扩容还是缩容 设置了-XX:NewRatio、-Xmn比例或者大小时，G1会忽略此设置 这个特性非常适合我们的服务，整堆大小够用，当较短平快的接口QPS上升时young区可自动扩容，当长生命周期对象较多时，空间可以让给old区 控制最大停顿时长 通过-XX:MaxGCPauseMills设定，默认为200ms，大部分应用这个值完全够用 单独管理大对象 大对象不会占用默认的region，而是使用额外的空间管理，这样彼此不影响 综上，整体而言，G1在我们有8GB堆的情况下，能够更加动态灵活、分区管理、分离大对象，这些比人工调优更适应线上runtime环境。\n因此G1很棒。\n结论 目前为止，我们可以得出如下结论：\n11-01 16:34 开始gc次数变多，gc次数整体一直持续到九点后才出现明显的下降趋势。晚上08:08分出现了二次激增，每分钟gc次数达到了 18 次。 随着gc次数变多，jvm内的daemon thread数量变多（用于gc的线程+tomcat nio为daemon thread），业务线程并没有变多。 gc均触发的是young gc，说明业务请求，此时更多的是需要年轻代的空间，即对象生命周期较短。 b服务的缓存一定程度上减少了部分打到a的请求量，但是a这一层的进程暂停问题依然存在。 young区整体接近满的状态，而日志中出现 allocation failure，说明eden区没有足够空间容纳新对象，因此young区整体可以扩大，对应eden区也需要扩大。 方案 基于以上分析，可以有如下方案：\n方案 优点 缺点 增加应用实例数 可解决此类问题；程序不需要改动； 消耗云、机器资源 增大young区内存 可解决此类问题；修改成本小； 调整gc collector，从cms调为g1 可解决此类问题；G1可以应对更大业务量、低延迟的业务场景；； 略微更消耗内存； 各区容量参考算法：\n空间 倍数 总大小 3-4 倍活跃数据的大小 新生代 1-1.5 活跃数据的大小 永久代 1.2-1.5 倍Full GC后的永久代空间占用 老年代 2-3 倍活跃数据的大小 小结 当前问题通过定性、定量分析，得出了解决结论： 增大young区内存配置 或者使用G1，动态智能扩充young区 npc问题：network delay、process pause、clock drift 一般不容易碰到，碰到此类问题需重视，解决后可提升个人、团队能力。 通过case study方式，加以积累，可有效地将问题解决转化为工程师能力、团队能力。 心得：\n解决问题不能靠猜，需要有理论、数据依据。 尝试性方案可以作为逼近答案的方式。 知识点需要系统化学习，结合实践加强理解。 Ref 自动化回归环境搭建复盘 What is Daemon thread in Java and Difference to Non daemon thread JVM Parameters https://stackoverflow.com/questions/13543468/maxtenuringthreshold-how-exactly-it-works https://reins.altervista.org/java/A_Collection_of_JVM_Options_MP.html https://stackoverflow.com/questions/28342736/java-gc-allocation-failure https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/g1_gc.html 小米Talos GC性能调优实践 从实际案例聊聊Java应用的GC优化 工具gceasy http://www.herongyang.com/Java-GC/Collector-Young-Generation-Collectors.html 关于生产环境改用G1垃圾收集器的思考 G1 Garbage Collector – Java 9 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/young-gc-insufficient-memory/","summary":"\u003cp\u003e分析排查一则接口超时问题，定位为\u003ccode\u003eJVM\u003c/code\u003e \u003ccode\u003eYoung\u003c/code\u003e区新生代过小引发GC频繁、触发\u003ccode\u003eSTW\u003c/code\u003e停顿过多。\u003c/p\u003e","title":"young gc stw pause引发接口超时问题一例"},{"content":"了解原地哈希思路，使用其解决LC几道代表性题目。\n原地哈希 概念学习 我个人对原地哈希的理解：\n原地：一般题干要求O(1)的空间复杂度，此时我们可以考虑使用入参数组，在不开辟多余空间的情况操作，就是就地、原地算法； 哈希：hashmap是我们大多数时候使用哈希数据结构的一种通用实现，内部也是使用了数组进行存储，部分题干中的数据，有比较明确的哈希映射关系，比如[1,n]数据，可以直接存放在n长度的数组中，对应哈希函数：hash(x)=x-1； 可以参考下liweiwei大佬的讲解：原地哈希（哈希函数为：f(nums[i]) = nums[i] - 1）。\n本文接下来通过LC上的几道题来感受、理解其特征、运用。\n题目 剑指 Offer 03. 数组中重复的数字 剑指 Offer 03. 数组中重复的数字\n这道题之前写过题解：数组中重复的数字 原地部分有序化题解 。\n思路 原地哈希解法的核心就是：\n识别题干已知条件 数据范围：[0,n-1] 有数字是重复出现的 返回任意一个重复数字 尝试使用原地哈希的思路，我们尝试把遍历到的每个数据都放到对应位置：nums[nums[i]]=nums[i]。\n这种操作重复，重复数据一定会相遇：如果此时nums[i]下标处已经有归位的数据，说明找到了重复数据。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public int findRepeatNumber(int[] nums) { int n = nums.length; // 尝试把遍历到的数字放到 i 的位置上，进行局部有序化 // 如果往过放的过程，发现i的位置已经存在了i数值，说明重复，即可返回 // i指向前序已经归位的下一个下标 int i=0; while(true){ while(nums[i]!=i){ if(nums[nums[i]]==nums[i]){ return nums[i]; } swap(nums,i,nums[i]); } i++; } } public void swap(int[] nums,int i1,int i2){ int tmp=nums[i1]; nums[i1]=nums[i2]; nums[i2]=tmp; } 442. 数组中重复的数据 442. 数组中重复的数据\n思路 分析题干：\n数据量n，数据范围[1, n] 需要找出所有重复数据 我们尝试用解上道题的套路来处理：\n遍历数组，将数据nums[i]放在下标nums[i]-1处 收集所有nums[i]!=i+1的数据 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public List\u0026lt;Integer\u0026gt; findDuplicates(int[] nums) { List\u0026lt;Integer\u0026gt; ans= new ArrayList\u0026lt;\u0026gt;(); int n=nums.length; for(int i=0;i\u0026lt;n;i++){ while(nums[nums[i]-1]!=nums[i]){ swap(nums,i,nums[i]-1); } } for(int i=0;i\u0026lt;n;i++){ if(nums[i]!=i+1){ ans.add(nums[i]); } } return ans; } private void swap(int[] nums, int i1, int i2){ int tmp=nums[i1]; nums[i1]=nums[i2]; nums[i2]=tmp; } 参考下官解的做法：\n将遍历到的数据对应下标值设置为相反数（一个数既表达了访问状态，也保留了原值） 下次碰到对应下标为负数时，说明是重复元素 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /** * 当前值对应下标，访问过后，我们将其设置为相反数，表示已经被访问 * 下次碰到重复值，发现对应下标值已经是负数，将其相反数塞进ans * \u0026lt;p\u0026gt; * 时间复杂度 O(n) * 空间复杂度 O(1) */ public List\u0026lt;Integer\u0026gt; findDuplicates(int[] nums) { // 访问过的数字，在对应下标处标记为负数相反数 List\u0026lt;Integer\u0026gt; ans = new ArrayList\u0026lt;\u0026gt;(); int n = nums.length; for (int i = 0; i \u0026lt; n; i++) { int curr = Math.abs(nums[i]); if (nums[curr - 1] \u0026lt; 0) { ans.add(curr); } else { nums[curr - 1] = -nums[curr - 1]; } } return ans; } 448. 找到所有数组中消失的数字 448. 找到所有数组中消失的数字\n思路 分析题干：\n数据量n，数据范围[1, n] 题目要求出没出现的数字 同样的套路，我们遍历数组，将nums[i]都归位到nums[i]-1下标处。\n再遍历一次，i+1!=nums[i]的就是消失的数字。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 /** * 原地hash，使用入参数组，进行数据还原。 * 题干限定了数据范围为 [1,n]，尝试将当前下标的数替换到对应下标上，nums[i]下标对应：nums[i]-1 * 碰到下标 nums[i]-1 处 与当前值相同时，进行下一个下标处理 * \u0026lt;p\u0026gt; * 最终，遍历整个数组，下标处值不等于 i+1 的，即为缺失的数据 * \u0026lt;p\u0026gt; * 时间复杂度 O(n) * 空间复杂度 O(1) */ public List\u0026lt;Integer\u0026gt; findDisappearedNumbers(int[] nums) { List\u0026lt;Integer\u0026gt; ans = new ArrayList\u0026lt;\u0026gt;(); int n = nums.length; for (int i = 0; i \u0026lt; n; i++) { // 当前位置值与下标不匹配，同时 nums[i]-1 下标处于当前值不等 // nums[i] != i + 1 \u0026amp;\u0026amp; while (nums[nums[i] - 1] != nums[i]) { swap(nums, i, nums[i] - 1); } } for (int i = 0; i \u0026lt; n; i++) { if (nums[i] != i + 1) { ans.add(i + 1); } } return ans; } private void swap(int[] nums, int i1, int i2) { int tmp = nums[i1]; nums[i1] = nums[i2]; nums[i2] = tmp; } 41. 缺失的第一个正数 41. 缺失的第一个正数\n思路 观察用例：\n[1,2,0] 将[1,n]范围的数据放到nums[i]-1的位置上 遍历到的第一个nums[i]!=i+1的即为所求 [1,2,3] 遍历完数组，此时n+1即为所求 [3,4,-1,1] 归位之后：[1,-1,3,4] -1即为所求 [7,8,9,11,12] 所有数据均不在[1,n]范围内 直接从1遍历，1即为所求 算法过程：\n将 nums[i]-1 有效下标范围的数据归位，即 x放在 x-1 的位置上，负数、0、大于n的数我们不动 依次从1开始遍历，没有归位的i值，即为缺失的最小正数 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 /** * 时间复杂度 O(n) * 空间复杂度 O(1) */ public int firstMissingPositive(int[] nums) { int n = nums.length; for (int i = 0; i \u0026lt; n; i++) { while (nums[i] - 1 \u0026gt;= 0 \u0026amp;\u0026amp; nums[i] - 1 \u0026lt; n \u0026amp;\u0026amp; nums[nums[i] - 1] != nums[i]) { swap(nums, i, nums[i] - 1); } } for (int i = 1; i \u0026lt;= n; i++) { if (nums[i - 1] != i) { return i; } } return n + 1; } private void swap(int[] nums, int i1, int i2) { int tmp = nums[i1]; nums[i1] = nums[i2]; nums[i2] = tmp; } 小结 原地哈希的固定套路： 使用已有的数组空间 根据数据特征来映射数据到数组中 找重复： 先归位 归位重复的即为重复项 找缺失： 先归位（根据题干适当检查数据范围） nums[i]!=i+1的即为缺失项 ","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/inplace-hash/","summary":"\u003cp\u003e了解原地哈希思路，使用其解决LC几道代表性题目。\u003c/p\u003e","title":"使用「原地哈希」解决一类问题"},{"content":"笔记本突然没法上网了，准确地说是能用微信但是浏览器无法上网，简单排查处理了下，这里记录下思路、过程。\n背景 症状 对键盘校验了下电容，软件没关，吃饭回来后发现电脑重启了。然后电脑就出现了问题：\n能用微信； 浏览器无法上网； 环境 我的笔记本是一台MacBook Pro (Retina, 15-inch, Mid 2015)。\n系统版本：10.15.6 (19G2021)。\n思路 检查WiFi是否通 检查ping是否能通，用dig同理 检查DNS配置 检查代理问题 搜索苹果论坛 操作 排除WiFi故障 首先排除了WiFi问题，在别的设备上网络完全OK。\n排除网路故障 其次使用ping/dig随便一个服务域名都能通，说明网络层是OK的。\n这里如果想验证某个服务是否有问题，可以使用 www.websitedown.info 这类测试网站。\ndig可以方便地查看是否有DNS解析的问题。\n处理DNS配置 在设置界面-\u0026gt;网络-\u0026gt;DNS选项卡可以配置一些参数： 直接改写文件也可以：cat /etc/resolv.conf。\n酌情增加开放DNS服务地址：\nDNS服务器 地址 备用地址 官网 114 DNS 114.114.114.114 114.114.115.115 https://www.114dns.com/ 114 DNS Safe 114.114.114.119 114.114.115.119 https://www.114dns.com/ Ali DNS 223.5.5.5 223.6.6.6 http://alidns.com/ DNSPod 119.28.28.28 / 119.29.29.29 182.254.116.116 / 182.254.118.118 https://www.dnspod.cn/Products/Public.DNS 根据苹果社区上一条老的记录，这里有时候可能需要重启下mDNSResponder进程： 参考：Connected to Internet But Can\u0026rsquo;t Browse?\n修改代理设置 处理这次问题的时候我找到了代理选项卡，把不需要的模式关闭了。 End 至此，经过一轮尝试、排查，问题解决。\nRef Connected to Internet But Can\u0026rsquo;t Browse? ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/macos/network/network-connected-but-cant-browse/","summary":"\u003cp\u003e笔记本突然没法上网了，准确地说是能用微信但是浏览器无法上网，简单排查处理了下，这里记录下思路、过程。\u003c/p\u003e","title":"解决MacOS突然无法上网问题"},{"content":"LeetCode 一类题型解析，二分搜索变体：单调序列，单调序列是指非严格递增、非严格递减。\n单调序列 概念学习 我们直接看图吧： 蓝线：表示一个非严格递增序列，其中 blue[1]==blue[2]，由于有等值元素存在，所以非严格递增； 绿线：表示一个非严格递减序列，其中 gree[2]==green[3]，由于有等值元素存在，所以非严格递减； 蓝绿线都是单调序列； 本文通过LC上的实际题目来感受、理解其特征、运用。\n二分运用 假设我们处理的序列都是升序的。\n朴素二分-找目标值任一位置 朴素二分本质上是通过二分来查找某一个元素的位置，只要找到元素等同于目标值，就可以返回位置。\n代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public static int bsearch(int[] nums, int target) { int low = 0; int high = nums.length - 1; while (low \u0026lt;= high) { int middle = low + ((high - low) \u0026gt;\u0026gt; 1); if (nums[middle] \u0026gt; target) { // 重置右边界 high = middle - 1; } else if (nums[middle] \u0026lt; target) { // 重置左边界 low = middle + 1; } else { return nums[middle]; } } // 不存在返回-1 return -1; } 变体-找目标值第一个位置 在朴素类型的基础上，增加一个题干：元素有等值情况，查找元素在序列中的第一个位置。\n找最后一个位置是同理的，边界、挪动方向不同而已。\n在上述代码基础上，稍做修改：找到等值元素后，我们尝试往左侧挪动，这里增加边界的判断即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public static int bsearchFindFirst(int[] nums, int target) { int low = 0; int high = nums.length - 1; while (low \u0026lt;= high) { int middle = low + ((high - low) \u0026gt;\u0026gt; 1); if (nums[middle] \u0026gt; target) { // 挪动右边界 high = middle - 1; } else if (nums[middle] \u0026lt; target) { // 挪动左边界 low = middle + 1; } else { // 到达左边界 || 左侧的值已经不符合等值条件了 if (middle == 0 || nums[middle - 1] \u0026lt; target) { return middle; } // 往前查找 high = middle - 1; } } // 不存在目标，返回-1 return -1; } 变体-找大于等于目标值的第一个位置 有序序列中，有可能不存在等值元素，这种时候我们的\u0026gt;=target情况处理就一致了，在上面的代码基础上，我们合并\u0026gt;= if分支。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public static int bsearchFindFirstBiggerOrEqual(int[] nums, int target) { int low = 0; int high = nums.length - 1; while (low \u0026lt;= high) { int middle = low + ((high - low) \u0026gt;\u0026gt; 1); // 合并了 \u0026gt; = 分支 if (nums[middle] \u0026gt;= target) { if (middle == 0 || nums[middle - 1] \u0026lt; target) { return middle; } high = middle - 1; } else if (nums[middle] \u0026lt; target) { low = middle + 1; } } // 不存在目标，返回-1 return -1; } 到这里，关于二分的基础做法都覆盖到了。\nLC单调序列题目 我们上面有一个假定：题干中直接给出了一个有序序列（大多数时候是一个有序数组）。\n而力扣上我们今天要聊的这些题，题干中并没有直接给出这个序列，单调有序序列需要我们根据题干条件自己来写。\n我把这类题叫做「生成单调序列」类型题目。\n如下是相关题目：\n题目 单调序列（y=f(x)） 875. 爱吃香蕉的珂珂 hours=f(speed)：吃得越快，花时间越少。单调递减函数。 1011. 在 D 天内送达包裹的能力 days=f(load)：船的负载能力越大，消耗天数越少。单调递减函数。 1552. 两球之间的磁力 ballsCnt=f(distance)：球与球之间距离越大，能放的球数越少。单调递减函数。 1482. 制作 m 束花所需的最少天数 flowerCnt=f(days)：等越久，成熟的花朵越多。单调递增函数。 LCP 12. 小张刷题计划 days=f(speed)：小张做题越快，花费时间越少。单调递减函数。 可以看到我标出了单调序列，根据题干稍做分析，不难总结出这个单调函数。\n下面我们以实际题目为例来做个示范。\n题目 153. 寻找旋转排序数组中的最小值 LeetCode 153. 寻找旋转排序数组中的最小值 二分题解\n这道题展示的是非单调序列案例，不是本文要看的单调序列，放这里是为了做个对比。\n可以看到非单调的序列，会出现 先递增再递减、先递减再递增 的情况。\n154题目相比153增加存在等值元素的题干。154符合上图中的蓝线特征。\n875. 爱吃香蕉的珂珂 https://leetcode.cn/problems/koko-eating-bananas/\n思路 根据上面的分析，我们需要抽象出一个单调函数y=f(x)。\n抽象单调函数 一般题目所求就是自变量x，也就是速度，koko吃香蕉的速度决定了吃完的时间，而题目又限定了吃完的时间为h。\n因此我们的单调函数表示吃完的时间，自变量为吃香蕉的速度：hours=f(speed)：吃得越快，花时间越少。单调递减函数。\n到这一步我们当然可以从最大速度逐个去找，这样复杂度就是O(maxSpeed-minSpeed)。使用二分则可以降低复杂度。\n我们每次取当前中间速度，求出对应耗费时间，与h对比，逐步逼近。\n题干中还限制koko一次只能吃一堆，因此koko吃香蕉的速度被限制在了[1,max(piles)]，最慢一次吃一根，最快一次把一堆都整完。这样二分的初始边界l r就有了。\n函数逻辑 设速度为speed，我们一共有piles个香蕉，依次计算每个香蕉吃几次。piles[i] % speed \u0026gt; 0时，说明当前香蕉需要piles[i] / speed + 1的时间，否则仅需商个时间。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public int minEatingSpeed(int[] piles, int h) { // 二分边界，最小速度应该是1，最大速度应该是一堆香蕉的最大值 int low = 1, high = 0; for (int pile : piles) { high = Math.max(high, pile); } // 循环 logm 轮 while (low \u0026lt;= high) { int mid = low + ((high - low) \u0026gt;\u0026gt; 1); long hoursNeed = hoursBySpeed(mid, piles); if (hoursNeed \u0026lt;= h) { // 等值区间是否到达边界 if (mid == 1 || hoursBySpeed(mid - 1, piles) \u0026gt; h) { return mid; } // 不是边界，high可以左移 high = mid - 1; } else if (hoursNeed \u0026gt; h) { // mid此时不在答案处，low处于非解区间，low可以右移 low = mid + 1; } } return -1; } /** * 复杂度 O(n) * 速度越大，所需时间越少 * hoursBySpeed 是一个递减函数 */ private long hoursBySpeed(int speed, int[] piles) { long hours = 0; for (int pile : piles) { hours += pile % speed == 0 ? pile / speed : pile / speed + 1; } return hours; } 复杂度分析 时间 设len(piles)=n，每次二分均需求一次n下的速度。\n二分整体循环log(max(piles)-1)次。\n整体时间复杂度O(len(piles) * log(max(piles)-1))。\n空间 空间复杂度O(1)。\n小结 碰到这类题，首先我们可以回顾下二分的用法，理解了边界挪动之后，二分就可以作为框架使用。\n而针对题干中的单调函数，则需要仔细分析题干，必要时可以画图、分析用例进行逐步推导。\n","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/b-search-monotonic-sequence/","summary":"\u003cp\u003eLeetCode 一类题型解析，二分搜索变体：单调序列，单调序列是指非严格递增、非严格递减。\u003c/p\u003e","title":"解决「生成单调序列」类型题目"},{"content":"LeetCode 895. 最大频率栈 题解，熟练掌握数据结构、集合的解法。\n题目 1895. 最大频率栈 https://leetcode.cn/problems/maximum-frequency-stack/\n设计一个类似堆栈的数据结构，将元素推入堆栈，并从堆栈中弹出出现频率最高的元素。\n实现 FreqStack 类:\nFreqStack() 构造一个空的堆栈。 void push(int val) 将一个整数 val 压入栈顶。 int pop() 删除并返回堆栈中出现频率最高的元素。 如果出现频率最高的元素不只一个，则移除并返回最接近栈顶的元素。 思路 这道题与 LeetCode 146. LRU 缓存 双向链表题解 类似。\n都是在基础数据结构之上，增加了应用层功能。\nLRU中，我们使用了链表来维护一个队列：\n数据写入时： 如果数据已存在，则删除队列已有元素，将元素插入到队头； 如果数据不存在，则直接将元素插入到队头； 数据读取时： 数据不存在，不做任何操作； 数据存在： 返回找到的数据（可能是KV结构）； 删除数据原位置，将数据插入到队头； 为了提高效率，我们使用了双向链表+map，方便定位数据、原地删除数据、头尾增删数据。言下之意，我们可以使用粗暴的解决方式，只是效率不高。\nLRU中，在原始集合基础上，对数据访问（读、写）增加了前置元素到队头的逻辑。\n而本题所求最大频率栈，则是在栈的基础上，对对数据访问（读、写）增加了更新频率的逻辑。\n维护频率值+map 思路 按照上述分析，我们需要维护、更新：\n频率 集合元素最大频率 每个元素对应频率 栈 负责读写 复杂度分析 时间 push 操作均为map/stack O(1)复杂度 pop 操作均为map/stack O(1)复杂度 空间 使用了额外的map stack，空间占用O(n)。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 public class FreqStack { /** * 值 ——》频率 */ Map\u0026lt;Integer, Integer\u0026gt; val2Freq; /** * 频率--》栈结构 */ Map\u0026lt;Integer, Deque\u0026lt;Integer\u0026gt;\u0026gt; freq2Stack; /** * 当前最大频率 */ int maxFreq; public FreqStack() { val2Freq = new HashMap\u0026lt;\u0026gt;(); freq2Stack = new HashMap\u0026lt;\u0026gt;(); maxFreq = 0; } void push(int val) { // val对应频率+1 Integer freq = val2Freq.getOrDefault(val, 0) + 1; // 更新val频率 val2Freq.put(val, freq); freq2Stack.putIfAbsent(freq, new LinkedList\u0026lt;\u0026gt;()); // val入栈新频率stack freq2Stack.get(freq).offerLast(val); // 当前数据频率最大，更新maxFreq maxFreq = Math.max(maxFreq, freq); } int pop() { // 根据maxFreq获取栈 Deque\u0026lt;Integer\u0026gt; stack = freq2Stack.get(maxFreq); if (stack.isEmpty()) { return -1; } Integer val = stack.pollLast(); // 更新val对应频率，频率-1 val2Freq.put(val, val2Freq.get(val) - 1); // 栈空，表示当前频率下没元素了，最大频率-1 if (stack.isEmpty()) { maxFreq--; } return val; } } ","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/maximum-frequency-stack/","summary":"\u003cp\u003eLeetCode 895. 最大频率栈 题解，熟练掌握数据结构、集合的解法。\u003c/p\u003e","title":"LeetCode 895. 最大频率栈 题解"},{"content":"了解Redis如何优化内存开销，总结、学习其思想。\nRedis官方文档对内存优化有专门讲解：Memory optimization。这也是本文主要参考的内容源。\n当我们出方案，省内存成为第一要务时，本文就值得参考。\n当然，这里的原理只能作为我们做设计权衡的一种参考，具体情况需要我们自己分析CPU、内存、复杂度、运行效果。\n小集合使用紧凑型存储编码 Redis从2.2版本开始引入了这种策略：针对阈值之下的集合类型，使用紧凑型、连续内存存储的encoding。\n在 译\u0026mdash;在Redis中存储亿万级的简单KV数据 中的案例就是用了这种策略。\n相关阈值配置项：\n1 2 3 4 5 hash-max-ziplist-entries 512 hash-max-ziplist-value 64 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 set-max-intset-entries 512 hash、zset数据结构会使用ziplist压缩内存，set则使用intset。\nziplist核心是紧凑、连续内存存储，对CPU缓存本地性更友好\nintset核心是只存整形数字，没有对象属性的开销\n特定数据结构详解，我们起别的文章说明\nentries表示集合内元素数量，value则表示值的内存大小。在阈值内时，使用紧凑型的数据结构，仅需一点CPU运算即可拿到对应数据，而内存占用则大大减少。\n这里配置项如果调大，官方建议多做测试。\n同时，SDS数据结构内部也针对不同元素大小做了编码压缩的处理：\nint == 只保存整形数字：str 为整形数字时 embstr == 连续内存：str \u0026lt; 44byte raw == SDS：44byte \u0026lt; str \u0026lt; 512 MB 使用32位机器 ES为什么建议使用32或者26GB的堆？ 中关于指针压缩的说明与此处同理。\n用32位机器来运行Redis，每个key的指针相比64位，会占用更少的内存。但是有总内存4GB的限制。\n当然，如果Redis实现了类似于指针压缩的技术，理论上这里可以突破4GB的限制。\n位操作 从2.2版本开始，Redis引入了位操作，原理可参考 BitMap结构实现及使用场景。\n位图非常适用于一些二值统计。其变体BloomFilter这种概率性结构也适用于数据判重。而完成巨量数据的操作，只需极小的内存占用。\n用hash结构替代字符串 hash:\nkey field value 业务内聚性 我们用Redis很多时候直接会用字符串结构来存储value，但实际上有些KV从业务上来说就是面向对象的，有其内聚性。比如User信息，我们直接将User需要的KV都存到hash结构即可。\n这样会共用key，也节省了内存占用。\n面向省内存抽象建模 简单来说，除了上一条中的本身业务上就内聚、有关联的情况，我们可以考虑在应用层做一些抽象的设计。\n这里抽象应该是指提取共性，共用hash中的key，以达到省内存的目的\n官方文档举了个例子，效果上很像为数据做了一层分区，例子对数据按100个一组来分区存储：\n假设我们的数据是这样的：\n1 2 3 object:102393 object:1234 object:5 对object:1234操作时，对应的key为object:12，而field为34，value不变。\n也就是说：key==object:12时，field应该包含了[01,99]。\n针对object:5这种前缀小于100的情况，应用层可以特殊处理。\n本质上，这里的抽象是做了归类、分区（sharding）。效果上，与上一条一致，通过共用key节省内存占用。\n从绝对的时间复杂度上，这里需要一些CPU运算，但是均摊复杂度仍然与hash的O(1)一致，同时紧凑型连续内存存储，对CPU缓存更友好。\n共用key没有作为全局的通用设计，原因是Redis需要支持缓存过期等策略，共用key不好处理这些情况。而例如在Redis使用整形数字，类似于JDK中的Integer，Redis也会对数字进行缓存共享。\n关于内存分配 Redis会根据maxmemory阈值配置来分配最大内存，如果没有设置，可能会有内存耗尽的风险。生产环境官方建议配置maxmemory。\n内存惰性释放 常驻内存占用取决于实例峰值内存，也就是说申请Redis内存资源时，需评估峰值用量。\nRedis同时也会记录有效数据的内存大小。\n即使有些key过期或者被删除了，Redis也不会主动释放内存，这点由malloc()实现决定。\nResident Set Size RSS：进程占用的内存页数\n重用内存页 当添加新的KV时，可以观察到RSS内存页数是稳定的，这是因为内存分配会重用之前已经free的内存页。\n内存碎片率不准 碎片率 = 进程峰值占用的内存页数 / 当前使用的内存\n基于上述逻辑，可得，内存碎片率是不准的。\n当峰值内存占用很大时，RSS很大，而当很多内存实际已经被删除后（未真正释放），当前使用内存很小，RSS/当前使用内存 会比较大。并不能完全体现内存真实的碎片状态。\n小结 综上，Redis优化内存占用的策略有：\n小集合使用紧凑型存储编码 使用32位机器 位操作 用hash结构替代字符串 同时，也简单了解了操作系统内存占用、释放、碎片率的基本逻辑，有助于我们理解Redis内存使用的真实情况。\nRef Redis Memory optimization ","permalink":"https://redolog.github.io/posts/rd/storage/redis/memory-optimization/","summary":"\u003cp\u003e了解Redis如何优化内存开销，总结、学习其思想。\u003c/p\u003e","title":"Redis内存优化策略"},{"content":"LeetCode 112/113. 路径总和，两道同类树形路径问题，熟悉DFS、回溯解法。\n题目 112. 路径总和 https://leetcode.cn/problems/path-sum/\n给你二叉树的根节点 root 和一个表示目标和的整数 targetSum 。判断该树中是否存在 根节点到叶子节点 的路径，这条路径上所有节点值相加等于目标和 targetSum 。如果存在，返回 true ；否则，返回 false 。\n叶子节点 是指没有子节点的节点。\nDFS 思路 树形遍历，我习惯用DFS处理。我们只需逐个节点遍历、求和检查即可。\n同时为了避免传递多余的targetSum变量，在每次进入dfs，我们都可以实时更新值。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 static class Recursion { public boolean hasPathSum(TreeNode root, int targetSum) { return dfs(root, targetSum); } private boolean dfs(TreeNode node, int targetSum) { if (node == null) { return false; } if (node.left == null \u0026amp;\u0026amp; node.right == null \u0026amp;\u0026amp; node.val == targetSum) { return true; } return dfs(node.left, targetSum - node.val) || dfs(node.right, targetSum - node.val); } } 复杂度分析 时间 n表示树节点个数 整个过程遍历一次完整树，时间复杂度O(n)。\n空间 正常情况下空间占用为递归调用栈，即树的高度，空间复杂度O(logn)。\n树形退化为链表时，空间占用为n个元素，空间复杂度O(n)。\n113. 路径总和 II https://leetcode.cn/problems/path-sum-ii/\n给你二叉树的根节点 root 和一个整数目标和 targetSum ，找出所有 从根节点到叶子节点 路径总和等于给定目标和的路径。\n叶子节点 是指没有子节点的节点。\nDFS回溯 思路 在112的基础上，113题要求返回符合和条件的路径，因此我们除了最外层的ansList，肯定也需要内部的一个集合来记录、回溯路径。\n我们用回溯的做法来遍历、求解。\n我们使用Deque\u0026lt;Integer\u0026gt; innerQueue记录子节点的处理状态，同时node.left!=null时，回溯相应添加状态。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 public class PathSumII { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ans; Deque\u0026lt;Integer\u0026gt; innerQueue; public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; pathSum(TreeNode root, int targetSum) { ans = new ArrayList\u0026lt;\u0026gt;(); innerQueue = new ArrayDeque\u0026lt;\u0026gt;(); dfs(root, targetSum); return ans; } private void dfs(TreeNode node, int targetSum) { if (node == null) { return; } innerQueue.offerLast(node.val); if (node.left == null \u0026amp;\u0026amp; node.right == null \u0026amp;\u0026amp; node.val == targetSum) { ans.add(new ArrayList\u0026lt;\u0026gt;(innerQueue)); return; } dfs(node.left, targetSum - node.val); if (node.left != null) { innerQueue.pollLast(); } dfs(node.right, targetSum - node.val); if (node.right != null) { innerQueue.pollLast(); } } } 复杂度分析 时间 n表示树节点个数 整个过程遍历一次完整树，时间复杂度O(n)。\n空间 树形退化为链表时，空间占用为n个元素，空间复杂度O(n)。\n","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/path-sum-i-ii/","summary":"\u003cp\u003eLeetCode 112/113. 路径总和，两道同类树形路径问题，熟悉\u003ccode\u003eDFS\u003c/code\u003e、回溯解法。\u003c/p\u003e","title":"LeetCode 112/113. 路径总和 DFS、回溯题解"},{"content":"LeetCode 121. 买卖股票的最佳时机 DP题解，熟悉简单DP问题。\n题目 121. 买卖股票的最佳时机 https://leetcode.cn/problems/best-time-to-buy-and-sell-stock/\n给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。\n你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。\n返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。\nDP 思路 李煜东著《算法竞赛进阶指南》，摘录如下：：\n动态规划三要素：\n状态； 阶段； 决策； 动态规划求解的三个基本条件：\n子问题重叠性； 无后效性； 最优子结构性质； DP解题关键要素：\n要素 状态表示 dp[i]表示当前能买入的最低价格、prices[i]表示当前天卖出的价格； 阶段划分 子序列结尾； 转移方程 dp[i] = Min(prices(0,i)) 边界 dp[0] = prices[0] 目标 Max(prices[i]-dp[i]) (0\u0026lt;=i\u0026lt;n) 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public int maxProfit(int[] prices) { int n = prices.length; // dp数组存储当前时刻前买入的最低价格 int[] dp = new int[n]; dp[0] = prices[0]; int ans = 0; for (int i = 1; i \u0026lt; prices.length; i++) { dp[i] = Math.min(dp[i - 1], prices[i]); // 判断当前时刻卖出是否划算？ if (prices[i] - dp[i] \u0026gt; ans) { ans = prices[i] - dp[i]; } } return ans; } 观察发现，上述dp[]其实无需保留每一项的最小值，我们仅需记录窗口内的最小值即可，可通过滚动数组来优化空间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /** * 使用滚动数组优化：只需维护窗口内的最低买入价格即可 */ static class ScrollArray { /* * 执行用时： * 2 ms * , 在所有 Java 提交中击败了 * 58.30% * 的用户 * 内存消耗： * 58.2 MB * , 在所有 Java 提交中击败了 * 5.05% * 的用户 * 通过测试用例： * 211 / 211 */ public int maxProfit(int[] prices) { int minBuyPrice = prices[0]; int ans = 0; for (int i = 1; i \u0026lt; prices.length; i++) { minBuyPrice = Math.min(minBuyPrice, prices[i]); ans = Math.max(prices[i] - minBuyPrice, ans); } return ans; } } 复杂度分析 时间 整个过程仅需遍历一次入参数组，时间复杂度O(n)。\n空间 写法1我们定义了prices大小的dp数组，空间复杂度O(n)。\n写法2优化了空间占用，空间复杂度O(1)。\n","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/best-time-to-buy-and-sell-stock/","summary":"\u003cp\u003eLeetCode 121. 买卖股票的最佳时机 DP题解，熟悉简单DP问题。\u003c/p\u003e","title":"LeetCode 121. 买卖股票的最佳时机 DP题解"},{"content":"LeetCode 470. 用 Rand7() 实现 Rand10() 进制转换题解，解决一道等概率类型的代表题。\n题目 470. 用 Rand7() 实现 Rand10() https://leetcode.cn/problems/implement-rand10-using-rand7/\n给定方法 rand7 可生成 [1,7] 范围内的均匀随机整数，试写一个方法 rand10 生成 [1,10] 范围内的均匀随机整数。\n你只能调用 rand7() 且不能调用其他方法。请不要使用系统的 Math.random() 方法。\n每个测试用例将有一个内部参数 n，即你实现的函数 rand10() 在测试时将被调用的次数。请注意，这不是传递给 rand10() 的参数。\n二进制转换 思路 由题干可得：rand7 可随机等概率生成 [1,7] 的整数。\n求一个可同样等概率生成 [1,10] 整数的函数。\n一次调用rand7中，无法生成[8,10]。\n尝试多次调用、组合生成的值 比如，rand7 + rand7，我们看看取值分布：\n取值并非等概率，并且无法通过分类来区分各个和的含义。\n按二进制位生成值 上面的思路行不通的原因是，求和取值分布不均匀。\n我又产生了新的思路：\n将rand7改成只生成0/1二进制值的函数； 然后组合二进制的 0、1、2、3位，生成一个[0,2^4]等概率的区间； 对二进制转成十进制表示的 [0,2^4] 区间，我们舍弃 [1,10] 区间外的值； 这种组合生成值的方式，由于每次调用生成的值含义不同（位不同），因此值的分布是均匀的。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 static class Binary { /** * 执行用时： * 17 ms * , 在所有 Java 提交中击败了 * 13.32% * 的用户 * 内存消耗： * 46.8 MB * , 在所有 Java 提交中击败了 * 77.39% * 的用户 * 通过测试用例： * 12 / 12 */ public int rand10() { int ans; do { ans = rand16(); } while (ans \u0026lt; 1 || ans \u0026gt; 10); return ans; } public int rand16() { return (epBaseRand7() \u0026lt;\u0026lt; 3) + (epBaseRand7() \u0026lt;\u0026lt; 2) + (epBaseRand7() \u0026lt;\u0026lt; 1) + epBaseRand7(); } /** * 基于rand7实现一个0-1等概率函数 */ public int epBaseRand7() { int ans; do { ans = rand7(); } while (ans == 4); return ans \u0026lt; 4 ? 0 : 1; } public int rand7() { return (int) (Math.random() * 7 + 1); } } 复杂度分析 时间 期望复杂度为 O(1)，最坏情况下为 O(∞)。\n空间 无额外空间占用，空间复杂度O(1)。\n七进制转换 思路 在二进制转换中，过程有些绕。\n实际上，我们可以直接用rand7 -1 表示七进制生成一位的值。然后使用七进制转成十进制即可，如此一来，只需(rand7() - 1) * 7 + rand7() - 1，rand7单次少调用了一些，代码上也更加简洁。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /** * 七进制 */ static class Septenary { /** * 执行用时： * 18 ms * , 在所有 Java 提交中击败了 * 10.50% * 的用户 * 内存消耗： * 47.2 MB * , 在所有 Java 提交中击败了 * 15.73% * 的用户 * 通过测试用例： * 12 / 12 */ public int rand7() { return (int) (Math.random() * 7 + 1); } public int rand10() { int ans; do { ans = (rand7() - 1) * 7 + rand7() - 1; } while (ans \u0026lt; 1 || ans \u0026gt; 10); return ans; } } 进阶问题优化 你能否尽量少调用 rand7() ?\n上述我们的实现中，每次rand7()组合调用的值，如果不在[1,10]，我们进行了重试，极端情况下，rand7()会调用多次。\n实际上，我们可以将组合出来的值进行区间映射：\n针对二进制转换，我们使用 (epBaseRand7() \u0026lt;\u0026lt; 4) + (epBaseRand7() \u0026lt;\u0026lt; 3) + (epBaseRand7() \u0026lt;\u0026lt; 2) + (epBaseRand7() \u0026lt;\u0026lt; 1) + epBaseRand7() 来生成[0,32]，其中[0,29] + 1均表示为[1,10]的倍数区间，如此一来，我们只需抛弃等概率分布下的[30,32]取值； 针对七进制转换，同样使用 (rand7() - 1) * 7 + rand7() - 1 生成[0,49]，其中[0,39] + 1均表示为[1,10]的倍数区间； 通过区间映射，我们可以提高实际运行中生成随机数的利用率。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /* * 执行用时： * 5 ms * , 在所有 Java 提交中击败了 * 99.37% * 的用户 * 内存消耗： * 46.9 MB * , 在所有 Java 提交中击败了 * 64.81% * 的用户 * 通过测试用例： * 12 / 12 */ /** * 进阶：将生成的 [11,40] 区间映射到 [1,10]，提高随机生成数的利用率 */ public int rand10Advanced() { int ans; do { ans = (rand7() - 1) * 7 + rand7() - 1; } while (ans \u0026lt; 1 || ans \u0026gt; 40); return ans % 10 + 1; } Ref 【宫水三叶】k 进制诸位生成 + 拒绝采样 ","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/implement-rand10-using-rand7/","summary":"\u003cp\u003eLeetCode 470. 用 Rand7() 实现 Rand10() 进制转换题解，解决一道等概率类型的代表题。\u003c/p\u003e","title":"LeetCode 470. 用 Rand7() 实现 Rand10() 进制转换题解"},{"content":"从零实现一个CopyOnWrite写时复制的map，同时了解写时复制的使用场景。\n概念 wiki:\nCopy-on-write (COW), sometimes referred to as implicit sharing[1] or shadowing,[2] is a resource-management technique used in computer programming to efficiently implement a \u0026ldquo;duplicate\u0026rdquo; or \u0026ldquo;copy\u0026rdquo; operation on modifiable resources.[3] If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred until the first write. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations.\nCopyOnWrite算法，一般中文叫「写时复制」。\n核心思想：\n延迟写； 将写开销延迟到真正数据变化的时刻； 修改数据时，不直接修改原资源引用，而是拷贝一份副本，在副本上更新数据，之后将数据引用直接指向副本，在此之前读取到旧数据的地方仍然引用旧的数据引用； 而读取则共享一份资源； 适用场景：\n由于有多个读写操作，因此适用于多 worker（进程、线程） 上下文（JVM中我们考虑多线程场景）； 读多写少； 可容忍读取快照，对数据一致性要求较低； 实现 COWMap 我们实现一个适用于很少更新场景的配置类map，如黑白名单的更新很少。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public class COWMap\u0026lt;K, V\u0026gt; { /** * volatile 提供JVM可见性、有序性语义，保证集合有写入后，其他线程可以感知到变化 */ private volatile Map\u0026lt;K, V\u0026gt; internalMap; public COWMap() { internalMap = new HashMap\u0026lt;\u0026gt;(); } public COWMap(Map\u0026lt;K, V\u0026gt; map) { internalMap = map; } public V get(K key) { return internalMap.get(key); } public void put(K key, V val) { synchronized (this) { // 先拷贝一份副本 Map\u0026lt;K, V\u0026gt; snapshot = new HashMap\u0026lt;\u0026gt;(internalMap); // 副本修改数据 snapshot.put(key, val); // 直接将 internalMap 替换为副本，这里写入保障可见性语义 internalMap = snapshot; } } } 关键操作 get读数据 多个线程读取数据时公用一份资源，即internalMap。\n同时为了明确JVM提供的读写一致语义，我们用volatile修饰。\nput写数据 更新数据时，上锁，保证同一时刻只有一个线程进入更新逻辑。\n步骤：\n先拷贝一份副本； 副本修改数据； 直接将 internalMap 替换为副本，这里写入保障可见性语义； 应用场景 通过例子可以更好地理解CopyOnWrite。\n虚拟内存管理 之前在 研究一下fork函数 中正好对Linux的fork调用做了一些解读。\n简单理解： 主进程调用fork生成子进程。在没有新数据更新的情况下，子进程共享父进程的页表映射，主进程内存发生修改时，子进程才将对应更新的页进行拷贝、更新子进程持有的页表。\n此例中的页表、内存，即为共享的资源。\n数据无修改时，多个进程间读取同一份内存。\nC++ STL String类中的写时复制 参考：C++ STL String类中的Copy-On-Write 。\n总结 以上，实现了CopyOnWrite最核心的逻辑，并且列举了主流的一些应用场景。\n写时复制在大多数场景中用处不多，在集合场景也不太适用，但是底层的延迟写、快照思想非常值得学习。\nRef https://en.wikipedia.org/wiki/Copy-on-write ","permalink":"https://redolog.github.io/posts/rd/algo/data-structure/cow-map/","summary":"\u003cp\u003e从零实现一个\u003ccode\u003eCopyOnWrite\u003c/code\u003e写时复制的\u003ccode\u003emap\u003c/code\u003e，同时了解写时复制的使用场景。\u003c/p\u003e","title":"COWMap结构基础实现"},{"content":"从零实现位图BitMap，使用实测数据感受其优劣，同时了解BitMap结构的使用场景。\n概念 wiki:\nA bitmap is the data structure that immediately pops in your head when there\u0026rsquo;s a need to map boolean information for a huge domain into a compact representation. It is a very popular data structure whenever memory space is at a premium. OS kernels (memory pages, inodes, etc.), digital imaging, etc.\n位图BitMap要点：\n紧凑型存储，节约内存占用； 支持二值存储、读取； 话不多说，我们直接来写一个：\n提供的功能包括：\n写 set 设置int i对应值为true； 读 get 读取int i对应布尔值 实现 BooleanArrayBitMap RealBitMap BooleanArrayBitMap 最直观的想法，当然是直接开辟一个boolean[]数组来表达每个下标下的boolean状态。\n当然，事实证明，这个思路符合BitMap部分特征，如api一致，但是不够节省内存。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public class BooleanArrayBitMap { /* * java语言中的boolean占一个字节 * 实测： * 执行 333335874次set操作，共耗时：8170 ms * BooleanArrayBitMap 10 亿容量 一共占用953.6743202209473 MB * * 而表达位图中的二值，仅需要一个位，即可 */ /** * 位图容量 */ int capacity; /** * 存储数组 */ boolean[] arr; public BooleanArrayBitMap(int capacity) { this.capacity = capacity; this.arr = new boolean[capacity]; } public void set(int i) { assert i \u0026lt; capacity \u0026amp;\u0026amp; i \u0026gt;= 0; arr[i] = true; } public boolean get(int i) { assert i \u0026lt; capacity \u0026amp;\u0026amp; i \u0026gt;= 0; return arr[i]; } public void printMemoryInMB() { System.out.println(\u0026#34;BooleanArrayBitMap \u0026#34; + capacity / 100000000 + \u0026#34; 亿容量 一共占用\u0026#34; + MemoryUtils.byte2MB(capacity + 4) + \u0026#34; MB\u0026#34;); } } 基于数组的简单封装，发现内存占用不少。Java的boolean占用1个字节，也就是8个bit位，而其实BitMap的二值只需一个位即可。\nRealBitMap 我们尝试用位操作来处理api。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 /** * 真正基于位存储、操作的位图 * * @author dragonsong @date 2022/7/14 * @see BooleanArrayBitMap 中基于boolean数组，一个boolean就占用一个字节，一个字节中有8位，对于表达二值其实有7位是浪费的 */ public class RealBitMap { /* * 执行 333331423次set操作，共耗时：8762 ms * RealBitMap 10 亿容量 一共占用29.802326202392578 MB * * 操作大致量级的set时，耗时与 BooleanArrayBitMap 接近，而内存占用却节省了 三十倍！ */ /** * 位图容量 */ int capacity; /** * 存储数组 * char占用16位，一个char可表达16个二值 */ char[] arr; public RealBitMap(int capacity) { this.capacity = capacity; // capacity % 16 有余数才需要增加一个char存储 this.arr = new char[capacity % 16 == 0 ? capacity / 16 : capacity / 16 + 1]; } public void set(int i) { assert i \u0026lt; capacity \u0026amp;\u0026amp; i \u0026gt;= 0; // 商命中实际存储的数组下标 int quotient = i / 16; // 余数命中实际操作的位 int remainder = i % 16; // 将1存到 remainder 位置 arr[quotient] |= (1 \u0026lt;\u0026lt; remainder); } public boolean get(int i) { assert i \u0026lt; capacity \u0026amp;\u0026amp; i \u0026gt;= 0; // 商命中实际存储的数组下标 int quotient = i / 16; // 余数命中实际操作的位 int remainder = i % 16; return (arr[quotient] \u0026amp; (1 \u0026lt;\u0026lt; remainder)) != 0; } public void printMemoryInMB() { System.out.println(\u0026#34;RealBitMap \u0026#34; + capacity / 100000000 + \u0026#34; 亿容量 一共占用\u0026#34; + MemoryUtils.byte2MB((double) arr.length / 2 + 4) + \u0026#34; MB\u0026#34;); } } RealBitMap在BooleanArrayBitMap基础上，仅仅修改为了用位操作数据的写入、读取。\n复杂度分析 时间 set 一次位操作，O(1) get 一次位操作，O(1) range 遍历取决于入参范围，O(n) 对比 看看十亿数据量级下的实测对比：\n1 2 3 4 5 执行 333335874次set操作，共耗时：8170 ms BooleanArrayBitMap 10 亿容量 一共占用953.6743202209473 MB 执行 333331423次set操作，共耗时：8762 ms RealBitMap 10 亿容量 一共占用29.802326202392578 MB 操作大致相同量级的set时，RealBitMap与BooleanArrayBitMap耗时接近，而内存占用却节省了 三十倍！\n优劣 pros cons CPU对位操作的优化，使得操作高效，读写性能接近数组寻址 数据量过小时，徒增了操作复杂度 极大节省内存占用 工业应用 位图BitMap非常适合用在一些极大数据量但是内存宝贵、二值判断的场景。\n并且结合一些变体，工业界有很好的应用。\n二值操作 如Redis中的位图。\nRedis中的 SDS 支持位图操作。实现位于 bitops.c。\n在一些比如上下班打卡、xx是否yyy（二值判断）的场景下，能以极低的内存开销完成功能。\nBloomFilter 基于BitMap+hash的概率型数据结构。\n可用于判重、流量快速失败的场景。\nRoaringBitMap 咆哮位图是另一个工业变体。\n核心思路是不同数据量，底层使用不同的结构。\n比如 简析ES/Lucene索引的基本设计原理 中也提到过Lucene底层在5版本之上就应用了 Daniel Lemire 的咆哮位图。关联Jira：LUCENE-5983。\n总结 以上，实现了最基本的位图BitMap结构，并且以实例对比了数值数组，输出了测试数据，列举了主流的工业应用场景。\nRef Introduction to Redis Data Structures: Bitmaps Redis: setbit ElasticSearch: Frame of Reference and Roaring Bitmaps https://github.com/redolog/algorithm-java/tree/main/src/main/java/com/algorithm/dataStructure/bit/bitmap ","permalink":"https://redolog.github.io/posts/rd/algo/data-structure/bitmap/","summary":"\u003cp\u003e从零实现位图\u003ccode\u003eBitMap\u003c/code\u003e，使用实测数据感受其优劣，同时了解\u003ccode\u003eBitMap\u003c/code\u003e结构的使用场景。\u003c/p\u003e","title":"BitMap结构实现及使用场景"},{"content":"从零实现一个一致性哈希算法，理解+实现。\n概念 wiki:\nIn computer science, consistent hashing is a special kind of hashing technique such that when a hash table is resized, only n/m keys need to be remapped on average where n is the number of keys and m is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation.\n一致性哈希，也叫哈希环，是一种对传统哈希表rehash过程的改进算法。通过创建一个环结构，加入虚拟节点，改变寻址逻辑：根据key产生的hashcode在环上寻找下一个虚拟节点，以此提升增删真实节点之后的性能。\n场景预设 分布式缓存，我们的集群中有多个节点，数据根据key分区到不同节点。\n传统hash路由 假定我们的hash结构现在存储的是 数据key-\u0026gt;访问节点 ，那么通过数据key查询时就可以定位到实际存储的节点，也就起到了数据分区、负载均衡的作用。\n首先回顾下传统hash函数，也叫散列函数，目的是将指定的入参转换成一个固定的key，如hashCode、MD5、Murmurhash。\n而传统hashtable、hashmap的实现下，一旦节点失效，则需要挪动所有的key（因为我们存的是 key-\u0026gt;节点），rehash分布，开销巨大。\n普通hash：一旦节点发生变动，所有数据均参与到rehash搬运中，开销涉及到所有节点对应数据量。\n我先实现了一个基于哈希表的coordinator，协调者是分布式常见组件。\n参见：HashTableCoordinator 。\n一致性哈希路由 而一致性hash，将增删节点后的开销降低到了最低。\n一致性hash：某个节点发生变动，仅有该节点数据分摊到其他节点中，开销涉及到单个节点对应数据量。\n设计思路 使用环状结构存储节点； 对应到代码中则是一个有序集合，我们可以考虑红黑树（足够平衡）、跳表，方便快速找到对应值的后续值，同时插入、删除数据复杂度也很低； 环上，节点之间增加若干虚拟节点，供映射 虚拟节点-\u0026gt;真实节点 的关系； 增加虚拟节点的原因是为了均衡环上数据的分布，因为数据要找节点，而只有真实节点的话，增删一个节点必然导致节点间距不同、分不均； 而这里我们加入了足够多的虚拟节点，每个虚拟节点按顺序关联到真实节点上，虚拟节点在环上分布足够均匀； 新增节点：创建一个真实节点，同时根据hash均匀创建环上的若干虚拟节点； 下线节点：去掉node，也去掉关联的一批虚拟节点，数据key下次查找时根据环上hashcode顺次查找到另一个虚拟节点上，触发对应据的更新，由于虚拟节点足够多，数据更新被延迟加载，同时分布够均匀； 根据key查找节点的过程： 根据key生成hashcode，在环上根据hashcode顺序查找，找到对应虚拟节点，就找到了真实节点； 图例 查找 假设此时来了一个key4查找对应数据，那么先经过我们的murmurhash计算出一个long值，对应到环上的一个位置，也就是跳表或者红黑树上的一个点，这个点之后一定有我们预先插入并且足够散列的一个虚拟节点，通过虚拟节点就可以找到真实节点，进而获取数据。\n如图： 增加新的节点 假设我们新增了节点4，在数据结构中的体现：\n新增节点4存储； 新增虚拟节点到节点4的映射； 同时我们假设一个节点对应10个虚拟节点节点： 可以看到我们的虚拟节点根据hash尽可能均匀的分布在环的各个位置，在我们的数据key足够多的情况下，会有近似 keyCnt/virtualNodeCnt 个key被归属到了新的虚拟节点，新的真实节点会接管这些key数据。\nkeyCnt 总数据量 virtualNodeCnt 虚拟节点总量\n相比传统哈希表的全量rehash开销控制到了最小。\n删除老的节点 集群碰到老的节点下线是很常见的。\n在数据结构中的体现：\n移除节点4存储； 移除虚拟节点到节点4的映射； 此时我们的环图退回到了之前的状态，节点4管理的key根据环依旧可以找到其他节点归属，并且节点4的数据被近似平分到了其他节点上，如图： 实现 单纯看理论总觉得空洞，我们来动手实现一下一致性哈希算法的基本过程。\n环结构选型 环的结构要求：\n顺时针可以找到某个值的下一个值，也就是说数据结构需要有序，并且可以很快找到后续值； 添加新数据需要足够高效； 移除老数据也需要高效； JDK中有序的集合大概有：\n优先级队列 PriorityQueue 但是只能比较方便的获取最值 红黑树 TreeMap 跳表 ConcurrentSkipListMap TreeMap 与 ConcurrentSkipListMap 都可以比较好的满足我们的要求。二者均实现了 SortedMap 接口，可以方便的获取顺序值的前后元素。\n本文我们就用 ConcurrentSkipListMap了。\n提供的功能 初始化环：节点-\u0026gt;虚拟节点映射； 新增节点； 移除节点； 根据数据key获取数据； 代码 参见：ConsistentHashCoordinator\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 public class ConsistentHashCoordinator { /** * 虚拟节点hash：真实节点 映射 * 环的结构使用 SortedMap，实现可选 TreeMap 基于红黑树 或者 ConcurrentSkipListMap 基于跳表 */ private final SortedMap\u0026lt;Long, Node\u0026gt; virtualHash2Node; /** * 一个真实节点对应的虚拟节点数量 */ private final int virtualCntPerNode; /** * 节点读取量 */ Map\u0026lt;Node, Integer\u0026gt; node2ReadCnt; /** * 初始化一致性hash环： 虚拟hash映射到真实节点 * * @param nodeList 节点列表 * @param virtualCntPerNode 一个真实节点对应多少个虚拟节点 */ public ConsistentHashCoordinator(List\u0026lt;Node\u0026gt; nodeList, int virtualCntPerNode) { this.virtualHash2Node = new ConcurrentSkipListMap\u0026lt;\u0026gt;(); this.virtualCntPerNode = virtualCntPerNode; for (Node node : nodeList) { addNode(node); } node2ReadCnt = new LinkedHashMap\u0026lt;\u0026gt;(); } /** * 存入kv数据，根据key获取到hash值，在环上找到下一个虚拟节点，根据环映射获取真实节点 * * @param key K * @param data V */ public void put(String key, Object data) { Node node = getNodeByKey(key); // 节点数据存储 node.put(key, data); } @NotNull private Node getNodeByKey(String key) { Long hash = MurmurHashUtils.hash(key); SortedMap\u0026lt;Long, Node\u0026gt; tailMap = virtualHash2Node.tailMap(hash); // hash值后面如果有值就从后面取，没有就从头取，环状遍历 Long virtualHash = tailMap.isEmpty() ? virtualHash2Node.firstKey() : tailMap.firstKey(); Node node = virtualHash2Node.get(virtualHash); if (node == null) { throw new IllegalStateException(\u0026#34;节点获取失败，数据key:\u0026#34; + key); } return node; } public Object getValueByKey(String key) { Node node = getNodeByKey(key); node2ReadCnt.put(node, node2ReadCnt.getOrDefault(node, 0) + 1); if (node.containsKey(key)) { Object data = node.get(key); // System.out.println(\u0026#34;从node：\u0026#34; + node + \u0026#34; 中查数据，key：：\u0026#34; + key + \u0026#34; value：：\u0026#34; + data); return data; } // System.out.println(\u0026#34;从node：\u0026#34; + node + \u0026#34; 中重载数据，key：：\u0026#34; + key); node.reloadByKey(key); return node.get(key); } /** * 集群新增节点 * * @param newNode 新节点实例 */ public void addNode(Node newNode) { for (int i = 0; i \u0026lt; virtualCntPerNode; i++) { Long hash = MurmurHashUtils.hash(\u0026#34;virtualNodeKey\u0026#34; + newNode.ip + newNode.port + i); virtualHash2Node.put(hash, newNode); } } /** * 集群下线节点 * 删除虚拟节点：真实节点映射后，环上依然存在其他节点以及虚拟的映射，根据key依然可以定位某个节点，如果数据不存在，执行reload * * @param oldNode 已有节点实例 */ public void removeNode(Node oldNode) { Iterator\u0026lt;Map.Entry\u0026lt;Long, Node\u0026gt;\u0026gt; iterator = virtualHash2Node.entrySet().iterator(); while (iterator.hasNext()) { Map.Entry\u0026lt;Long, Node\u0026gt; entry = iterator.next(); Long virtualHash = entry.getKey(); Node node = virtualHash2Node.get(virtualHash); if (node != null \u0026amp;\u0026amp; node.equals(oldNode)) { iterator.remove(); } } } public void clearRead() { node2ReadCnt.clear(); } public void print() { Map\u0026lt;Node, Integer\u0026gt; node2VirtualCnt = new LinkedHashMap\u0026lt;\u0026gt;(); for (Map.Entry\u0026lt;Long, Node\u0026gt; entry : virtualHash2Node.entrySet()) { Node node = entry.getValue(); node2VirtualCnt.put(node, node2VirtualCnt.getOrDefault(node, 0) + 1); } System.out.println(\u0026#34;==============\u0026#34;); System.out.println(node2VirtualCnt); System.out.println(\u0026#34;==============\u0026#34;); System.out.println(\u0026#34;==============\u0026#34;); System.out.println(node2ReadCnt); System.out.println(\u0026#34;==============\u0026#34;); } } UT测试 测试基本流程是否有误； 测试节点分布是否均匀； 参见：ConsistentHashCoordinatorTest\n根据用例的输出，可以方便的看到读取数据的分布情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ============== ============== {Node{ip=\u0026#39;ip2\u0026#39;, port=8089}=1000, Node{ip=\u0026#39;ip3\u0026#39;, port=8068}=1000, Node{ip=\u0026#39;ip5\u0026#39;, port=8058}=1000, Node{ip=\u0026#39;ip4\u0026#39;, port=8078}=1000, Node{ip=\u0026#39;ip1\u0026#39;, port=8088}=1000, Node{ip=\u0026#39;ip6\u0026#39;, port=8038}=1000} ============== ============== {Node{ip=\u0026#39;ip6\u0026#39;, port=8038}=177055, Node{ip=\u0026#39;ip1\u0026#39;, port=8088}=163883, Node{ip=\u0026#39;ip4\u0026#39;, port=8078}=164781, Node{ip=\u0026#39;ip3\u0026#39;, port=8068}=158297, Node{ip=\u0026#39;ip5\u0026#39;, port=8058}=171041, Node{ip=\u0026#39;ip2\u0026#39;, port=8089}=164943} ============== ============== ============== ============== {Node{ip=\u0026#39;ip2\u0026#39;, port=8089}=1000, Node{ip=\u0026#39;ip3\u0026#39;, port=8068}=1000, Node{ip=\u0026#39;ip5\u0026#39;, port=8058}=1000, Node{ip=\u0026#39;ip6\u0026#39;, port=8038}=1000} ============== ============== {Node{ip=\u0026#39;ip6\u0026#39;, port=8038}=259754, Node{ip=\u0026#39;ip2\u0026#39;, port=8089}=233252, Node{ip=\u0026#39;ip3\u0026#39;, port=8068}=247943, Node{ip=\u0026#39;ip5\u0026#39;, port=8058}=259051} ============== ============== ============== ============== {Node{ip=\u0026#39;ip8\u0026#39;, port=9999}=1000, Node{ip=\u0026#39;ip2\u0026#39;, port=8089}=1000, Node{ip=\u0026#39;ip3\u0026#39;, port=8068}=1000, Node{ip=\u0026#39;ip5\u0026#39;, port=8058}=1000, Node{ip=\u0026#39;ip6\u0026#39;, port=8038}=1000} ============== ============== {Node{ip=\u0026#39;ip6\u0026#39;, port=8038}=206454, Node{ip=\u0026#39;ip2\u0026#39;, port=8089}=182593, Node{ip=\u0026#39;ip3\u0026#39;, port=8068}=198483, Node{ip=\u0026#39;ip5\u0026#39;, port=8058}=203033, Node{ip=\u0026#39;ip8\u0026#39;, port=9999}=209437} ============== ============== 达到了期望。\n总结 以上，理论分析了一致性哈希的基本过程、关键点，并实现了最基础的一个一致性哈希结构，输出了用例效果。\nRef https://en.wikipedia.org/wiki/Consistent_hashing ","permalink":"https://redolog.github.io/posts/rd/algo/hash/consistent-hashing/","summary":"\u003cp\u003e从零实现一个一致性哈希算法，理解+实现。\u003c/p\u003e","title":"理解并实现一致性哈希"},{"content":"从零实现一个跳表。\n概念 wiki:\nIn computer science, a skip list is a probabilistic data structure that allows O(logn) search complexity as well as O(logn) insertion complexity within an ordered sequence of n elements. Thus it can get the best features of a sorted array (for searching) while maintaining a linked list-like structure that allows insertion, which is not possible with a static array. Fast search is made possible by maintaining a linked hierarchy of subsequences, with each successive subsequence skipping over fewer elements than the previous one (see the picture below on the right).\n跳表是一种优雅的数据结构。\n简单来说，跳表是链表+二分的合体，结构以及查询的过程如图所示：\n通过在一层链表基础上增加上层索引，使得查询可以利用结构上的二分。\n今天我们来实现一个基本的跳表。\n提供的功能包括：\n查询：V get(K key) 插入：void put(K key, V val) 实现 基于数组维护各层next指针的 SimpleSkipList 基于上下左右节点指针的 SimpleSkipList2 为了方便理解，我这里以SimpleSkipList2为例。\n生成随机level时，默认使用0.5的下一层生成概率。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 /** * 基于上下左右指针结构的跳表 * * @author dragonsong @date 2022/6/28 */ public class SimpleSkipList2\u0026lt;K extends Comparable\u0026lt;K\u0026gt;, V\u0026gt; { /** * 头节点位于跳表最左上，为查询起点 */ Node\u0026lt;K, V\u0026gt; head; /** * k-v 个数 */ int size; /** * 生成数据的最大层数 */ int maxLevel; public SimpleSkipList2() { head = new Node\u0026lt;\u0026gt;(MAX_LEVEL); Node\u0026lt;K, V\u0026gt; curr = head; // head每层都生成 while (curr.level != 0) { Node\u0026lt;K, V\u0026gt; lower = curr.copyWithLowerLevel(); curr.down = lower; lower.up = curr; curr = curr.down; } size = 0; maxLevel = 0; } V get(K key) { Node\u0026lt;K, V\u0026gt; node = getNode(key); if (node == null || node.key == null || node.key.compareTo(key) != 0) { return null; } return node.val; } /** * @param key 待查找的key * @return 查找链表中等值元素，或者查找链路上的prev前序节点 */ Node\u0026lt;K, V\u0026gt; getNode(K key) { Node\u0026lt;K, V\u0026gt; curr = head; while (curr != null) { // 先同层往右走 Node\u0026lt;K, V\u0026gt; next = curr.next; if (next == null) { // 只有head的情况 if (curr.down == null) { return curr; } curr = curr.down; } else { // 已经有数据插入的情况 // 先同层向右找 while (next != null \u0026amp;\u0026amp; next.key.compareTo(key) \u0026lt;= 0) { curr = next; next = next.next; } // 无down节点时，返回待插入位置的前序节点 if (curr.key != null \u0026amp;\u0026amp; curr.key.compareTo(key) == 0 || curr.down == null) { return curr; } // 再往下走 curr = curr.down; } } return null; } void put(K key, V val) { Node\u0026lt;K, V\u0026gt; prev = getNode(key); if (prev == null) { throw new IllegalStateException(); } if (prev.key != null \u0026amp;\u0026amp; prev.key.compareTo(key) == 0) { // 存在：直接更新 prev.val = val; return; } int level = SkipListLevelUtils.genRandomLevel(); maxLevel = Math.max(level, maxLevel); while (prev.level \u0026gt; 0) { prev = prev.down; } Node\u0026lt;K, V\u0026gt; down = null; // 从0往上创建 for (int i = 0; i \u0026lt; level; i++) { Node\u0026lt;K, V\u0026gt; newNode = new Node\u0026lt;\u0026gt;(key, val, i); insertHorizontal(prev, newNode); insertVertical(down, newNode); down = newNode; // 寻找下一个prev，有上取上，无上往回倒 while (prev.up == null) { prev = prev.prev; } prev = prev.up; } ++size; } /** * 纵向插入 * * @param down 下面的节点 * @param newNode 新节点 */ private void insertVertical(Node\u0026lt;K, V\u0026gt; down, Node\u0026lt;K, V\u0026gt; newNode) { if (down == null) { return; } newNode.down = down; down.up = newNode; } /** * 横向插入 * * @param prev 原前序节点 * @param newNode 新节点 */ private void insertHorizontal(Node\u0026lt;K, V\u0026gt; prev, Node\u0026lt;K, V\u0026gt; newNode) { Node\u0026lt;K, V\u0026gt; prevNext = prev.next; prev.next = newNode; if (prevNext != null) { prevNext.prev = newNode; } newNode.next = prevNext; newNode.prev = prev; } /** * 逐层打印跳表结构 */ public void print() { Node\u0026lt;K, V\u0026gt; start = head; while (start.level \u0026gt; maxLevel) { start = start.down; } System.out.println(\u0026#34;maxLevel:\u0026#34; + maxLevel); Node\u0026lt;K, V\u0026gt; curr; while (start != null \u0026amp;\u0026amp; start.level \u0026gt;= 0) { System.out.println(\u0026#34;当前层数：\u0026#34; + start.level); curr = start; while (curr != null) { System.out.print(curr + \u0026#34;--\u0026gt;\u0026#34;); curr = curr.next; } System.out.println(); start = start.down; } } /** * 链表节点 * * @param \u0026lt;K\u0026gt; 键类型，可排序 */ static class Node\u0026lt;K extends Comparable\u0026lt;K\u0026gt;, V\u0026gt; { /** * 保存的键类型 */ K key; /** * 保存的值类型 */ V val; /** * 层数 */ int level; /** * 上下前后节点引用 */ Node\u0026lt;K, V\u0026gt; prev, next, up, down; public Node(int level) { this.level = level; } public Node(K key, V val, int level) { this.key = key; this.val = val; this.level = level; } public Node\u0026lt;K, V\u0026gt; copyWithLowerLevel() { return new Node\u0026lt;\u0026gt;(this.key, this.val, this.level - 1); } @Override public String toString() { return \u0026#34;Node{\u0026#34; + \u0026#34;key=\u0026#34; + key + \u0026#34;, val=\u0026#34; + val + \u0026#39;}\u0026#39;; } } } 关键设计 链表节点 Node 在双向链表基础上，我们增加了上下指针，同时为了方便层数判断，也增加了一个节点的层数。\n查询过程 getNode getNode是实现跳表的核心算法。\n核心过程：\n先同层往右走 再往下找 一直找到最后一层的最右，如果没有等值节点，返回查找最接近的前序节点 注意边界 添加数据 在getNode基础上增加了链表关联指针的逻辑、生成随机层数。\n链表指针关联 代码中我们抽出了两个方法，熟悉链表的同学对这个应该没有问题：\n1 2 insertHorizontal(prev, newNode); insertVertical(down, newNode); 生成随机层数 我们实现的时候假定生成每层的概率是可调整的，复杂度分析按照0.5概率来判断。工程中这里可做调整，甚至可以优化。\n这里可以看看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /** * 1层概率为 DEFAULT_PROBABILITY * 2层概率为 DEFAULT_PROBABILITY^2 * n层概率为 DEFAULT_PROBABILITY^n * 越上层，生成概率越低，分母指数级上升 * * @return level范围为 [MIN_LEVEL,MAX_LEVEL) 左闭右开 */ public static int genRandomLevel() { int level = MIN_LEVEL; while (Math.random() \u0026lt;= DEFAULT_PROBABILITY \u0026amp;\u0026amp; level \u0026lt; MAX_LEVEL) { ++level; } return level; } 这里其实还可以多了解一点等概率，之前我写过一点基本的代码： https://github.com/redolog/algorithm-java/tree/main/src/main/java/com/algorithm/probability\n复杂度分析 空间 每一层生成的基础概率为0.5。\n此时我们的层数分布遵循：最高层维护两个数据，每降低一层 数量减半。 层高设为h，数据量n：\n第一层 n/2 第二层 n/2^2 第h层 n/(2^h)=2 所以 h=logn。\nn/2+n/4+n/8…+8+4+2=n-2 为各层节点的数量和。\n空间复杂度 O(n)，索引层为额外的空间。\n时间 写入、读取复杂度一致，均取决于每层跳转的节点数 以及 跳表高度。\n读取过程：\n从最上层一直往下寻找，最顶层只有 head/tail 元素，顶层最多跳转到tail，大部分情况是从head往下跳。 到了倒数第二层，上一步已经跳过了一半元素，根据同样的步骤，查找元素时，要么不跳往下，要么跳一次（跳两次就到了上一次判断过的边） 因此，每一层跳跃，最多经过三个节点。 此时写入、读取的复杂度为 O(3*log2n)，去掉常数阶、低阶，复杂度为 O(logn)。\n根据大O算法的衡量标准，跳表的时间复杂度与二分一致。\n总结 以上，实现了最基础的跳表结构，并且分析了复杂度，同时简要提及了关键性的设计。\n后续可以结合工业实现来进行优化、对比。\nRef https://en.wikipedia.org/wiki/Skip_list https://github.com/redolog/algorithm-java/tree/main/src/main/java/com/algorithm/dataStructure/skiplist ","permalink":"https://redolog.github.io/posts/rd/algo/data-structure/skiplist/","summary":"\u003cp\u003e从零实现一个跳表。\u003c/p\u003e","title":"跳表结构基础实现"},{"content":"了解LRU工业级实现，以MySQL、Redis为例。\nLRU基本实现及优化 中，我介绍了LRU算法的基本逻辑并给出了实现，本文我们来了解下工业级应用是怎么设计、实现LRU的。\n低频访问的数据，如果继续留存在缓存中的话，会浪费缓存空间、内存。这就是缓存污染。LRU是一种缓存替换、淘汰策略，这种策略核心目的就是尽快淘汰缓存的污染项。\nMySQL 版本：5.7\nMySQL的Buffer Pool是一块缓冲区域，用内存存储了表、索引信息，加速频繁操作的数据读取。\nBuffer Pool结构 借用官方文档的图： Buffer Pool整体依然是一个链表结构，每一个节点是一个page，一个page中可容纳多行数据（对CPU缓存行更友好）。\n相比我们实现的双向链表，MySQL Buffer Pool 链表分成了两大区域：\nNew Sublist New区数据由Old区被频繁访问的数据晋升而来，表示的是读取更多更需要缓存的数据 Old Sublist 新缓存先进入Old区，如无频繁读取则快速淘汰 设计剖析 New区Old区，这是典型的分代设计。默认比例：New5Old3。\n新老代区分，是为了提高缓存的命中率，淘汰不太需要的数据。\n比如有些全量查询会将大量数据丢入Buffer Pool内存，而后续这些数据的读取很少，如果仅用我们之前的实现，这些数据就需要更久的时间被淘汰，也就是缓存的污染程度变高了。\n分代之后，读取更多的数据很快晋升到New区，而Old区比例更小，读取不频繁的数据可以更快被淘汰。\nMonitoring the Buffer Pool Using the InnoDB Standard Monitor 记录了InnoDB Standard Monitor下Buffer Pool相关的监控指标项，其中有的指标与LRU有关。\n小结 InnoDB中的Buffer Pool LRU设计是一种变体，加入了分代的设计，区分了高频、低频访问的数据，能更快地淘汰低频访问的数据，提高了内存的利用率、缓存的命中率。\nRedis 版本：4.0\nRedis中提供了多种淘汰策略：\nnoeviction volatile-random volatile-ttl volatile-lru volatile-lfu allkeys-lru allkeys-random allkeys-lfu 其中volatile表示淘汰针对的是设置了过期时间的数据，allkeys针对所有数据。\nlru后缀则是本文关注的算法相关项。\nRedis通过maxmemory配置服务端的最大内存容量，实际数据超出这个值之后，就开始执行淘汰策略。而volatile类型的key则只要达到过期时间就开始淘汰。\nRedis的LRU设计也是另一种变体，是一种近似的算法：Approximated LRU algorithm。\n每次淘汰时，对maxmemory-samples 5【配置项】个数据进行采样，选其中最应该淘汰的数据剔除内存。\n效果： 设计剖析 Redis源码相对少很多，我们从源码的角度来看看。\nredisObject 全局对象 Redis全局对象redisObject定义了一个lru时间戳字段：\n1 2 3 4 5 6 7 8 9 10 typedef struct redisObject { unsigned type:4; unsigned encoding:4; // lru淘汰策略下是一个全局时间戳，精度为秒级别，LRU_BITS 为24个位，这个值会定时更新 unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or * LFU data (least significant 8 bits frequency * and most significant 16 bits access time). */ int refcount; void *ptr; } robj; db.c 定义db级别的c api lookupKey 函数负责从全局哈希表中找到对应的kv。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 robj *lookupKey(redisDb *db, robj *key, int flags) { dictEntry *de = dictFind(db-\u0026gt;dict,key-\u0026gt;ptr); if (de) { robj *val = dictGetVal(de); /* Update the access time for the ageing algorithm. * Don\u0026#39;t do it if we have a saving child, as this will trigger * a copy on write madness. */ if (server.rdb_child_pid == -1 \u0026amp;\u0026amp; server.aof_child_pid == -1 \u0026amp;\u0026amp; !(flags \u0026amp; LOOKUP_NOTOUCH)) { if (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_LFU) { updateLFU(val); } else { val-\u0026gt;lru = LRU_CLOCK(); } } return val; } else { return NULL; } } 可以看到16行会更新lru时间戳。\nserver.c 定义服务端全局行为 server.c 中的serverCron负责定时任务的执行，注释上可以看到：\n以server.hz配置项为周期运行 通过定时的方式完成异步任务 其中就包括本文涉及的key淘汰任务的触发，这类任务是一种惰性的查找式设计 processCommand 处理请求，其中会检查内存是否达到maxmemory，调用了freeMemoryIfNeeded，以此来触发缓存淘汰流程。\n服务端启动时，调用了evictionPoolAlloc，这里会初始化lru淘汰使用的一个池，内部是evictionPoolEntry 结构，主要记录了keyname、idle字段。\nfreeMemoryIfNeeded 中调用 evictionPoolPopulate 来更新待淘汰的键值数据。\n之后直接调用删除内存的实现。\n采样筛选的逻辑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 // MAXMEMORY_FLAG_LRU|MAXMEMORY_FLAG_LFU || MAXMEMORY_VOLATILE_TTL 淘汰分支 if (server.maxmemory_policy \u0026amp; (MAXMEMORY_FLAG_LRU|MAXMEMORY_FLAG_LFU) || server.maxmemory_policy == MAXMEMORY_VOLATILE_TTL) { struct evictionPoolEntry *pool = EvictionPoolLRU; // 当前还未选出淘汰的key while(bestkey == NULL) { unsigned long total_keys = 0, keys; // 依次处理每个db for (i = 0; i \u0026lt; server.dbnum; i++) { db = server.db+i; // 选择所有的key范围 或者 超时的key范围 dict = (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_ALLKEYS) ? db-\u0026gt;dict : db-\u0026gt;expires; if ((keys = dictSize(dict)) != 0) { // 填充淘汰池 evictionPoolPopulate(i, dict, db-\u0026gt;dict, pool); total_keys += keys; } } // 没有淘汰的key if (!total_keys) break; // 从后往前：从最应该淘汰到最不该淘汰 for (k = EVPOOL_SIZE-1; k \u0026gt;= 0; k--) { if (pool[k].key == NULL) continue; bestdbid = pool[k].dbid; if (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_ALLKEYS) { // 从全局哈希表匹配 de = dictFind(server.db[pool[k].dbid].dict, pool[k].key); } else { de = dictFind(server.db[pool[k].dbid].expires, pool[k].key); } /* Remove the entry from the pool. */ if (pool[k].key != pool[k].cached) sdsfree(pool[k].key); pool[k].key = NULL; pool[k].idle = 0; /* If the key exists, is our pick. Otherwise it is * a ghost and we need to try the next element. */ if (de) { // 筛到的key不为空，赋值给bestkey，供删除用 bestkey = dictGetKey(de); break; } else { /* Ghost... Iterate again. */ } } } } 小结 Redis中实现的LRU变体流程：\nprocessCommand 处理请求，检查内存是否达到maxmemory，定时任务中也会有触发入口； freeMemoryIfNeeded中检查、计算待释放的内存； evictionPoolPopulate更新淘汰池； 采样，选择采样样本中最应该淘汰的bestKey； 执行淘汰； 核心是近似获取待删除的数据key。 近似的目的是为了节省内存，保证精确的链表结构需要维护在内存中。同时定时定量拉取数据保证了淘汰操作只占用恒定的内存，有利于Redis服务端的稳定运行。\n总结 MySQL中主要通过分代来更快淘汰污染的缓存。而Redis主要通过定量拉取、近似采样的方式节省服务端内存，也保证了服务稳定运行。\n可以看到工业级实现会有更多的考虑：\n判断服务端真实使用场景； 节省内存； 保证服务端稳定； Ref MySQL Buffer Pool LRU Algorithm Redis Key eviction Redis source code ","permalink":"https://redolog.github.io/posts/rd/algo/cache/replacement-policy/lru-industrial-case/","summary":"\u003cp\u003e了解\u003ccode\u003eLRU\u003c/code\u003e工业级实现，以\u003ccode\u003eMySQL\u003c/code\u003e、\u003ccode\u003eRedis\u003c/code\u003e为例。\u003c/p\u003e","title":"LRU工业级案例"},{"content":"从零实现一个LRU，同时优化我们的版本。\n概念 wiki:\nDiscards the least recently used items first. This algorithm requires keeping track of what was used when, which is expensive if one wants to make sure the algorithm always discards the least recently used item. General implementations of this technique require keeping \u0026ldquo;age bits\u0026rdquo; for cache-lines and track the \u0026ldquo;Least Recently Used\u0026rdquo; cache-line based on age-bits. In such an implementation, every time a cache-line is used, the age of all other cache-lines changes. LRU is actually a family of caching algorithms with members including 2Q by Theodore Johnson and Dennis Shasha,[5] and LRU/K by Pat O\u0026rsquo;Neil, Betty O\u0026rsquo;Neil and Gerhard Weikum.[6]\nLeast Recently Used 最近最少使用，是一种缓存替换策略。\nLRU意味着最近、最多使用的优先级应该提高。\n特性 容器头表示优先级高 Most Recently Used，容器尾表示优先级低 Least Recently Used 每次近期访问都前置元素的位置 插入新数据时判断capacity/size，如果满了capacity，则释放队尾元素 最近：插入新数据，放到队头 最多：多次访问已有数据，放到队头 wiki给出了一个示例，一个序列中访问顺序：A B C D E D F，缓存容量为4。\n根据特性实现我们的LRU算法，由于跟缓存紧密联系，大家一般都实现叫做LRUMap，我在github上的实现按照底层的结构来命名。\n提供的功能包括：\n访问数据 get(K key) 写入数据 put(K key,V val) 实现 LinkedListLRU ArrayLRU DoubleLinkedListLRU 我们姑且不论复杂度分析，先按照基本逻辑实现一个可用的版本，根据版本缺陷进行优化。\n基于单向链表 首先我们先按照单向链表来实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 public class LinkedListLRU\u0026lt;T\u0026gt; { /** * 最新数据保存在链表头，最老数据逐渐搬移到链表尾 */ LRUListNode\u0026lt;T\u0026gt; dummy; /** * lru容量 */ int capacity; /** * 当前数据量 */ int size; public LinkedListLRU(int capacity) { this.capacity = capacity; this.size = 0; this.dummy = new LRUListNode\u0026lt;T\u0026gt;(); } /** * 每次访问： * - 无数据时返回null * - 有数据时获取到位置，将node搬移到链表头 * \u0026lt;p\u0026gt; * 每次均需遍历整个链表，访问复杂度 O(n) * * @param val 待查询的key * @return 命中的链表节点 */ public LRUListNode\u0026lt;T\u0026gt; get(T val) { LRUListNode\u0026lt;T\u0026gt; prev = dummy; while (prev.next != null) { if (prev.next.val.equals(val)) { LRUListNode\u0026lt;T\u0026gt; root = dummy.next; // 删除curr当前位置 LRUListNode\u0026lt;T\u0026gt; curr = prev.next; prev.next = curr.next; // 挪动curr到链表头 dummy.next = curr; curr.next = root; return curr; } else { prev = prev.next; } } return null; } /** * 每次插入： * - 原来无对应数据时，将val插入到root的位置，同时判断是否达到capacity，如果达到，淘汰链表尾部节点 * - 有数据时，找到原位置并删除，插入到root * \u0026lt;p\u0026gt; * 插入时也需要先遍历链表，复杂度 O(n) * 单插入、删除节点复杂度均为 * * @param val 待插入的val */ public void put(T val) { if (dummy.next == null) { dummy.next = new LRUListNode\u0026lt;\u0026gt;(val); if (size \u0026gt; capacity) { deleteTail(); } ++size; return; } LRUListNode\u0026lt;T\u0026gt; prev = findPrevByVal(val); if (prev == null) { insertByPrev(dummy, val); if (size \u0026gt; capacity) { deleteTail(); } } else { deleteByPrev(prev); insertByPrev(dummy, val); } } private void deleteByPrev(LRUListNode\u0026lt;T\u0026gt; prev) { prev.next = prev.next.next; --size; } /** * 根据前序节点插入 * * @param prev 前序 * @param val 当前节点值 */ private void insertByPrev(LRUListNode\u0026lt;T\u0026gt; prev, T val) { LRUListNode\u0026lt;T\u0026gt; nextTmp = prev.next; LRUListNode\u0026lt;T\u0026gt; newNode = new LRUListNode\u0026lt;\u0026gt;(val); newNode.next = nextTmp; prev.next = newNode; ++size; } /** * 遍历到尾部，遍历复杂度 O(n)，删除复杂度 O(1) */ private void deleteTail() { if (dummy.next == null) { return; } LRUListNode\u0026lt;T\u0026gt; prev = dummy; while (prev.next.next != null) { prev = prev.next; } prev.next = null; --size; } private LRUListNode\u0026lt;T\u0026gt; findPrevByVal(T val) { LRUListNode\u0026lt;T\u0026gt; prev = dummy; while (prev.next != null) { if (prev.next.val.equals(val)) { return prev; } prev = prev.next; } return null; } void print() { System.out.println(this); LRUListNode\u0026lt;T\u0026gt; curr = dummy.next; while (curr != null) { System.out.print(curr.val + \u0026#34;--\u0026gt;\u0026#34;); curr = curr.next; } System.out.println(); } @Override public String toString() { return \u0026#34;LinkedListLRU{\u0026#34; + \u0026#34;capacity=\u0026#34; + capacity + \u0026#34;, size=\u0026#34; + size + \u0026#39;}\u0026#39;; } static class LRUListNode\u0026lt;T\u0026gt; { T val; LRUListNode\u0026lt;T\u0026gt; next; public LRUListNode() { } public LRUListNode(T val) { this.val = val; } } } 缺点 可以看到基于链表可以实现LRU置换的逻辑，但是由于链表单向指针的局限，我们每次查找一个节点、删除节点都需要O(n)复杂度。\n基于数组 开始我先按照只使用数组的思路实现了一版，发现查找没有利用好数组下标随机访问的特性，加入了k2Idx的map结构来加速查找。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 public class ArrayLRU\u0026lt;K, V\u0026gt; { /** * 数组装数据 */ Entry\u0026lt;K, V\u0026gt;[] elements; /** * 队头：恒指向0 */ int head; /** * 队尾：指向待插入的下标 */ int tail; /** * 容量 */ int capacity; /** * key——》数组下标，方便访问，降低查坐标复杂度 */ Map\u0026lt;K, Integer\u0026gt; k2Idx; @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public ArrayLRU(int capacity) { this.capacity = capacity; this.head = tail = 0; elements = new Entry[capacity]; k2Idx = new HashMap\u0026lt;\u0026gt;(MapUtils.capacity(capacity)); } /** * 根据key访问元素： * - 无元素时，返回null * - 有元素时，返回对应entry * \u0026lt;p\u0026gt; * 复杂度： * 每次均遍历整个数组，并且需要搬移前序数据，时间复杂度 O(n) * \u0026lt;p\u0026gt; * 优化： * 使用map缓存 key-\u0026gt;index，降低查找元素位置的复杂度为 O(1) * * @param key 查询key * @return Entry */ public Entry\u0026lt;K, V\u0026gt; get(K key) { if (head == tail) { return null; } Integer idx = k2Idx.get(key); if (null == idx || idx \u0026lt; head || idx \u0026gt;= tail) { return null; } Entry\u0026lt;K, V\u0026gt; target = elements[idx]; // 搬移 i到0队头的位置 moveArrFromEndInclusive(idx, target); return target; // for (int i = head; i \u0026lt; tail; i++) { // Entry\u0026lt;K, V\u0026gt; target = elements[i]; // if (target.key.equals(key)) { // // 搬移 i到0队头的位置 // moveArrFromEndInclusive(i, target); // return target; // } // } // return null; } /** * 从 i往1 的闭区间右移元素 */ private void moveArrFromEndInclusive(int i, Entry\u0026lt;K, V\u0026gt; target) { for (int j = i; j \u0026gt; 0; j--) { elements[j] = elements[j - 1]; k2Idx.put(elements[j].key, j); } elements[head] = target; k2Idx.put(target.key, head); } /** * 复杂度： * 每次均需遍历数组、搬移数据，时间复杂度 O(n) */ public void put(K key, V val) { int targetIdx = findIdxByKey(key); if (targetIdx \u0026gt;= head \u0026amp;\u0026amp; targetIdx \u0026lt; tail) { // 元素已存在数组中 搬移 Entry\u0026lt;K, V\u0026gt; targetEntry = elements[targetIdx]; targetEntry.val = val; moveArrFromEndInclusive(targetIdx, targetEntry); } else { if (tail \u0026gt; capacity - 1) { // 淘汰队尾 elements[tail - 1] = null; --tail; } ++tail; moveArrFromEndInclusive(tail - 1, new Entry\u0026lt;\u0026gt;(key, val)); } } private int findIdxByKey(K key) { // 优化为从map中获取位置 Integer idx = k2Idx.get(key); return idx == null ? -1 : idx; // for (int i = 0; i \u0026lt; tail; i++) { // if (elements[i].key.equals(key)) { // return i; // } // } // return -1; } public void print() { System.out.println(this); Arrays.stream(elements).filter(Objects::nonNull).forEach(element -\u0026gt; { System.out.print(element.key + \u0026#34;-\u0026gt;\u0026#34; + element.val + \u0026#34;》》\u0026#34;); }); System.out.println(); } @Override public String toString() { return \u0026#34;ArrayLRU{\u0026#34; + \u0026#34;head=\u0026#34; + head + \u0026#34;, tail=\u0026#34; + tail + \u0026#34;, capacity=\u0026#34; + capacity + \u0026#39;}\u0026#39;; } static class Entry\u0026lt;K, V\u0026gt; { K key; V val; public Entry(K key, V val) { this.key = key; this.val = val; } } } 优点 加入了k2Idxmap可以加速查找对应元素位置。\n缺点 搬移数据依然有较高开销，整体复杂度还是O(n)。\n基于双向链表 观察我们上述关键操作：\n写入新元素时，基于单向链表可以以O(1)的复杂度原地完成，数组则需要数据搬移； 元素数据量达到容量时，队尾数据需要删除，所以最好可以O(1)完成； 随机元素访问时，我们要以O(1)复杂度找到，这一步已经在基于数组的版本中通过加入map做到了快速定位，当然，有一点内存开销； 因此，除了特性，到这一步我们梳理出了优化点，根据这些优化点，我们基于双向链表来实现。\n双向链表写入新元素同样是O(1)复杂度； 可以维护头尾指针，方便头尾操作； 同样维护map来定位随机node； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 public class DoubleLinkedListLRU\u0026lt;K, V\u0026gt; { /** * first head 指向第一个真实节点的prev */ DoubleLinkedListNode\u0026lt;K, V\u0026gt; head; /** * last tail 指向最后真实节点的next */ DoubleLinkedListNode\u0026lt;K, V\u0026gt; tail; /** * 当前数据量 */ int size; /** * lru容量 */ int capacity; /** * 通过key定位链表中的双向链表节点 */ Map\u0026lt;K, DoubleLinkedListNode\u0026lt;K, V\u0026gt;\u0026gt; k2Node; public DoubleLinkedListLRU(int capacity) { this.size = 0; this.capacity = capacity; k2Node = new HashMap\u0026lt;\u0026gt;(MapUtils.capacity(capacity)); this.head = new DoubleLinkedListNode\u0026lt;\u0026gt;(); this.tail = new DoubleLinkedListNode\u0026lt;\u0026gt;(); head.next = tail; tail.prev = head; } public V get(K key) { // O(1) DoubleLinkedListNode\u0026lt;K, V\u0026gt; node = k2Node.get(key); if (node == null) { return null; } // O(1) // 删除原位置元素 removeNode(node); insertNewHead(node); return node.entry.val; } public void put(K key, V val) { // O(1) DoubleLinkedListNode\u0026lt;K, V\u0026gt; targetNode = k2Node.get(key); if (targetNode == null) { // 数据原来不存在 if (size == capacity) { // 淘汰末尾元素 evictTail(); } DoubleLinkedListNode\u0026lt;K, V\u0026gt; newNode = new DoubleLinkedListNode\u0026lt;\u0026gt;(key, val); insertNewHead(newNode); ++size; } else { // 数据原来已存在，直接更换节点位置 removeNode(targetNode); insertNewHead(targetNode); } } // O(1) private void evictTail() { DoubleLinkedListNode\u0026lt;K, V\u0026gt; last = tail.prev; tail.prev = last.prev; last.prev.next = tail; last.next = null; last.prev = null; k2Node.remove(last.entry.key); --size; } // O(1) private void insertNewHead(DoubleLinkedListNode\u0026lt;K, V\u0026gt; targetNode) { if (head.next == tail) { tail.prev = targetNode; } targetNode.next = head.next; targetNode.prev = head; head.next.prev = targetNode; head.next = targetNode; k2Node.put(targetNode.entry.key, targetNode); } // O(1) private void removeNode(DoubleLinkedListNode\u0026lt;K, V\u0026gt; targetNode) { targetNode.prev.next = targetNode.next; targetNode.next.prev = targetNode.prev; k2Node.remove(targetNode.entry.key); } public DoubleLinkedListNode\u0026lt;K, V\u0026gt; randomNode() { int i = NumberUtils.randomInt(size); System.out.println(\u0026#34;randomI:\u0026#34; + i); DoubleLinkedListNode\u0026lt;K, V\u0026gt; curr = head; while (i \u0026gt; 0) { curr = curr.next; --i; } return curr; } public void print() { System.out.println(this); DoubleLinkedListNode\u0026lt;K, V\u0026gt; curr = head; while (curr != null) { if (curr.entry != null) { System.out.print(curr.entry.val + \u0026#34;--\u0026gt;\u0026#34;); } curr = curr.next; } System.out.println(); } @Override public String toString() { return \u0026#34;DoubleLinkedListLRU{\u0026#34; + \u0026#34;, size=\u0026#34; + size + \u0026#34;, capacity=\u0026#34; + capacity + \u0026#39;}\u0026#39;; } static class DoubleLinkedListNode\u0026lt;K, V\u0026gt; { DoubleLinkedListNode\u0026lt;K, V\u0026gt; prev; DoubleLinkedListNode\u0026lt;K, V\u0026gt; next; Entry\u0026lt;K, V\u0026gt; entry; public DoubleLinkedListNode(K key, V val) { this.entry = new Entry\u0026lt;\u0026gt;(key, val); } public DoubleLinkedListNode() { } @Override public String toString() { return \u0026#34;DoubleLinkedListNode{\u0026#34; + \u0026#34;prev=\u0026#34; + (prev == null ? \u0026#34;null\u0026#34; : prev.entry) + \u0026#34;, next=\u0026#34; + (next == null ? \u0026#34;null\u0026#34; : next.entry) + \u0026#34;, entry=\u0026#34; + entry + \u0026#39;}\u0026#39;; } } static class Entry\u0026lt;K, V\u0026gt; { K key; V val; public Entry(K key, V val) { this.key = key; this.val = val; } @Override public String toString() { return \u0026#34;Entry{\u0026#34; + \u0026#34;key=\u0026#34; + key + \u0026#34;, val=\u0026#34; + val + \u0026#39;}\u0026#39;; } } } 可以看到在我们的关键操作上，基于双向链表的版本都做到了O(1)。\n总结 以上，实现了不同版本的LRU，根据前面的版本缺陷分析优化得出了基于双向链表的版本。\n想要实现一个bug free并且工业级的LRU，还有更多细节需要处理。\nRef https://en.wikipedia.org/wiki/Cache_replacement_policies ","permalink":"https://redolog.github.io/posts/rd/algo/cache/replacement-policy/lru/","summary":"\u003cp\u003e从零实现一个\u003ccode\u003eLRU\u003c/code\u003e，同时优化我们的版本。\u003c/p\u003e","title":"LRU基本实现及优化"},{"content":"从零实现一个最简单的特里树，同时了解特里树的使用场景。\n概念 wiki:\nIn computer science, a trie, also called digital tree or prefix tree,[1] is a type of k-ary search tree, a tree data structure used for locating specific keys from within a set. These keys are most often strings, with links between nodes defined not by the entire key, but by individual characters. In order to access a key (to recover its value, change it, or remove it), the trie is traversed depth-first, following the links between nodes, which represent each character in the key.\nTrie 树，也叫“字典树”。顾名思义，它是一个树形结构（n叉）。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。\nTrie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。\n提供的功能包括：\n插入新的单词 insert 查找单词是否匹配 find 限定只处理a-z26个小写字母组成的英文单词场景。\n实现 TrieTree 结构 我们的节点 TrieNode26Char 表示特里树节点：\nval表示当前节点值； TrieNode26Char[] children 则存储下层节点； 可以看到这种实现下如果某层节点不够满、前缀复用率不高，数组空置会比较严重，会浪费空间，这个是优化点； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class TrieNode26Char { /** * 当前节点值 */ char val; /** * 子节点 */ TrieNode26Char[] children; /** * 是否为一个单词末位 */ boolean isEndingChar; public TrieNode26Char(char val) { this.val = val; this.children = new TrieNode26Char[26]; } } 关键操作 插入单词与匹配单词串逻辑类似，均是沿着树往下层节点找，核心思想是dfs。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 public class TrieTree { /** * 根节点不存储val */ private final TrieNode26Char root = new TrieNode26Char(\u0026#39;-\u0026#39;); public void insert(String word) { insert(word.toCharArray()); } public void insert(char[] word) { TrieNode26Char curr = root; for (int i = 0; i \u0026lt; word.length; i++) { int iIdx = word[i] - \u0026#39;a\u0026#39;; if (curr.children[iIdx] == null) { curr.children[iIdx] = new TrieNode26Char(word[i]); } curr = curr.children[iIdx]; } curr.isEndingChar = true; } public boolean find(String word) { return find(word.toCharArray()); } public boolean find(char[] word) { TrieNode26Char curr = root; for (char c : word) { int idx = c - \u0026#39;a\u0026#39;; if (curr.children[idx] == null) { return false; } curr = curr.children[idx]; } return curr.isEndingChar; } public void print() { Deque\u0026lt;TrieNode26Char\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.offerLast(root); int level = 0; while (!queue.isEmpty()) { ++level; int size = queue.size(); System.out.print(\u0026#34;第\u0026#34; + level + \u0026#34;层：\u0026#34;); for (int i = 0; i \u0026lt; size; i++) { TrieNode26Char node = queue.pollFirst(); if (node == null) { continue; } System.out.print(node.val + \u0026#34; \u0026#34;); if (node.children != null) { for (TrieNode26Char child : node.children) { queue.offerLast(child); } } } System.out.println(); } } } 复杂度分析 时间 查找、匹配单词复杂度只跟word长度有关，查一次最多遍历len(word)个节点。因此时间复杂度O(len(word))。\n从根创建树的复杂度则取决于所有单词的长度，为O(n)。\n空间 按我们目前的最简实现，数组空间复杂度会比较高，特别是某层子节点只存了一个字符时，数组也需要开辟26的长度。\n使用场景 pros cons 1. 字符串中包含的字符集不能太大。 适用于查找前缀匹配的字符串（非精确搜索），如搜索提示。 2. 要求字符串的前缀重合比较多，不然空间消耗会变大很多。 3. 跳转结构，对CPU缓存不友好。 总结 以上，实现了最简单的特里树，并且分析了复杂度，对比了场景优劣。\nRef https://github.com/redolog/algorithm-java/commit/bffe93a166003014813a45bc3d7e20566ae72bad https://en.wikipedia.org/wiki/Trie ","permalink":"https://redolog.github.io/posts/rd/algo/data-structure/trie-tree/","summary":"\u003cp\u003e从零实现一个最简单的特里树，同时了解特里树的使用场景。\u003c/p\u003e","title":"特里树最简实现及使用场景"},{"content":"LeetCode 153. 寻找旋转排序数组中的最小值 二分题解，重新认识二分。\n题目 153. 寻找旋转排序数组中的最小值 https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array/\n已知一个长度为 n 的数组，预先按照升序排列，经由 1 到 n 次 旋转 后，得到输入数组。例如，原数组 nums = [0,1,2,4,5,6,7] 在变化后可能得到： 若旋转 4 次，则可以得到 [4,5,6,7,0,1,2] 若旋转 7 次，则可以得到 [0,1,2,4,5,6,7] 注意，数组 [a[0], a[1], a[2], \u0026hellip;, a[n-1]] 旋转一次 的结果为数组 [a[n-1], a[0], a[1], a[2], \u0026hellip;, a[n-2]] 。\n给你一个元素值 互不相同 的数组 nums ，它原来是一个升序排列的数组，并按上述情形进行了多次旋转。请你找出并返回数组中的 最小元素 。\n你必须设计一个时间复杂度为 O(log n) 的算法解决此问题。\n思路 这道题与 搜索二维矩阵 II 有一个共同点，那就是数据是保证了部分排序的，结合题干要求复杂度O(log n)，我们可以尝试二分的思路。\n二分的核心：每一轮遍历都可以丢弃一些不符合条件的数据，以此提高后续搜索的效率。\n朴素二分：针对全局有序的数组，一次丢弃正好一半范围的数据； 树形BST二分：针对排列为树形的数组（二维数组、矩阵），一次可以丢弃一行或者一列的数据； 旋转二分：针对本题这种大小区间不定的情形，一次可以丢弃一半范围的数据； 二分变体解 思路 复杂度分析 时间 O(logn)时间复杂度。\n空间 无额外空间占用，空间复杂度O(1)。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 /** * 执行用时： * 0 ms * , 在所有 Java 提交中击败了 * 100.00% * 的用户 * 内存消耗： * 40.9 MB * , 在所有 Java 提交中击败了 * 61.54% * 的用户 * 通过测试用例： * 150 / 150 */ public int findMin(int[] nums) { int n = nums.length; int l = 0, r = n - 1; // r不管怎么变，都小于l值 while (l \u0026lt; r) { int mid = l + ((r - l) \u0026gt;\u0026gt; 1); if ((nums[l] \u0026lt; nums[mid] \u0026amp;\u0026amp; nums[mid] \u0026lt; nums[r]) || (l == mid \u0026amp;\u0026amp; nums[l] \u0026lt; nums[r])) { break; } if (nums[r] \u0026gt;= nums[mid]) { r = mid; } else if (nums[mid] \u0026gt;= nums[l]) { l = mid + 1; } } return nums[l]; } 实际上这里代码可以适当简化，只比较右区间值nums[r]与nums[mid]即可判断出上图中的三种情况：\nnums[r]\u0026lt;nums[mid]：说明mid此时在旋转过的左区间，l向右，l=mid+1； nums[r]\u0026gt;nums[mid]：说明mid在右区间，r向左，r=mid； 只使用nums[l]与nums[mid]比较，上图中的nums[l]\u0026lt;nums[mid]时无法决定挪动左右哪个区间。\n简化代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public int findMin(int[] nums) { int n = nums.length; int l = 0, r = n - 1; while (l \u0026lt; r) { int mid = l + ((r - l) \u0026gt;\u0026gt; 1); if (nums[r] \u0026gt; nums[mid]) { r = mid; } else { l = mid + 1; } } return nums[l]; } 154. 寻找旋转排序数组中的最小值 II https://leetcode.cn/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof/ https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array-ii/ 已知一个长度为 n 的数组，预先按照升序排列，经由 1 到 n 次 旋转 后，得到输入数组。例如，原数组 nums = [0,1,4,4,5,6,7] 在变化后可能得到： 若旋转 4 次，则可以得到 [4,5,6,7,0,1,4] 若旋转 7 次，则可以得到 [0,1,4,4,5,6,7] 注意，数组 [a[0], a[1], a[2], \u0026hellip;, a[n-1]] 旋转一次 的结果为数组 [a[n-1], a[0], a[1], a[2], \u0026hellip;, a[n-2]] 。\n给你一个可能存在 重复 元素值的数组 nums ，它原来是一个升序排列的数组，并按上述情形进行了多次旋转。请你找出并返回数组中\u0026gt; 的 最小元素 。\n二分解 关键需要画图分析用例情况。\n思路 154相比153只改变了一个条件：元素可重复。对应循环中的判断新增了一种等值的情况：nums[r] == nums[mid]。\n用例分布分析：\n无等值元素的情况下，153解法代码即可满足。 有等值元素的情况下 全部等值，不管怎么挪动区间都可以求解 如下图 我们再看个用例：[1011111]\n上述两图的情况下，均满足：nums[r] == nums[mid]，这种情况下如果收缩l，上图的第一种情况就会出错，如果收缩r，我们无法直接跳跃到mid的位置，只能逐步收缩，即r=r-1。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public int minArray(int[] nums) { int n = nums.length; int l = 0, r = n - 1; while (l \u0026lt; r) { int mid = l + ((r - l) \u0026gt;\u0026gt; 1); if (nums[r] \u0026gt; nums[mid]) { r = mid; } else if (nums[r] \u0026lt; nums[mid]) { l = mid + 1; } else { r = r - 1; } } return nums[l]; } Ref 153官解 154 K神题解 leetCode34 二分法经典应用-在排序数组中查找元素的第一个和最后一个位置 ","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/find-minimum-in-rotated-sorted-array/","summary":"\u003cp\u003eLeetCode 153. 寻找旋转排序数组中的最小值 二分题解，重新认识二分。\u003c/p\u003e","title":"LeetCode 153. 寻找旋转排序数组中的最小值 二分题解"},{"content":"从零实现一个最小堆，同时了解堆结构的使用场景。\n概念 wiki:\nIn computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C.[2] The node at the \u0026ldquo;top\u0026rdquo; of the heap (with no parents) is called the root node.\n数据结构中的堆与系统概念中的堆不是一个概念，数据结构的堆是一种逻辑结构，编程语言中的堆往往指一块内存区域、池。\n本文以最小堆为例，最小堆是一种基于完全二叉树的数据结构，也叫优先级队列 PriorityQueue，满足以下性质：\n一颗完全二叉树，除了最后一层其它层都满排列，最后一层靠左排列； 父节点小于等于任一子节点（大顶堆则相反）； 由于完全二叉树向左排列的特性，使用数组存放不浪费空间同时检索高效。\n提供的功能包括：\n插入新的数据 add 堆化 heapify siftUp 从最小的子节点开始与父节点验证堆性质、合理化 siftDown 从最小的父节点开始与子节点验证堆性质、合理化 移除最值 removeMin 查看最值 peekMin 实现 KthLargestElementInAnArray MaxHeap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 public class MinHeap { // 从下标1的位置存完全二叉树堆元素 int[] arr; int capacity; int size; public MinHeap() { } public MinHeap(int capacity) { this.capacity = capacity; arr = new int[capacity + 1]; size = 0; } /** * 添加元素 */ public void add(int ele) { if (size == capacity) { throw new IllegalStateException(\u0026#34;最小堆已满\u0026#34;); } ++size; arr[size] = ele; siftUp(); } /** * 查看最小值（堆顶）元素 */ public int peekMin() { return arr[1]; } /** * 移除最小元素 */ public void removeMin() { arr[1] = arr[size]; --size; siftDown(); } private void siftDown() { // 从顶端开始遍历树，检查堆性质 int parentIdx = 1; int leftChildIdx = getLeftChildIdx(parentIdx); int rightChildIdx = getRightChildIdx(parentIdx); while (leftChildIdx \u0026lt;= size \u0026amp;\u0026amp; rightChildIdx \u0026lt;= size) { if (arr[parentIdx] \u0026lt;= arr[leftChildIdx] \u0026amp;\u0026amp; arr[parentIdx] \u0026lt;= arr[rightChildIdx]) { break; } int smallerIdx = arr[rightChildIdx] \u0026lt;= arr[leftChildIdx] ? rightChildIdx : leftChildIdx; swap(arr, smallerIdx, parentIdx); parentIdx = smallerIdx; leftChildIdx = getLeftChildIdx(parentIdx); rightChildIdx = getRightChildIdx(parentIdx); } } private void siftUp() { // 从末端往上检查堆性质 int childIdx = size; int parentIdx = getParentIdx(childIdx); while (parentIdx \u0026gt;= 1 \u0026amp;\u0026amp; arr[parentIdx] \u0026gt; arr[childIdx]) { swap(arr, parentIdx, childIdx); childIdx = parentIdx; parentIdx = getParentIdx(childIdx); } } public int getParentIdx(int childIdx) { return childIdx / 2; } public int getLeftChildIdx(int parentIdx) { return 2 * parentIdx; } public int getRightChildIdx(int parentIdx) { return 2 * parentIdx + 1; } } 关键操作 堆化 堆化是堆结构最核心的方法，本质是逐个节点检验完全二叉树、堆的性质。\nsift为筛选的意思\nsiftUp是从最下的子节点开始逐个节点向上遍历，当前子节点向上与其父节点比较。 可以从上面的代码看到起点是：\n1 2 int childIdx = size; int parentIdx = getParentIdx(childIdx); 而siftDown则是从最下的父节点开始逐个节点向上遍历，当前父节点向下与其子节点比较。 遍历起点是：\n1 2 3 4 // 从顶端开始遍历树，检查堆性质 int parentIdx = 1; int leftChildIdx = getLeftChildIdx(parentIdx); int rightChildIdx = getRightChildIdx(parentIdx); 建堆 可以考虑两种建堆方式：\n逐个向堆中添加元素； 参考 KthLargestElementInAnArray 中的用例。 遍历父节点，持续向下siftDown建堆； 参考 HeapSort 中的用例。 复杂度分析 空间 建堆、堆化，是原地操作，无需额外空间，空间复杂度O(1)。\n时间 建堆 建堆复杂度取决于树高*节点个数，n个数据树高为 log2n，公式推导: 复杂度= 求和(当前层节点个数 * 当前层高) 设：顶节点高为h，中间层节点数为 2^k，对应中间层高为 h-k，倒数第二层高：1，对应节点数为 2^(h-1) 对复杂度求和： S1 = 2^0 * h + 2^1 * (h-1) + 2^2 * (h-2) + ... + 2^k * (h-k) + ... +2^(h-1) * 1 S2 = 2* (2^0 * h + 2^1 * (h-1) + 2^2 * (h-2) + ... + 2^k * (h-k) + ... +2^(h-1) * 1) = 2^1 * h + 2^2 * (h-1) + 2^3 * (h-2) + ... + 2^(k+1) * (h-k) + ... +2^h * 1 S2为复杂度*2，S2-S1=S= -h + 2^1 + 2^2 + ... + 2^k + ... + 2^(h-1) + 2^h 代入等比数列去和公式（q公比，S为和，n表示数据量，Sn=(An*q-A1)/(q-1)） 求得 S=-h + (2^h * 2 - 2)/(2 - 1)=-h+2^h 其中 h 为 log2n，所以 S= n -h 因此建堆复杂度为 O(n)，其中h为常数阶省略 堆化 一轮堆化操作复杂度只取决于树高，时间复杂度O(logn)。\n性能短板 对数据局部性访问不够友好 堆化父子节点遍历时，会跳跃访问数组元素，以排序为例，快排则是连续访问，因此堆化对CPU缓存行不够友好。\n堆排序不稳定 堆化操作会打乱数据原有顺序，还是以排序为例，如果是排序有序度比较高的数据，堆排序先建堆，这一步已经打乱了顺序，后续排序的操作徒增了数组元素交换的次数，而快排、插入排序等算法在这种情况下则不会交换有序元素。\n应用场景 排序 用例： https://github.com/redolog/algorithm-java/blob/main/src/test/java/com/algorithm/sort/HeapSortTest.java\n堆排序时间复杂度结合上述对堆化复杂度的分析为O(n)，而排序后续元素操作是取出最值元素，将lastChild交换到root的位置继续堆化，持续堆化n-1轮，因此排序复杂度为O(n+n*logn)，去除低阶，得出O(n*logn)。\n堆排序在一众排序算法中名列前茅，当然，实际使用需结合上述性能短板的结论。\nTopK 问题求解 用例： https://github.com/redolog/algorithm-java/blob/main/src/test/java/com/algorithm/dataStructure/array/KthLargestElementInAnArrayTest.java\n使用，假设我们的数据从小到大排列：\n最大堆可以维持前k项小数据，堆顶则是这堆小数据中最大的 最小堆可以维持后k项大数据，堆顶则是这堆大数据中最小的 新数据访问时，通过堆顶判断边界，最终堆中维护了整个数据集中的区间最值。\nTopK 变体：分位数 如果我们同时持有一个最大堆、一个最小堆，调整两个容器容量比例则可以维持分位值。\n比如中位数的比例则是一半一半，元素总数为奇数时，从持有元素个数多一的堆中取堆顶则是中位数。\n如果有 n 个数据，n 是偶数，我们从小到大排序，那前 2/n​ 个数据存储在大顶堆中，后 2/n​ 个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果 n 是奇数，情况是类似的，大顶堆就存储 2/n​+1 个数据，小顶堆中就存储 2/n​ 个数据。\n实例：\nLeetCode 295. 数据流的中位数 堆题解 优先级队列 堆本身就是优先级队列。使用这种有序集合我们可以很方便地做一些对顺序有要求的操作：\n合并有序文件 假设有十个小的有序文件，如何将里面的数据合并成一个有序文件呢？\n我们可以使用堆作为中间容器，每次从十个文件中取最小值放入最小堆，堆中移除堆顶，复杂度仅有O(1)，而如果用数组存放，每次获取要么O(n)，要么每次加入新元素都需要排序，而维持最小堆的有序只需要O(logn)的复杂度。\n权重定时器 有序容器中，元素按权重排序。\n在定时器场景下，我们的循环每次从队列中取出待执行的定时任务进行对应时段的休眠，一种比较笨的实现思路是每次休眠一秒，拿出任务检查是否到达执行时间，而有了权重后，我们入队时就按照执行时间间隔排好序，出队一个任务按照对应权重休眠对应时间，这样就没有多余损耗。\n总结 以上，实现了最基础的一个最小堆最大堆结构，并且分析了复杂度，列举了主流的一些应用场景。\nRef https://en.wikipedia.org/wiki/Heap_(data_structure) ","permalink":"https://redolog.github.io/posts/rd/algo/data-structure/heap/","summary":"\u003cp\u003e从零实现一个最小堆，同时了解堆结构的使用场景。\u003c/p\u003e","title":"堆结构实现及使用场景"},{"content":"从零实现并查集，并逐步优化各版本中的问题。\n概念 wiki:\nIn computer science, a disjoint-set data structure, also called a union–find data structure or merge–find set, is a data structure that stores a collection of disjoint (non-overlapping) sets. Equivalently, it stores a partition of a set into disjoint subsets. It provides operations for adding new sets, merging sets (replacing them by their union), and finding a representative member of a set. The last operation makes it possible to find out efficiently if any two elements are in the same or different sets.\n并查集是一种实现了数学中集合概念的数据结构。内部元素彼此不重复，因此我们可以很方便的使用数组来实现此结构（下标不重复）。\n提供的功能包括：\n添加新的集合 init 合并多个集合 union 查找集合中的标识，也就是我们下述实现的树根 find 基于数组实现的 QuickFindUF 根据概念，我们可以基于数组实现一个并查集：\n使用数组存储元素，下标标识元素，表示并查集的树； 元素值（数组值）相同，视为元素是连接的； union合并元素操作：将元素下标指向的值变成相同的； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 public class QuickFindUF { /** * 数据集容量 */ int capacity; /** * 使用数组存储元素， */ int[] elements; public QuickFindUF(int n) { this.capacity = n; this.elements = new int[n]; for (int i = 0; i \u0026lt; elements.length; i++) { elements[i] = i; } } /** * 复杂度 O(n) */ void union(int p, int q) { if (isConnected(p, q)) { return; } // 将p点以及连接的点，都指向 elements[q] for (int i = 0; i \u0026lt; elements.length; i++) { if (elements[i] == find(p)) { elements[i] = find(q); } } } int find(int i) { return elements[i]; } boolean isConnected(int p, int q) { return find(p) == find(q); } } 可以看到union操作复杂度为O(n)，我们需要优化。\n以树根为中心的 QuickUF 我们同样基于数组表示这棵并查集树，将数组指向尽可能组织起来； 初始时每个元素指向自己，表示为一个根； 查找指向时，沿着树一直向上查找根节点； union合并时，将新节点指向原节点树根，由于union多个节点都指向树根，因此理论上树高远小于总数据量，因此效率提高； 修改点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 int[] parentArr; /** * 寻找i元素对应树根 * * @param i 元素下标 * @return 指向的树根 */ int find(int i) { // i保证不越界 // 只要i在树上没到达树根，就一直往上走 while (parentArr[i] != i) { i = parentArr[i]; } return i; } /** * 将q节点指向的树根指向p的树根 * * @param p 前序节点 * @param q 后继节点 */ void union(int p, int q) { int pRoot = find(p); int qRoot = find(q); if (pRoot == qRoot) { return; } parentArr[qRoot] = pRoot; } 实测下本版本效果并不太好。\nQuickUF 中我们每次union时很可能会将一棵较高的树，并到一棵较低的树上，这样其实总的树高是变高了，复杂度因此而攀升，这也是实测下运行慢的主要原因。\n判断子集元素数 SizeUF 本轮优化中 SizeUF 我们每次union都可以判断下集合元素数，每次将元素较少的树并到较多的树上，这样总的树高可以得到控制。\n我们新增一个数组sizeArr来保存集合元素数，O(1)即能获取到树对应元素数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 public class SizeUF { /** * 并查集总数据量 */ int n; /** * 记录每个元素（实际是下标）对应父元素，根指向自己 */ int[] parentArr; /** * 记录每个元素（实际是下标）对应并集元素数量，供union时判断 */ int[] sizeArr; public SizeUF(int n) { this.n = n; this.parentArr = new int[n]; this.sizeArr = new int[n]; for (int i = 0; i \u0026lt; parentArr.length; i++) { parentArr[i] = i; sizeArr[i] = 1; } } int find(int i) { // 需保证i不越界 while (parentArr[i] != i) { i = parentArr[i]; } return i; } boolean isConnected(int p, int q) { if (p == q) { return true; } int pRoot = find(p); int qRoot = find(q); return pRoot == qRoot; } void union(int p, int q) { int pRoot = find(p); int qRoot = find(q); if (pRoot == qRoot) { return; } if (sizeArr[pRoot] \u0026lt; sizeArr[qRoot]) { _union(qRoot, pRoot); } else { _union(pRoot, qRoot); } } /** * 将元素更少的树并到元素更多的树根上 * * @param more 多元素树根 * @param less 少元素树根 */ void _union(int more, int less) { parentArr[less] = parentArr[more]; // 更少元素的树并到更多元素的树上，此时更少元素的根统一由 more 下标根负责，叠加less子树上的元素数 sizeArr[more] += sizeArr[less]; } } SizeUF 中根据树元素个数做了union时的优化，防止将更多元素的树合并到更少元素的树上，但是带来一个问题： 根据元素个数判断树高是不准确的。\n能否继续优化？\n判断集合树高 RankUF find查找元素时，决定复杂度的是树高，因此 RankUF 中，我们优化 SizeUF size的逻辑。\nsizeArr更名为：hArr。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 public class RankUF { /** * 并查集元素总量 */ int n; /** * 元素指向父的指针 */ int[] parentArr; /** * 当前节点（下标）对应集合树高，树高决定了find查找的复杂度，使用树高优化 SizeUF 带来的问题 */ int[] hArr; public RankUF(int n) { this.n = n; this.parentArr = new int[n]; this.hArr = new int[n]; for (int i = 0; i \u0026lt; parentArr.length; i++) { parentArr[i] = i; hArr[i] = 0; } } /** * 查找i下标所在集合的根 */ int find(int i) { // 保证i数组下标范围 while (parentArr[i] != i) { i = parentArr[i]; } return i; } /** * @param p p点下标 * @param q q点下标 * @return p所在树根==q所在树根时，表示p、q对应元素在同一集合内！ */ boolean isConnected(int p, int q) { if (p == q) { return true; } return find(p) == find(q); } void union(int p, int q) { int pRoot = find(p); int qRoot = find(q); if (pRoot == qRoot) { return; } if (hArr[pRoot] \u0026gt; hArr[qRoot]) { _union(pRoot, qRoot); } else if (hArr[pRoot] \u0026lt; hArr[qRoot]) { _union(qRoot, pRoot); } else { _union(pRoot, qRoot); hArr[pRoot] += 1; } } /** * 将更矮的树合并到更高的树上 * * @param higherRoot 更高树根 * @param lowerRoot 更矮树根 */ private void _union(int higherRoot, int lowerRoot) { parentArr[lowerRoot] = higherRoot; } } RankUF 中根据树高来精确判断union时的策略。但是在极端情况下，树仍然会很高，树有可能变矮吗？\n当然可以！\n路径压缩 CompressRankUF 我们可以使用路径压缩的方式将树高压短。\nCompressRankUF 通过在查询数据时将树高一步步压缩（子节点的父直接指向根节点），以此提高了后续数据查询、合并时的效率。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 public class CompressRankUF { /** * 数据规模 */ int n; /** * 各元素父节点指针 */ int[] parentArr; /** * 树高 */ int[] rankArr; public CompressRankUF(int n) { this.n = n; this.parentArr = new int[n]; this.rankArr = new int[n]; for (int i = 0; i \u0026lt; parentArr.length; i++) { parentArr[i] = i; rankArr[i] = 0; } } int findRecurse(int i) { if (i == parentArr[i]) { return i; } parentArr[i] = findRecurse(parentArr[i]); return parentArr[i]; } int findTraverse(int i) { while (parentArr[i] != i) { // 当前节点的父直接指向父的父 parentArr[i] = parentArr[parentArr[i]]; i = parentArr[i]; } return i; } boolean isConnected(int p, int q) { if (p == q) { return true; } return findRecurse(p) == findRecurse(q); } void union(int p, int q) { int pRoot = findRecurse(p); int qRoot = findRecurse(q); if (pRoot == qRoot) { return; } if (rankArr[pRoot] \u0026gt; rankArr[qRoot]) { parentArr[qRoot] = pRoot; } else if (rankArr[pRoot] \u0026lt; rankArr[qRoot]) { parentArr[pRoot] = qRoot; } else { parentArr[pRoot] = qRoot; // 树高+1 rankArr[qRoot] += 1; } } } 看看我们100000数据量下的测试效果：\n1 2 3 4 5 QuickFindUFTest 运行 100000次，耗时：3468ms QuickUFTest 运行 100000次，耗时：12501ms SizeUFTest 运行 100000次，耗时：25ms RankUFTest 运行 100000次，耗时：24ms CompressRankUFTest 运行 100000次，耗时：21ms 小结 基于数组我们实现了基本的并查集，并逐步优化了每个版本中的缺陷：\n版本 缺陷 优化 QuickFindUF union操作复杂度O(n) QuickUF union可能将较高的树合并到较低的树上，带来边缘情况下的性能退化 以树根组织数据，理论上提高了union的时间效率 SizeUF 通过元素数判断树高是不准确的，依然有QuickUF中的问题 判断树中元素数来决定合并时的策略，一定程度上防止了矮树合并到高树上 RankUF 树高未进行优化 通过树高判断合并策略，比Size的策略更准确 CompressRankUF 无 查找时压缩树高 一方面我们经过了理论上的分析、优化，一方面也需要做好工程使用下的压测，部分时间复杂度分析在极端情况下并不完全准确反映性能表现。\n以上即我今天对并查集的实现、优化过程。\nRef 本文使用的代码commit ","permalink":"https://redolog.github.io/posts/rd/algo/data-structure/union-find/","summary":"\u003cp\u003e从零实现并查集，并逐步优化各版本中的问题。\u003c/p\u003e","title":"并查集常见实现及优化"},{"content":"LeetCode 46. 全排列 DFS题解，理解DFS、回溯、递归树。\n题目 46. 全排列 https://leetcode.cn/problems/permutations/\n给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。\n全排列这个题目非常适合用来理解回溯、DFS、递归。\n官方回溯解 了解下回溯法的定义：https://suanfa8.com/algorithm-idea/backtracking/01-intro/\n回溯完全符合我们自己去思考全排列的过程，比如我们有[1,2,3]：\n第一步，我们选一个数作为排列中的首项，比如1； 第二步，我们选一个数作为第二项，比如2； 第三步，我们选一个数作为第三项，此时只剩下3，得到目前的排列[1,2,3]； 第四步，我们退回第三步，发现选了[1,2]之后没有别的选择，继续回退； 第五步，退回到第二步，我们除了2还可以选择3作为第二项，此时2作为第三项，得到新的排列[1,3,2]； 继续回退到第一步，我们选2作为首项，继续上述过程； 上述过程我们可以用一棵递归树表示，借用官解的图：\n我们简单理解下回溯的框架：\n题解过程是一棵递归树的执行过程； 用DFS的思路思考，用递归编码； 在递归树上遍历路径： 寻找解、排除非解（剪枝、failfast）； 当前解得出后，树节点回退（撤销状态、状态重置）； 状态：遍历过程前进后退的进度（对应状态变量、容器）； 因此我们的代码有大致框架模板：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 void dfs(int[] nums, List\u0026lt;Integer\u0026gt; paths, boolean[] usedArr, List\u0026lt;Integer\u0026gt; ansList) { if (paths.size() == n) { // 递归对应边界 ansList.add(new ArrayList\u0026lt;\u0026gt;(paths)); return; } for (int i = 0; i \u0026lt; nums.length; i++) { if (usedArr[i]) { // 判断当前路径下节点遍历状态 continue; } usedArr[i] = true; paths.add(nums[i]); dfs(nums, paths, usedArr, ansList); // 回溯：回退之前的状态 usedArr[i] = false; paths.remove(); } } 回到全排列这道题目，根据以上我们的分析，我们需明确以下几点：\n使用DFS递归的形式编码，对应我们的dfs()方法； DFS遍历，寻找每一种可能的排列，完成一组排列，在递归树上回退状态； 状态： 递归到了哪里？第几层？ 数组节点值在本轮遍历中是否已被占用？ 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permute(int[] nums) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ansList = new ArrayList\u0026lt;\u0026gt;(); if (nums == null || nums.length == 0) { return ansList; } int n = nums.length; // 排列问题，是典型的树形遍历问题，我们使用深度有限遍历的方式走访每个可能的分支 // 使用一个 path 容器，存放当前分支下的数据，回溯即回撤时从末尾回退数据，使用一个栈结构保存 Deque\u0026lt;Integer\u0026gt; pathQueue = new LinkedList\u0026lt;\u0026gt;(); // 同时，我们需要记录每个位置元素使用的状态，为boolean类型数组，同时需要回溯时回退状态 boolean[] usedArr = new boolean[n]; // 深度优先遍历，递归函数 dfs(nums, n, pathQueue, usedArr, ansList); return ansList; } private void dfs(int[] nums, int n, Deque\u0026lt;Integer\u0026gt; pathQueue, boolean[] usedArr, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ansList) { if (pathQueue.size() == n) { // 递归边界：当分支数据记录到n的数据规模时，递归停止，同时，将记录好的分支数据添加到ansList中 // 如果直接添加pathStack，添加的是引用副本，递归调用中容器内数据会变动，所以我们拷贝一份数据副本 ansList.add(new ArrayList\u0026lt;\u0026gt;(pathQueue)); return; } for (int i = 0; i \u0026lt; nums.length; i++) { if (usedArr[i]) { // 在当前树路径上该节点已被选 continue; } usedArr[i] = true; pathQueue.offerLast(nums[i]); dfs(nums, n, pathQueue, usedArr, ansList); // 回溯：回退之前的状态 usedArr[i] = false; pathQueue.pollLast(); } } 复杂度分析 时间复杂度： 回溯算法时间复杂度复杂度由递归树节点数*节点上的操作决定。 我们假设排列为 A(n,m) 表示从n个数中选m个为一组排列。等同于P(n,m)。\n非叶子节点数： 第一层：A(n,1)，从n个数中选一个数； 第二层：A(n,2)，从n个数中选两个数； \u0026hellip; 倒数第一层：A(n,m-1)，从n个数中选m-1 个数； 非叶子节点总数为上述和，代入排列公式： 1 2 3 4 5 6 7 sum= n!/(n-1)! + n!/(n-2)! + ... + n!/(n-m+1)! = n!*((1/(n-1)!) + (1/(n-2)!) +...+ (1/(n-m+1)!) ) 其中 n==m ，代入公式，得 sum= n!*((1/(n-1)!) + (1/(n-2)!) +...+ 1) \u0026lt;= n!*((1/(2^(n-1))) + ... + (1/4) + (1/2) +1) \u0026lt;= n!*2*((2^n-1)/2^n) \u0026lt; 2n! 非叶子节点上的操作 在每个非叶子节点上我们都通过dfs()方法去遍历nums，也就是一个节点上我们都得循环查找未使用的位置。这个操作代价为 n（需要给pathQueue装满数据，也就是size==n）。 综上，回溯非叶子节点的复杂度为 O(n*2n!)\n叶子节点数：n!\n叶子节点上的操作：ansList.add(new ArrayList\u0026lt;\u0026gt;(pathQueue)); 拷贝n个数据的数组，代价为n；\n综上回溯叶子节点的复杂度为 O(n*n!)\n综上，整个Permutations回溯的时间复杂度为 O(n*n!)。\n空间复杂度： 全排列一共n!种排列形式，其中每一种排列中，我们使用siz==n的容器空间开销，因此空间复杂度为 O(n*n!)\nRef liweiwei1419 5.1 回溯算法简介 负雪明烛 46. Permutations 全排列 B站 46. 全排列 Permutations 【LeetCode 力扣官方题解】 ","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/permutations/","summary":"\u003cp\u003eLeetCode 46. 全排列 \u003ccode\u003eDFS\u003c/code\u003e题解，理解\u003ccode\u003eDFS\u003c/code\u003e、回溯、递归树。\u003c/p\u003e","title":"LeetCode 46. 全排列 DFS题解"},{"content":"LeetCode 206. 反转链表题解，借用本题与关联题目，加强对递归的理解。\n借着几道简单题，加强理解下递归。\n递归 递归是一种编程技巧。\n递归（英语：Recursion），又译为递回，在数学与计算机科学中，是指在函数的定义中使用函数自身的方法。递归一词还较常用于描述以自相似方法重复事物的过程。例如，当两面镜子相互之间近似平行时，镜中嵌套的图像是以无限递归的形式出现的。也可以理解为自我复制的过程。\n一种便于理解的心理模型，是认为递归定义对对象的定义是按照“先前定义的”同类对象来定义的。\n使用分治的思想，一个大问题我们可以拆分为n个小问题，其中n问题可由n-1的解获得。 即后序答案由前序答案获得，同时递归有处理边界，总有一个前序的解是已知的。\n任何使用递归实现的方法，都可以用非递归的方式（压栈）实现。\n递归在程序中包含两个重要部分：\n重复性、可叠加、最小化的动作【递推公式】； 边界条件，即函数是一定可返回、不爆栈的； 案例 排队获取队号 假设我们去做核酸排队，我们并不知道自己排在多少位。如何算出排位呢？\n迭代：跳出队列，挨个数。 递归：问前面的人，前面的人再问前面的人，最后排在首位与工作人员接头的就是一号，将号码传回，依次叠加。 递归的做法相比迭代，我们自己的工作量少了很多，只需要在前序数字上+1。\n递推公式 1 f(n)=f(n-1)+1 边界条件 1 n==1 【与核算人员接头了】 代码 1 2 3 4 5 6 public int getQueueNum(DoubleListNode node) { if (node.prev == null) { return 1; } return getQueueNum(node.prev) + 1; } 计算斐波那契数列第n个值 在数学上，斐波那契数列以如下被以递推的方法定义：F(0)=0，F(1)=1, F(n)=F(n - 1)+F(n - 2)（n ≥ 2，n ∈ N*）\n递推公式 1 F(n)=F(n - 1)+F(n - 2) 边界条件 1 F(0)=0，F(1)=1 【数列前两个值是已知的】 代码 1 2 3 4 5 6 7 8 9 public int fib(int n) { if (n == 0) { return 0; } if (n == 1) { return 1; } return fib(n - 1) + fib(n - 2); } 小结 考虑问题的递归方案时，关键就在于分析：\n递推公式； 边界条件； 同时跳转型数据结构很多问题，天然有被递归解决的特性，如链表、树结构。因为每一层、每个节点结构都是一致的，问题的解也是一致的，只是序列、前后项不同。\n同时思考解时，只考虑相邻解的关系，不要被递归执行序绕晕。\n优点 缺点 解决一些复杂问题时，编码很简洁 由于递归执行需要栈空间，会带来额外开销 无需多余的辅助变量、循环 可能造成OOM、栈溢出 可以用记录法、缓存降低复杂性、提高性能 不好的结构可能引发不必要的复杂性 非常适合处理跳转型、结构化的格式：树、图、JSON 题目 反转链表 给你单链表的头节点 head ，请你反转链表，并返回反转后的链表。 https://leetcode.cn/problems/reverse-linked-list/\n分析 问题是：\n反转链表； 返回反转后的链表头节点； 递归公式 我们先考虑问题1，尽可能缩小问题集。对应解设为f(node)，那么递推公式可以写为：\n1 f(node)=node节点的下个节点的next指针指向node + f(node.next) 考虑递推公式时，我们可以假设f(node.next)的前序/后序问题已解决，只要有边界，递归解就一定执行的完。\n边界条件 个人觉得有两个边界：\n头节点反转时，需要设置head.next=null，因为后序节点的解不管这个，所以需要在头节点边界处理好； 调用到最后一个节点后，递归解停止执行； 代码 根据以上分析，可得代码：\n1 2 3 4 5 6 7 8 9 10 11 public static void reverseList(ListNode node) { // 边界2 if (node == null || node.next == null) { return; } reverseList(node.next); // 下个的节点的next指向当前节点 node.next.next = node; // 边界1 node.next = null; } 问题1已解，我们再考虑问题2：2. 返回反转后的链表头节点；，在边界2的返回处，我们将原先的末端节点返回，同时在递归调用f(node)的地方拿到此节点，递归序执行完毕后，将该结果返回给调用方。\n代码微调即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 public static ListNode reverseList(ListNode node) { // 边界2 if (node == null || node.next == null) { // 末端节点作为边界返回 return node; } ListNode lastOne=reverseList(node.next); // 下个的节点的next指向当前节点 node.next.next = node; // 边界1 node.next = null; return lastOne; } 相同的树 给你两棵二叉树的根节点 p 和 q ，编写一个函数来检验这两棵树是否相同。\n如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。\nhttps://leetcode.cn/problems/same-tree/\n分析 问题是：\n判断树结构、值是否相同； 递归公式 如果是判断线性非跳转结构，比如数组，那我们的想法无法就是起两个指针，依次判断对应索引上值是不是一致即可。\n类比一下，我们解设解为f(node1,node2)，那么递推公式可以写为：\n1 2 3 4 // val判断值相等 // node1.left,node2.left node1.right,node2.right 判断保证结构一致 // 只要有一个判断为false，f解整体为false，因此每个递归调用间关系为且 f(node1,node2)=node1.val==node2.val + f(node1.left,node2.left) + f(node1.right,node2.right) 边界条件 两个边界：\n当前节点值不同时，f(node1,node2)的解即为false； 调用到最后一个节点后，递归解停止执行； 代码 根据以上分析，可得代码：\n1 2 3 4 5 6 7 8 9 public boolean isSameTree(TreeNode p, TreeNode q) { if (p == null) { return q == null; } if (q == null) { return false; } return p.val == q.val \u0026amp;\u0026amp; isSameTree(p.left, q.left) \u0026amp;\u0026amp; isSameTree(p.right, q.right); } 另一棵树的子树 给你两棵二叉树 root 和 subRoot 。检验 root 中是否包含和 subRoot 具有相同结构和节点值的子树。如果存在，返回 true ；否则，返回 false 。\n二叉树 tree 的一棵子树包括 tree 的某个节点和这个节点的所有后代节点。tree 也可以看做它自身的一棵子树。\nhttps://leetcode.cn/problems/subtree-of-another-tree/\n分析 问题是：\n判断后者是否为前者树🌲的子集； 从某个节点开始，两棵树结构、值一致； 所以本题解有部分逻辑与【相同的树】一致。\n递归公式 我们解设解为f(root,subRoot)，那么递推公式可以写为：\n1 2 3 4 // isSameTree 判断树是否一致，参考上题分析 // node1.left,node2.left node1.right,node2.right 判断寻找下个可能满足sameTree的节点 // 只要有一个判断为true，f解整体为true，因此每个递归调用间关系为或 f(root,subRoot)=isSameTree(root,subRoot) + f(node1.left,node2.left) + f(node1.right,node2.right) 边界条件 两个边界：\nisSameTree返回true，即root某个节点开始找到了与subRoot的子树时，解为true； 调用到最后一个节点后，递归解停止执行； 代码 根据以上分析，可得代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public boolean isSubtree(TreeNode root, TreeNode subRoot) { if (root == null) { return subRoot == null; } return isSameTree(root, subRoot) || isSubtree(root.left, subRoot) || isSubtree(root.right, subRoot); } public boolean isSameTree(TreeNode p, TreeNode q) { if (p == null) { return q == null; } if (q == null) { return false; } return p.val == q.val \u0026amp;\u0026amp; isSameTree(p.left, q.left) \u0026amp;\u0026amp; isSameTree(p.right, q.right); } 24. 两两交换链表中的节点 2022-10-11 新增\n给你一个链表，两两交换其中相邻的节点，并返回交换后链表的头节点。你必须在不修改节点内部的值的情况下完成本题（即，只能进行节点交换）。\nhttps://leetcode.cn/problems/swap-nodes-in-pairs/\n分析 问题是：\n链表节点两个为一组，两个节点互换位置（a/b两节点，a.next指向b.next，b.next指向原b（a.next），同时需维护组外的前序，即prev.next=b）； 组内节点互换； 组外维护新的首尾； 一组两个节点称为一个pair； 递归公式 将对应解设为f(node)，那么递推公式可以写为：\n1 2 // node.next.next 表示下一组的头 f(node)=node/node.next互换 + f(node.next.next) 同样，考虑递推公式时，我们需要假设f(node.next)的前序/后序问题已解决，只要有边界，递归解就一定执行的完。\n边界条件 当下一组只有一个节点 或者 下一组没有节点时，我们的f()就执行完了； 代码 根据以上分析，可得代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /** * 返回当前链表被两两交换后的head头 * \u0026lt;p\u0026gt; * 时间复杂度：整个过程完整遍历一次链表，O(n) * 空间复杂度：递归有栈开销，O(n) */ public ListNode swapPairs(ListNode head) { if (head == null || head.next == null) { return head; } // 暂存head.next.next（用于反转下一段，并且返回给当前段用作segment.next） // 暂存head.next（用于返回新链表的头） ListNode third = head.next.next, second = head.next; head.next.next = head; head.next = swapPairs(third); return second; } ","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/reverse-linked-list/","summary":"\u003cp\u003eLeetCode 206. 反转链表题解，借用本题与关联题目，加强对递归的理解。\u003c/p\u003e","title":"LeetCode 206. 反转链表 递归题解"},{"content":"解读编程技巧中的sentinel value、哨兵。\n定义 sentinel哨兵语义：守卫边界的士兵。\nsentinel作为一种编程技巧具备了守卫边界的语义。\n用途 标识一段循环、序列的边界（起点、终点） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // 打印输入的数的和，0作为sentinel终止标识 public void printNumSumUntil0() { int number, sum, count; count = 0; sum = 0; // Creating object of Scanner class to take keyboard input Scanner input = new Scanner(System.in); System.out.println(\u0026#34;Enter any number, or 0 to stop\u0026#34;); // taking input from keyboard number = input.nextInt(); // number zero(0) is the sentinel value while (number != 0) { sum = sum + number; count++; System.out.println(\u0026#34;Enter another number, or 0 to stop\u0026#34;); number = input.nextInt(); } // Printing sum of all the numbers inputted System.out.println(\u0026#34;The sum of numbers = \u0026#34; + sum); } 统一、简化写法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // 如链表中的`dummy`哑节点指向原头节点例子 // 合并两个有序链表 public static ListNode mergeTwoLists(ListNode l1, ListNode l2) { if (l1 == null) { return l2; } if (l2 == null) { return l1; } ListNode dummy = new ListNode(); // 使用了一个空的dummy节点，不需要其他特殊判断 ListNode prev=dummy; while (l1 != null \u0026amp;\u0026amp; l2 != null) { if (l1.val \u0026gt; l2.val) { prev = prev.next = l2; l2 = l2.next; } else { prev = prev.next = l1; l1 = l1.next; } } prev.next = l1 == null ? l2 : l1; return dummy.next; } 特殊处理哨兵边界值，以提升性能 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 // 在数组a中，查找key，返回key所在的位置 // 其中，n表示数组a的长度 int find(char* a, int n, char key) { // 边界条件处理，如果a为空，或者n\u0026lt;=0，说明数组中没有数据，就不用while循环比较了 if(a == null || n \u0026lt;= 0) { return -1; } int i = 0; // 这里有两个比较操作：i\u0026lt;n和a[i]==key. while (i \u0026lt; n) { if (a[i] == key) { return i; } ++i; } return -1; } // 在数组a中，查找key，返回key所在的位置 // 其中，n表示数组a的长度 // 我举2个例子，你可以拿例子走一下代码 // a = {4, 2, 3, 5, 9, 6} n=6 key = 7 // a = {4, 2, 3, 5, 9, 6} n=6 key = 6 int find(char* a, int n, char key) { if(a == null || n \u0026lt;= 0) { return -1; } // 这里因为要将a[n-1]的值替换成key，所以要特殊处理这个值 if (a[n-1] == key) { return n-1; } // 把a[n-1]的值临时保存在变量tmp中，以便之后恢复。tmp=6。 // 之所以这样做的目的是：希望find()代码不要改变a数组中的内容 char tmp = a[n-1]; // 把key的值放到a[n-1]中，此时a = {4, 2, 3, 5, 9, 7} a[n-1] = key; int i = 0; // while 循环比起代码一，少了i\u0026lt;n这个比较操作 while (a[i] != key) { ++i; } // 恢复a[n-1]原来的值,此时a= {4, 2, 3, 5, 9, 6} a[n-1] = tmp; if (i == n-1) { // 如果i == n-1说明，在0...n-2之间都没有key，所以返回-1 return -1; } else { // 否则，返回i，就是等于key值的元素的下标 return i; } } 归并排序中merge()操作可以考虑使用sentinel value统一处理一些逻辑，例子： https://github.com/redolog/algorithm-java/commit/98ad862de98fba36f7cc4aa2f44ff135d61e1516\n但是个人认为徒增了复杂度。\nRef https://www.answers.com/Q/What_is_a_sentinel_in_programming https://www.tutorialsfield.com/sentinel-value-java/ ","permalink":"https://redolog.github.io/posts/rd/faq/sentinel-value/","summary":"\u003cp\u003e解读编程技巧中的\u003ccode\u003esentinel value\u003c/code\u003e、哨兵。\u003c/p\u003e","title":"sentinel value in programming"},{"content":"LeetCode 142. 环形链表 II 题解，寻找链表环入口。\n题目 https://leetcode.cn/problems/linked-list-cycle-ii/ 寻找环入口。\n题解 数学版本。经过作为运营的女朋友的指导，我懂了数学推导的解法。\n假设 假设我们的链表长这样：\nB \u0026lt;- \u0026lt;- \u0026lt;- \u0026lt;- \u0026lt;-| | | | | O -\u0026gt; x -\u0026gt; x -\u0026gt; x -\u0026gt; x -\u0026gt; A -\u0026gt; x -\u0026gt; x - \u0026gt; x - \u0026gt; x 其中\nO：链表起点 x：普通的中间节点 A：链表环入口 B：快慢指针相遇节点 D：distance 表示路程 s：slow 慢指针路程 f：fast 快指针路程 n：相遇时快指针在环内已运行的圈数 a：OA b：环一圈 我们启动两个指针\n慢指针：slow一次前行一步 slow=slow.next 快指针：fast一次前行两步步 fast=fast.next.next 推导 假设现在快慢指针在B点相遇，我们可以得出：\n快指针路程为慢指针二倍：f=2s 快指针路程为慢指针路程（a+AB）加上n圈环路程：f=s+nb 综上1-2，可得 s=nb，也就是相遇时，慢指针恰好走了 n圈的环 对应路程 分析： 而我们分析下，当从链表起点走到环入口的地方时，路程满足：a+nb，n==0时则是慢指针刚好第一次经过环入口，之后便是在环内重复绕圈。\n此时，根据上方3结论：慢指针恰好走了 nb，那么只要慢指针再走 a 的路程，就到了A点。\n因此，程序可以这么写：快慢指针运行，第一次相遇（B点）后，令快指针回到O点，以一步的步长前进，当两个指针再次相遇时，此时恰好是A点，即题目所求。\n对应代码： https://github.com/redolog/algorithm-java/commit/60f69f4fa0825e91e9595bf6c549999b3b70ac7f\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public static ListNode detectCycleWithMath(ListNode head) { if (head == null || head.next == null) { return null; } ListNode fast = head, slow = head; while (fast != null \u0026amp;\u0026amp; fast.next != null) { slow = slow.next; fast = fast.next.next; if (slow == fast) { fast = head; break; } } while (fast != null \u0026amp;\u0026amp; slow != null) { if (slow == fast) { return slow; } slow = slow.next; fast = fast.next; } return slow; } ","permalink":"https://redolog.github.io/posts/rd/algo/oj/leetcode/linked-list-cycle-ii/","summary":"\u003cp\u003eLeetCode 142. 环形链表 II 题解，寻找链表环入口。\u003c/p\u003e","title":"LeetCode 142. 环形链表 II 题解"},{"content":"MySQL性能优化cheatsheet。\nSQL执行过程 客户端发送一条查询给服务器； 服务器通过权限检查之后，先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段； 服务器端进行SQL解析、预处理，再由优化器根据该SQL所涉及到的数据表的统计信息进行计算，生成对应的执行计划； MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询； 将结果返回给客户端。 SQL执行的最大瓶颈在于磁盘的IO，即数据的读取；不同SQL的写法，会造成不同的执行计划的执行，而不同的执行计划在IO的上面临完全不一样的数量级，从而造成性能的差距。\n所以，优化SQL，其实就是让查询优化器根据我们的计划选择匹配的执行计划，来减少查询中产生的IO。\nschema(表结构)对性能的影响 冗余数据的处理； 适当的数据冗余可以提高系统的整体查询性能(项目中有些用户信息需要经常一起拿出，可以考虑冗余信息到用户表)；\n大表拆小表，有大数据的列单独拆成小表；\n在一个数据库中，一般不会设计属性过多的表； 在一个数据库中，一般不会有超过500/1000万数据的表(拆表，按照逻辑拆分，按照业务拆分)； 有大数据的列单独拆成小表(富文本、静态数据)； 根据需求的展示设置更合理的表结构；\n把常用属性分离成小表；\n项目中，我们可以根据信息被使用的特征拆成多张表； 减少查询常用属性需要查询的列； 关系数据库的三范式：\n第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库，是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值； 第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。 第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。 (不允许有冗余数据) 索引 核心原理 索引和索引的优化：\n索引的原理：把无序的数据变成有序的查询（B+树）； 结构 索引的物理结构：\n数据库文件存储的位置：my.ini配置文件中dataDir对应的数据目录中； 每一个数据库一个文件夹； MYISAM引擎：每一个表(table_name)\u0026ndash;\u0026gt; table_name.MYI：存放的是数据表对应的索引信息和索引内容； table_name.FRM：存放的是数据表的结构信息； table_name.MYD：存放的是数据表的内容； InnoDB引擎：每一个表(table_name)\u0026ndash;\u0026gt; table_name.frm：存放的是数据表的结构信息； 数据文件和索引文件都是统一存放在ibdata文件中； 索引文件都是额外存在的，对索引的查询和维护都是需要消耗IO的； 索引的结构：\n默认情况下，一旦创建了一个表，这个表设置了主键，那么MySQL会自动的为这个主键创建一个unique的索引； 索引类型： Normal：普通的索引；允许一个索引值后面关联多个行值； UNIQUE：唯一索引；允许一个索引值后面只能有一个行值；之前对列添加唯一约束其实就是为这列添加了一个unique索引；当我们为一个表添加一个主键的时候，其实就是为这个表主键列(设置了非空约束)，并为主键列添加了一个唯一索引； Fulltext：全文检索，MySQL初期的全文仅在MyISAM上支持，目前InnoDB也支持，可参考 innodb-fulltext-index，一般情况下我们不在MySQL层面实践全文搜索； 索引的方法(规定索引的存储结构)： (数据结构，算法基础) b+tree：是一颗树(n叉树）： 使用平衡树实现索引，是mysql中使用最多的索引类型； 在innodb中，存在两种索引类型，第一种是主键索引（primary key），在索引内容中直接保存数据的地址； 第二种是其他索引，在索引内容中保存的是指向主键索引的引用； 所以在使用innodb的时候，要尽量的使用主键索引，速度非常快； b+tree很好地支持了范围查询，叶子节点持有下个数据页的指针； hash：把索引的值做hash运算，并存放到hash表中，使用较少，一般是memory引擎使用； 优点：因为使用hash表存储，按照常理，hash的性能比B-TREE效率高很多。 hash索引的缺点： hash索引只能适用于精确的值比较，=，in，或者\u0026lt;\u0026gt;；无法使用范围查询； 无法使用索引排序； 利弊 索引的利弊：\n索引的好处：\n提高表数据的检索效率； 如果排序的列是索引列(如果查询的列==排序的列[并且在这列上做了索引])，大大降低排序成本； 在分组操作中如果分组条件是索引列，也会提高效率； 索引的问题：\n索引需要额外的维护成本； 因为索引文件是单独存在的文件，对数据的增加，修改，删除，都会产生额外的对索引文件的操作，这些操作需要消耗额外的IO，会降低增/改/删的执行效率； 使用标准 怎么创建索引？\n较频繁的作为查询条件的字段应该创建索引； 唯一性太差的字段不适合单独创建索引，即使频繁作为查询条件； 案例分析 作为索引的列，如果不能有效的区分数据，那么这个列就不适合作为索引列；比如(性别，状态不多的状态列)\n举例：\n1 2 -- accountflow 表示账户流水 SELECT sum(amount) FROM accountflow WHERE accountType = 0； 假如把accountType作为索引列，因为accountType只有14种，所以，如果根据accountType来创建索引，最多只能按照1/14的比例过滤掉数据；但是，如果可能出现，只按照该条件查询，那我们就要考虑到其他的提升性能的方式了；\n方案：\n第一种方案：单独创建一个系统摘要表；在这个表里面有一个列叫做系统总充值金额；每次充值成功，增加这个列的值；以后要查询系统总充值金额，只需要从这个系统摘要表中查询；(缺陷：如果充值频率过快，会导致表的锁定问题；) 第二种方案：流水一旦发生了，是不会随着时间改变的；针对这种信息，我们就可以使用增量查询（结算+增量查询）； 创建一张日充值表；记录每一天的充值总金额(beginDate，endDate，totalAmount)，每天使用定时器对当前的充值记录进行结算；日充值报表里面记录只能记录截止昨天的数据； 创建一张月充值表；记录每一个月的充值总金额(beginDate，endDate，totalAmount)，每月最后一天使用定时器对当月的充值记录进行结算(数据源从日充值报表来)； 要查询系统总充值，从月报表中汇总(当前月之前的总充值金额)，再从日充值报表中查询当天之前的日报表数据汇总；再从流水中查询当前截止查询时间的流水；使用另外一张当天流水表记录当天的流水；再把三个数据累加； 更新非常频繁的字段不适合创建索引；原因，索引有维护成本； 不会出现在WHERE 子句中的字段不该创建索引； 索引不是越多越好；(只为必要的列创建索引) 不管你有多少个索引，一次查询至多采用一个索引；(索引和索引之间是独立的) 因为索引和索引之间是独立的，所以说每一个索引都应该是单独维护的；数据的增/改/删，会导致所有的索引都要单独维护； 索引的使用限制： BLOB 和TEXT 类型的列只能创建前缀索引；\nMySQL 目前不支持函数索引（在MySQL中，索引只能是一个列的原始值，不能把列通过计算的值作为索引）； 实例：请查询1981年入职的员工：\n1 SELECT * FROM emp WHERE year(hire_date)=\u0026#39;1981\u0026#39;； 问题：查询的列是在过滤之前经过了函数运算；所以，就算hire_date作为索引，year(hire_date)也不会使用索引； 解决方案：\n1 SELECT * FROM emp WHERE hire_date BETWEEN \u0026#39;1981-01-01\u0026#39; AND \u0026#39;1981-12-31\u0026#39;； 在创建一列，这列的值是year(hire_date)，然后把这列的值作为索引； 使用不等于（!= 或者\u0026lt;\u0026gt;）的时候MySQL 无法使用索引\n过滤字段使用了函数运算后（如abs(column)），MySQL 无法使用索引\nJoin 语句中Join 条件字段类型不一致的时候MySQL 无法使用索引\n使用LIKE 操作的时候如果条件以通配符开始（ \u0026lsquo;%abc\u0026hellip;\u0026rsquo;）MySQL 无法使用索引\n字符串是可以用来作为索引的； 字符串创建的索引按照字母顺序排序； 如果使用LIKE，实例：SELECT * FROM userinfo WHERE realName LIKE \u0026lsquo;吴%\u0026rsquo;；这种情况是可以使用索引的； 但是LIKE \u0026lsquo;_嘉\u0026rsquo; 或者LIKE \u0026lsquo;%嘉\u0026rsquo;都是不能使用索引的； 使用非等值查询的时候MySQL 无法使用Hash 索引\n单列索引和复合索引： 因为一个查询一次至多只能使用一个索引，所以，如果都使用单值索引(一个列一个索引)，在数据量较大的情况下，不能很好的区分数据； 所以，MySQL引入了多值索引(复合索引)； 复合索引就是由多列的值组成的索引；并且(注意)，多列的索引是有顺序的!!!! 复合索引的原理：就是类似orderby(orderby后面可以跟多个排序条件order by hire_date，username desc)； 就是在排序和分组(创建倒排表的时候)，按照多个列进行排序和合并； 1 2 3 4 SELECT * FROM accountflow WHERE actionTime \u0026lt; \u0026#39;xxxxx\u0026#39; AND account_id = 5 可以使用actionTime+account_id的复合索引； SELECT * FROM accountflow WHERE actionTime \u0026lt; \u0026#39;xxxxx\u0026#39; 可以使用actionTime+account_id的复合索引； SELECT * FROM accountflow WHERE account_id = 5 不可以使用actionTime+account_id的复合索引； SELECT * FROM accountflow WHERE account_id = 5 AND actionTime \u0026lt; \u0026#39;xxxxx\u0026#39; 不可以使用actionTime+account_id的复合索引； 复合索引，在查询的时候，遵守向左原则；只要在查询的时候，是按照复合索引从左到右的顺序依次查询，不管查询条件是否完全满足所有的符合索引的列，都可以使用部分的符合索引； 在实际应用中，基本上都使用复合索引； SQL优化 查看MySQL的执行计划和执行明细状态(explain+profiling)\nExplain：可以让我们查看MySQL执行一条SQL所选择的执行计划； Profiling：可以用来准确定位一条SQL的性能瓶颈； EXPLAIN： 使用方式： explain SQL； 返回结果： ID：执行查询的序列号； select_type：使用的查询类型 DEPENDENT SUBQUERY：子查询中内层的第一个SELECT，依赖于外部查询的结果集； DEPENDENT UNION：子查询中的UNION，且为UNION 中从第二个SELECT 开始的后面所有SELECT，同样依赖于外部查询的结果集； PRIMARY：子查询中的最外层查询，注意并不是主键查询； SIMPLE：除子查询或者UNION 之外的其他查询； SUBQUERY：子查询内层查询的第一个SELECT，结果不依赖于外部查询结果集； UNCACHEABLE SUBQUERY：结果集无法缓存的子查询； UNION：UNION 语句中第二个SELECT 开始的后面所有SELECT，第一个SELECT 为PRIMARY UNION RESULT：UNION 中的合并结果； table：这次查询访问的数据表； type：对表所使用的访问方式： all：全表扫描 const：读常量，且最多只会有一条记录匹配，由于是常量，所以实际上只需要读一次； eq_ref：最多只会有一条匹配结果，一般是通过主键或者唯一键索引来访问； fulltext：全文检索，针对full text索引列； index：全索引扫描； index_merge：查询中同时使用两个（或更多）索引，然后对索引结果进行merge 之后再读取表数据； index_subquery：子查询中的返回结果字段组合是一个索引（或索引组合），但不是一个主键或者唯一索引； rang：索引范围扫描； ref：Join 语句中被驱动表索引引用查询； ref_or_null：与ref 的唯一区别就是在使用索引引用查询之外再增加一个空值的查询； system：系统表，表中只有一行数据； unique_subquery：子查询中的返回结果字段组合是主键或者唯一约束； possible_keys：可选的索引；如果没有使用索引，为null； key：最终选择的索引； key_len：被选择的索引长度； ref：过滤的方式，比如const（常量），column（join），func（某个函数）； rows：查询优化器通过收集到的统计信息估算出的查询条数； Extra：查询中每一步实现的额外细节信息 Distinct：查找distinct 值，所以当mysql 找到了第一条匹配的结果后，将停止该值的查询而转为后面其他值的查询； Full scan on NULL key：子查询中的一种优化方式，主要在遇到无法通过索引访问null值的使用使用； Impossible WHERE noticed after reading const tables：MySQL Query Optimizer 通过收集到的统计信息判断出不可能存在结果； No tables：Query 语句中使用FROM DUAL 或者不包含任何FROM 子句； Not exists：在某些左连接中MySQL Query Optimizer 所通过改变原有Query 的组成而使用的优化方法，可以部分减少数据访问次数； Select tables optimized away：当我们使用某些聚合函数来访问存在索引的某个字段的时候，MySQL Query Optimizer 会通过索引而直接一次定位到所需的数据行完成整个查询。当然，前提是在Query 中不能有GROUP BY 操作。如使用MIN()或者MAX（）的时候； Using filesort：当我们的Query 中包含ORDER BY 操作，而且无法利用索引完成排序操作的时候，MySQL Query Optimizer 不得不选择相应的排序算法来实现。 Using index：所需要的数据只需要在Index 即可全部获得而不需要再到表中取数据； Using index for group-by：数据访问和Using index 一样，所需数据只需要读取索引即可，而当Query 中使用了GROUP BY 或者DISTINCT 子句的时候，如果分组字段也在索引中，Extra 中的信息就会是Using index for group-by； Using temporary：当MySQL 在某些操作中必须使用临时表的时候，在Extra 信息中就会出现Using temporary 。主要常见于GROUP BY 和ORDER BY 等操作中。 Using where：如果我们不是读取表的所有数据，或者不是仅仅通过索引就可以获取所有需要的数据，则会出现Using where 信息； Using where with pushed condition：这是一个仅仅在NDBCluster 存储引擎中才会出现的信息，而且还需要通过打开Condition Pushdown 优化功能才可能会被使用。控制参数为engine_condition_pushdown 。 profiling： Query Profiler是MySQL5.1之后提供的一个很方便的用于诊断Query执行的工具，能够准确的获取一条查询执行过程中的CPU，IO等情况；\n开启profiling：set profiling=1； 执行QUERY，在profiling过程中所有的query都可以记录下来； 查看记录的query：show profiles； 选择要查看的profile：show profile cpu， block io for query 6； status是执行SQL的详细过程； Duration：执行的具体时间； CPU_user：用户CPU时间； CPU_system：系统CPU时间； Block_ops_in：IO输入次数； Block_ops_out：IO输出次数；\nprofiling只对本次会话有效；\nJOIN： JOIN的原理： 在mysql中使用Nested Loop Join来实现join； A JOIN B：通过A表的结果集作为循环基础，一条一条的通过结果集中的数据作为过滤条件到下一个表中查询数据，然后合并结果； JOIN的优化原则： 尽可能减少Join 语句中的Nested Loop 的循环总次数，用小结果集驱动大结果集； 优先优化Nested Loop 的内层循环； 保证Join 语句中被驱动表上Join 条件字段已经被索引； 扩大join buffer的大小； 优化原则 原则一：选择需要优化的SQL 选择需要优化的SQL：\n不是所有的SQL都需要优化，在优化的过程中，首选更需要优化的SQL； 怎么选择?优先选择优化高并发低消耗的SQL；\n1小时请求1W次，1次10个IO； 1小时请求10次，1次1W个IO； 考虑： 从单位时间产生的IO总数来说，相同的； 针对一个SQL，如果我能把10个IO变成7个IO，一小时减少3W个IO； 针对第二个SQL，如果能把1W个IO变成7K个IO，一小时减少3W个IO； 从优化难度上讲，1W-\u0026gt;7K难的多； 从整体性能上来说，第一个SQL的优化能够极大的提升系统整体的性能；第二个SQL慢一点，无非也就是10个连接查询慢一点； 定位性能瓶颈；\nSQL运行较慢有两个影响原因，IO和CPU，明确性能瓶颈所在； 明确优化目标； 原则二：从Explain和Profile入手 任何SQL的优化，都从Explain语句开始；Explain语句能够得到数据库执行该SQL选择的执行计划； 首先明确需要的执行计划，再使用Explain检查； 使用profile明确SQL的问题和优化的结果； 原则三：永远用小结果集驱动大的结果集 原则四：在索引中完成排序 原则五：使用最小Columns 减少网络传输数据量； 特别是需要使用column排序的时候。MySQL排序原理，是把所有的column数据全部取出，在排序缓存区排序，再返回结果；如果column数据量大，排序区容量不够的时候，就会使用先column排序，再取数据，再返回的多次请求方式； 原则六：使用最有效的过滤条件 过多的WHERE条件不一定能够提高访问性能； 一定要让WHERE条件使用自己预期的执行计划； 原则七：避免复杂的JOIN和子查询 复杂的JOIN和子查询，需要锁定过多的资源，MySQL在大量并发情况下处理锁定性能下降较快； 不要过多依赖SQL的功能，把复杂的SQL拆分为简单的SQL； MySQL子查询性能较低，应尽量避免使用； innodb_buffer和事务 Innodb_buffer_pool_size：innodb的缓存，可以用于缓存索引，同时还会缓存实际的数据； innodb_buffer_pool_size 参数用来设置Innodb 最主要的Buffer(Innodb_Buffer_Pool)的大小，对Innodb 整体性能影响也最大，可以按需要设置大一些；\ninnodb中的事务处理：\n理解Innodb事务机制： 事务在buffer中对数据进行修改； 事务的变化记录在事务日志中； 在合适的时机同步事务日志中的数据到数据库中； 所以什么时候提交事务日志文件，对系统性能影响较大，可以通过设置innodb_flush_log_at_trx_commit来修改事务日志同步时机： innodb_flush_log_at_trx_commit = 0，每1秒钟同步一次事务日志文件； innodb_flush_log_at_trx_commit = 1. 默认设置，每一个事务完成之后，同步一次事务日志文件； innodb_flush_log_at_trx_commit = 2. 事务完成之后，写到事务日志文件中，等到日志覆盖再同步数据； 注意，1性能最差，2不能完全保证数据是写到数据文件中，如果宕机，可能会有数据丢失现象，但性能最高；1性能和安全性居中； 主从架构 简单理解下主从架构诞生的背景： 应用开发中常见的由量带来的问题：系统太卡！\n研发判断是web服务器的压力过大，此时我们可以增加web服务器的配置、数量，由单点增加量后借助负载均衡来分发流量，这样可以解决web服务器压力过大的问题。 如果还是卡，通过查看MySQL监控发现QPS上来了，单点数据库遇到瓶颈，可以考虑在应用层增加缓存层，此时MySQL单点压力被分发了，问题解决。 如果还是卡，并且已经实施了上述方案，分析应用使用MySQL的姿势，发现大多数请求都是查询。此时可以考虑主从架构，使用从节点分担主节点读的压力。 MySQL主从复制的文档，默认复制操作是异步的，因此在TPS突刺时会有延迟的现象，具体优化可参见官方配置文档。\n考虑到主从复制的延迟问题，研发在应用设计实现时需做提前考虑：\n所有写操作由主节点完成； 对读写一致性实时性要求高的业务需在应用层指定策略（一直读主、延迟读从）； 对读写一致性实时性要求不高的业务可以宽松处理； 事务需保证读写都在一个实例上； 主从的大致流程：\n主从同步的目的是要保证从节点与主节点一致的DDL/DML/TCL语句的执行状态，MySQL使用binlog记录变更。\n打开binlog，主节点记录所有DDL/DML/TCL语句； 主节点使用被动注册的方式联通主从节点，从节点主动请求主节点，这种方式下增删从节点对主节点无影响； 从节点起一个线程向主节点请求，询问某个点之后更新的数据； 主节点收到从节点请求，读取binlog，返回给从节点； 从节点获取响应之后将数据写入relaylog，relaylog记录主节点请求返回的变更； 从节点开启另外专用的线程处理relaylog； 2024.02.27补充图例： Ref 后端思维之数据库性能优化方案 ","permalink":"https://redolog.github.io/posts/rd/storage/mysql/performance-optimization/cheatsheet/","summary":"\u003cp\u003e\u003ccode\u003eMySQL\u003c/code\u003e性能优化\u003ccode\u003echeatsheet\u003c/code\u003e。\u003c/p\u003e","title":"MySQL性能优化cheatsheet"},{"content":"设计模式是前辈们针对实际编码中各种问题对应解决方案的抽象总结，是一种最佳实践。因此值得每一位工程师学习借鉴。 使用的时候重点是识别面临的问题的场景，识破问题关键，挑选适当的模式进行编码。 识别这一步最为关键。\nwiki/Software_design_pattern 针对软件设计模式的发展历史有详细说明，简单来说，GoF四人帮大佬们在1994年发布了书籍 Design Patterns: Elements of Reusable Object-Oriented Software ，这本书对于设计模式的工业落地、推广具有重大意义。\n设计模式在工业界实践已久，但是长久以来未能取得进展。\n本文以GoF版本为准，这也是工业界使用最广泛的标准化解释。本文使用Java示例，示例仓库：design-pattern-java。\n设计模式一共23种，分三个大类：\nCreational 创建型\n「不直接创建对象，定义对应的创建逻辑」，解耦对象的创建、使用。\nStructural 结构型\n「关注类、对象间如何组合，一般利用语言特性（比如继承、实现、组合）或者平台特性（agent、字节码）进行结构组装」，解耦不同功能。\nBehavioral 行为型\n「关注对象间如何交互、通信」，解耦不同行为。\n编程指导原则参见 编程原则一览。\nCreational 创建型 Singleton 单例 Singleton 单例\nrestricts object creation for a class to only one instance.\n用于创建全局唯一的对象。\n全局可区分不同粒度：\n线程 进程 集群 目的：限制对象个数。\nPrototype 原型 Prototype 原型\ncreates objects by cloning an existing object.\n描述：\n基于一个已有对象实例，克隆、复制一个新的对象。\n适用场景：\n新旧实例有很多相似之处，可以复制； 新实例重新创建有过多开销； 提供一定的业务语义，比如XNew对象来源于XOld对象，有关联性； 此时可考虑原型。\n使用\n实现接口Cloneable，重写clone方法。\njava本身提供的clone调用的是本地方法，有性能优势，缺点是默认只支持浅拷贝。需要深拷贝一般通过序列化、反序列化实现。\nBuilder 构建者 Builder 构建者\nconstructs complex objects by separating construction and representation.\n描述： 分离构造与表示，以此来创建复杂对象。\n支持一步步构建对象，将构建过程细化，支持创建多种不同属性值的实例。\n适用场景：\n对象属性过多时，使用构建者替换构造器，更加清晰、灵活； 使用\n推荐直接使用@Lombok注解@Builder，利用 JSR269 插件化注解处理的接口，帮我们在生成字节码之前通过注解自动生成代码。\nFactory method 工厂方法 creates objects without specifying the exact class to create.\nAbstract factory 抽象工厂 groups object factories that have a common theme.\n描述：\n工厂方法：不指定特定类，将实例创建交给工厂。 抽象工厂：将具有共同主题的工厂组合起来。 工厂模式用于创建不同但是同类型的对象（OOP中的接口或者父类）。\n相比工厂方法，抽象工厂中的抽象我理解是抽象了共性、层次 工厂创建的对象有共性，工厂也有共性。\n比如轮胎有多种品牌、型号，生产轮胎的厂商也有多个。\n而厂商除了轮胎，还可以生产底盘。轮胎、底盘、厂商，有层次。 在生产汽车的工厂中，定义的接口可能是：生产轮胎，生产底盘。产品间也有层次，汽车是他们共同的主题。\n中文语境下的简单工厂、工厂方法、抽象工厂其实本质上只是所需工厂的复杂度、维度不同。越往后，复杂度、对象层次维度越高。\n目的：解耦对象的创建、使用。\nStructural 结构型 Adapter 适配器 allows classes with incompatible interfaces to work together by wrapping its own interface around that of an already existing class.\n描述： 适配器模式通过包装已有接口的方式利用已有功能，使得新旧不兼容的接口能够对接生效。\n适用场景：\n新老接口定义不兼容（签名不一致）；\n有一种情况是有一个接口设计有缺陷。\n新接口可复用老接口功能实现；\n统一多个接口；\n适配器目的主要是补救、兼容。\n现实举例： 电源适配器，不同地区标准不同的转接头。 场景举例： 我买了一台港版MacBook，一般的电源适配器负责对接笔记本与电源接口，而港版的电脑如果想充电，我就需要再买一个转接头。\n内地电源插线板对应我们的老接口；InlandPatchBoardI 港版插头对应我们的新接口；HongkongPlugI 这个国标转接头对应我们这里的适配器；HongkongPowerAdaper 也就是说，我们先有一个老接口（国标插线板），然后新需求产生了一个新接口（港版插头），此时新增一个国标转接头，新老接口就完成了对接，我们的改动成本也相对可控。\n图例 通过图、代码的方式对比下类适配器与对象适配器的区别。\n类适配器： 对象适配器： 使用入口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class AdapterApp { // 插线板通电 public static void main(String[] args) { // 内地默认逻辑 InlandPatchBoardI inlandPatchBoardGeneral = new InlandPatchBoardImpl(); inlandPatchBoardGeneral.powerOn(); // 港版，class型适配器 InlandPatchBoardI hongkongPatchClsPlug = new HongkongClsAdapter(); hongkongPatchClsPlug.powerOn(); // 港版，object型适配器 InlandPatchBoardI hongkongPatchObjectPlug = new HongkongObjectAdapter(new HongkongPlugImpl()); hongkongPatchObjectPlug.powerOn(); } } 可以看到二者区别为： 类模式使用继承的方式交互，对象模式使用组合的方式交互。一般推荐使用组合，即对象模式实现适配器。\n使用举例\nSpringMVC-WebMvcConfigurerAdapter\n使用SpringMVC的工程如果需要增加拦截器，我们可以实现接口 WebMvcConfigurer，由于Java以前接口必须实现所有非默认方法，所以实现类中必须写出所有接口方法，如果继承 WebMvcConfigurerAdapter，则只需要覆写 addInterceptors 一个方法。\n由于Java8支持了interface提供默认方法，关键字 default ，默认方法实现类无需必须实现。因此后续SpringMVC废弃了 WebMvcConfigurerAdapter。\nApacheCommonsIO-FileAlterationListenerAdaptor\napache的commons-io包下FileAlterationListener，提供了查看文件状态变更的接口。FileAlterationListenerAdaptor 则是对应的适配器实现。\n当我们想要监控文件状态变更时，继承FileAlterationListenerAdaptor是一个更好的选择。\n小结\n适配器最简单的为默认适配器（上述框架使用例子）； 对象模式与类模式，一般推荐使用组合，即对象模式实现适配器； 与代理模式简单对比： 比较对象适配器模式和代理模式。在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是控制访问原方法的活；适配器做的是适配的活，为的是提供“把港版电源线包装成国标的线，然后当做国标电源来使用”，而港版的电源和国标电源之间原本没有继承关系。 Bridge 桥接 decouples an abstraction from its implementation so that the two can vary independently.\n通过抽象接口，与实现解耦，以此达到互相变化不影响的目的。\nComposite 组合 composes zero-or-more similar objects so that they can be manipulated as one object. GoF: Compose objects into tree structure to represent part-whole hierarchies.Composite lets client treat individual objects and compositions of objects uniformly.\n组合多个相似对象为树形结构，表示：「部分、整体」的层次。简化使用方的调用。\n注意：组合模式与OOP中的组合两个类不一样！\n场景：\n操作系统的文件元信息表示是INode，本质上「目录-文件」是树形结构； InnoDB的索引结构是B+树，也是树形结构； 我们的业务应用一般有菜单，菜单一般是树形； 我们的人员组织架构，一般是树形； 代码示例： design-pattern-java Composite\n在这里例子中，我们的部门与员工统一表示为组织节点OrgNode，员工+部门组成树形结构，其中员工、部门分别是「部分」表示，组合在一起形成「整体」。我们的操作统一到了OrgNode中，简化了调用。\nDecorator 装饰 dynamically adds/overrides behaviour in an existing method of an object.\nFacade 门面 provides a simplified interface to a large body of code.\nFlyweight 享元模式 reduces the cost of creating and manipulating a large number of similar objects.\n针对不可变对象：\n复用对象 减少内存开销 减少创建开销 适用场景：\n系统中存在大量重复对象，并且对象是不可变的，此时可实现对象实例的享元，即一个对象大家共用。 在不同的粒度可以实现不同的享元，如细化到属性维度。 目的：复用对象、节省内存。\nProxy 代理 provides a placeholder for another object to control access, reduce cost, and reduce complexity.\n代理对象持有原对象的引用，用于：\n控制访问； 降低开销； 降低复杂度； Behavioral 行为型 Chain of responsibility 职责链 delegates commands to a chain of processing objects. Avoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it.\n将命令委派到处理器的链条上。 通过「将请求交给多个对象处理」的方式，来避免耦合请求的生产者、消费者。 消费者将请求在链条上逐个传递。「GoF定义」 链条可选择某个消费者处理后终止，或者继续传递直到所有消费者都处理完毕。 「变体」 优势：\n复用逻辑。 使用扩展的方式增加功能：高内聚、低耦合「灵活」。 应用：\nFilter Servlet Filter javax.servlet.Filter 对应 IHandler处理器接口。 javax.servlet.FilterChain 对应 HandlerChain链条。 ApplicationFilterChain 对应为Tomcat中对Servlet规范的实现。 Interceptor SpringMVC Interceptor HandlerInterceptor 对应 IHandler处理器接口。 HandlerExecutionChain 对应 HandlerChain链条实现。 Command 命令 creates objects that encapsulate actions and parameters. The command pattern encapsulates a request as an object, thereby letting us parameterize other objects with different requests, queue or log requests, and support undoable operations.\n使用对象封装行为、参数。 命令模式将请求封装成对象，不同的请求对应不同对象，从而将参数内聚到某个类中。支持队列操作、日志记录、回滚「附加功能」。 核心作用：为命令执行增加功能。 Interpreter 解释器 implements a specialized language. Interpreter pattern is used to defines a grammatical representation for a language and provides an interpreter to deal with this grammar.\n描述：\n实现特定语言。 解释器模式用于为一种特定语言提供语法表示，提供解析语法的解释器。 应用偏小众：编译器、规则引擎、正则表达式。 实现：\n解释器主要实现语法解析。 针对负责解析，遵循大拆小的基本策略。 Iterator 迭代器 accesses the elements of an object sequentially without exposing its underlying representation.\nMediator 中介 allows loose coupling between classes by being the only class that has detailed knowledge of their methods. Mediator pattern defines a separate object that encapsulates the interaction between a set of objects and the objects delegate their interaction to a mediator object instead of interacting with each other directly.\n描述：\n中介作为唯一知道上下游对接细节的类「对象」，从而做到解耦上下游对接。 中介模式定义一个中介对象，封装对接双方交互的逻辑，对接双方不直接交互，只能通过中介操作。 现实举例：\n飞机间互相通信，统一通过塔台调度。 多端数据互相同步，统一通过log数据平台交互。 优点：\n集中化管理交互逻辑，简化交互调用。 缺点：\n中介类可能变为「上帝类」，复杂而庞杂。 Memento provides the ability to restore an object to its previous state (undo). Captures and externalize an object\u0026rsquo;s internal state so that it can be restored later, all without violating encapsulation.\n备忘录模式，aka快照模式「snapshot」。 赋予对象回滚状态的能力。 在不违背封装原则的前提下，捕获一个对象的内部状态，以便后续回滚状态使用。 类比：\nRedis中的RDB、AOF。是典型的高频增量、低频全量快照优化实现。 存储系统中的wal预写日志。 Observer 观察者 is a publish/subscribe pattern, which allows a number of observer objects to see an event. Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically.\n描述：\n一种发布订阅模式，实现了一组观察者对象，用于监听事件变化。 定义对象间的一对多关系，当前者对象状态发生变化时，后者对象组可自动收到通知。 结构、叫法：\n被观察者 Observable Subject Publisher Producer EventEmitter Dispatcher 观察者 Observer Observer Subscriber Consumer EventListener Listener 目的：解耦观察者、被观察者。\n复杂实现：Google EventBus、Spring event。\nState 状态 allows an object to alter its behavior when its internal state changes.\nStrategy 策略 allows one of a family of algorithms to be selected on-the-fly at runtime. Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it.\n定义：\n运行时选择具体执行的算法。 定义封装一组算法，彼此可替换。策略模式允许使用侧与算法的变化互相独立。 目的：解耦策略的定义、创建、使用。\nTemplate method 模板方法 defines the skeleton of an algorithm as an abstract class, allowing its subclasses to provide concrete behavior.\n定义： 通过抽象类的方式定义类骨架，通过子类实现具体算法。\n目的：\n骨架：复用结构。 子类实现：拓展逻辑。 示例： 定义骨架、规范：\nInputStream.read() AbstractList.addAll() 拓展：\njavax.servlet.http.HttpServlet service()中定义了根据不同header执行对应方法的骨架「定义了处理请求的流程」，而例如doPost/doGet都交给实现类去实现。 Junit在不使用注解的方式下，使用继承TestCase的方式编码UT 这种方式是比较老的设计，现在基本都用注解了，会比较灵活。 TestCase 中定义了UT单测跑起来的基本架子， runBare() 定义架子，runTest() 交给子类也就是具体业务来实现。 Visitor separates an algorithm from an object structure by moving the hierarchy of methods into one object.\n对比 设计模式最大的区分在于设计意图、应用场景。回到本文最开始的观点：重点是识别面临的问题的场景，识破问题关键。\n享元 vs 单例 vs 缓存 vs 池 设计意图 享元 复用对象「共享一个对象的同时使用权」、节省内存 单例、多例 限制对象个数 缓存 提高访问效率 池 重复使用「共享一个对象的非同时使用权」、节省时间 代理 vs 桥接 vs 装饰器 vs 适配器 设计意图 代理 控制访问 桥接 分离接口与实现，隔离变化 装饰器 增强原接口功能 适配器 事后补救，兼容新老接口 模板方法 vs 同步回调 vs 异步回调 应用场景 代码实现 优点 模板方法 定义骨架，定制逻辑 基于继承，子类重写父类方法 同步回调 定义骨架，定制逻辑 基于组合，传递回调对象 灵活 异步回调 与观察者模式一致 基于组合，传递回调对象 灵活 策略 vs 工厂 vs 命令 应用场景、设计意图 策略 vs 工厂 策略 vs 命令 策略 解决根据运行时状态从一组策略中选择不同策略的问题 包含策略的定义、创建、使用。结构与工厂类似 不同策略可互相替换，结果一致 工厂 封装对象创建过程 命令 控制命令的执行：异步、延迟、队列、回滚、日志 将函数封装成对象。 不同命令不可互相替换，结果不一致 中介 vs 观察者 应用场景 区别 中介 交互复杂，解耦对象间的交互 交互关系复杂，上下游流向可互相转换，如a-\u0026gt;b 转换为 b-\u0026gt;a 观察者 明确的一对多关系下，解耦观察者、被观察者 观察者被观察者关系明确，上下游流向固定不可变 Ref wiki/Software_design_pattern wiki/Design_Patterns wiki/Creational_pattern tutorialspoint.com/design_pattern ","permalink":"https://redolog.github.io/posts/rd/design/pattern/index/","summary":"\u003cp\u003e设计模式是前辈们针对实际编码中各种问题对应解决方案的抽象总结，是一种最佳实践。因此值得每一位工程师学习借鉴。\n使用的时候重点是\u003cem\u003e识别面临的问题的场景，识破问题关键\u003c/em\u003e，挑选适当的模式进行编码。\n\u003cstrong\u003e识别\u003c/strong\u003e这一步最为关键。\u003c/p\u003e","title":"设计模式一览"},{"content":"软件编程有多年工业界实践的经验沉淀，这些原则、法则是指导我们设计、编码的方法论。本文只针对OOD。\n我尽量一句话概括一项原则、法则：\nGRASP GRASP\nGeneral Responsibility Assignment Software Patterns (or Principles), abbreviated GRASP, is a set of \u0026ldquo;nine fundamental principles in object design and responsibility assignment\u0026rdquo;\n通用职责分配原则，缩写GRASP，包含九种OOP设计基础原则。\n信息隐藏 | 信息专家 | 封装 信息隐藏 | 信息专家 | 封装\nInformation hiding(Information expert) interchangeable with term Encapsulation\n将具备共同含义、变化频率的部分封装起来，并通过接口隔离实现，通过访问权限隔离访问。\n特性：高内聚、低耦合。\n创建者 | Creator | Factory 创建者 | Creator | Factory\nIn object-oriented programming (OOP), a factory is an object for creating other objects – formally a factory is a function or method that returns objects of a varying prototype or class[1] from some method call, which is assumed to be \u0026ldquo;new\u0026rdquo;.[a] More broadly, a subroutine that returns a \u0026ldquo;new\u0026rdquo; object may be referred to as a \u0026ldquo;factory\u0026rdquo;, as in factory method or factory function.\nOOP中，创建者、工厂，负责特定对象的创建。\nController | 非UI场景 The controller pattern assigns the responsibility of dealing with system events to a non-UI class that represents the overall system or a use case scenario. A controller object is a non-user interface object responsible for receiving or handling a system event.\nController负责接收、处理系统事件。与MVC中的C含义一致。\n相关模式：命令模式、门面模式、分层架构、纯虚构。\n转发 | Indirection The indirection pattern supports low coupling and reuses potential between two elements by assigning the responsibility of mediation between them to an intermediate object.\n转发模式：使用一个中间层转发对接前后的元素或层，解耦了前后元素，并且复用了已有能力，起分配作用。\n特性：低耦合、复用能力。\n低耦合 | Low coupling 低耦合 | Low coupling\nCoupling is a measure of how strongly one element is connected to, has knowledge of, or relies on other elements.\n以上为耦合的定义。而低耦合（松耦合）是一种评估效果的模式：\n类与类之间是否依赖是更少、更低一层的； 对一个类的修改是否对其他类有更少的影响； 有更多复用的可能性； 高内聚 | High cohesion 高内聚 | High cohesion\nHigh cohesion is an evaluative pattern that attempts to keep objects appropriately focused, manageable and understandable.\n与低耦合属于同类，均为评估效果的模式：\n对象职责是否明确、专注； 对象是否更好管理； 代码是否易懂； 多态 | Polymorphism 多态 | Polymorphism\nIn programming language theory and type theory, polymorphism is the provision of a single interface to entities of different types[1] or the use of a single symbol to represent multiple different types.[2] The concept is borrowed from a principle in biology where an organism or species can have many different forms or stages.\n受生物学启发，一个生物或者有机体可以有多种形态、阶段。\n同理：\n在编程语言理论中，多态通过单个接口，提供了多种类型实现。 在类型理论研究中，单一符号可以表示多种类型。 实现分类：\nAd hoc polymorphism 特定多态\n方法、函数、修饰符的重载。\nParametric polymorphism 参数类型多态\n泛型多态。泛型不指定实际调用时的具体类型，而是用通用符号进行函数定义。\nSubtyping 子类型多态\n定义时只指定公共父类符号。\n隔离变化 | 开闭原则 | Protected variations 隔离变化 | 开闭原则 | Protected variations\nThe protected variations pattern protects elements from the variations on other elements (objects, systems, subsystems) by wrapping the focus of instability with an interface and using polymorphism to create various implementations of this interface.\n识别预测可能的变化点，通过接口或者抽象隔离变化。\n纯虚构 | Pure fabrication 纯虚构 | Pure fabrication\nA pure fabrication is a class that does not represent a concept in the problem domain, specially made up to achieve low coupling, high cohesion, and the reuse potential thereof derived (when a solution presented by the information expert pattern does not). This kind of class is called a \u0026ldquo;service\u0026rdquo; in domain-driven design.\n虚构层、虚构对象没有对应的领域概念，是为了高内聚、低耦合、复用（内聚了信息专家没有的信息）创造出的代码表示。\nDDD、MVC中我们称虚构层为service。\n特性：高内聚、低耦合。\nSOLID SOLID\n单一职责原则 单一职责原则\nSingle-responsibility principle A class should have one, and only one, reason to change.\n一个类只应该负责一件事。当针对这个类需要变更时，只能由一个理由驱动。\n开闭原则 开闭原则\nOpen–closed principle You should be able to extend a classes behavior, without modifying it.\nOOP背景下，软件实体（类、模块、函数）对拓展开放，对修改封闭。\n当需要增加、修改功能时，不要修改源码（包括原来的二进制文件），通过拓展来完成变更。\n里式替换原则 里式替换原则\nLiskov substitution principle\nLiskov\u0026rsquo;s notion of a behavioural subtype defines a notion of substitutability for objects; that is, if S is a subtype of T, then objects of type T in a program may be replaced with objects of type S without altering any of the desirable properties of that program (e.g. correctness).\nFunctions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\nDerived classes must be substitutable for their base classes.\n父类实例可以在不修改任何类属性的前提下，替换为子类实例。\n接口隔离原则 接口隔离原则\nInterface segregation principle\nMany client-specific interfaces are better than one general-purpose interface.\n小的、精确的接口，比大的接口更好。通过约束接口来优化依赖关系。\n依赖倒置原则 依赖倒置原则\nDependency inversion principle\nDepend upon abstractions, [not] concretions.\n依赖接口，不依赖实现。\n组合优于继承 组合优于继承\nComposition over inheritance\n当我们想重用代码或者实现多态时，要优先使用组合的方式。\n迪米特法则 迪米特法则\nLaw of Demeter LOD\n又叫最少知识法则，面向解耦的目的，调用方对被调用方的内部结构、实现知道的越少越好。\n控制反转法则 控制反转法则\nIoC(Inversion of Control)\n通过框架控制对象创建、流程，用于实现拓展性、模块化。\nOOP下的实现方式：\nservice locator pattern 服务定位设计模式 使用抽象层封装了寻找真实业务处理者的逻辑 举例：JNDI dependency injection 依赖注入 Constructor Parameter Setter Interface Method 举例：Spring的依赖注入容器 contextualized lookup 上下文寻址 也叫 Contextualized Dependency Lookup (CDL) todo: Contextualized Dependency Lookup (CDL) is similar, in some respects, to Dependency Pull, but in CDL,lookup is performed against the container that is managing the resource, not from some central registry,and it is usually performed at some set point template method design pattern 模板方法设计模式 strategy design pattern 策略设计模式 KISS KISS, an acronym for keep it simple, stupid, is a design principle noted by the U.S. Navy in 1960.\nThe KISS principle states that most systems work best if they are kept simple rather than made complicated; therefore, simplicity should be a key goal in design, and unnecessary complexity should be avoided.\nKISS法则是说：我们的系统能够运行良好的一大原因是足够简单。因此，简单、简洁、避免不必要的复杂是我们设计的核心目标。\n变体：\nKeep it simple, silly keep it short and simple keep it short and sweet keep it simple and straightforward keep it small and simple keep it simple, soldier keep it simple, sailor keep it sweet and simple DRY \u0026ldquo;Don\u0026rsquo;t repeat yourself\u0026rdquo; (DRY) is a principle of software development aimed at reducing repetition of software patterns,[1] replacing it with abstractions or using data normalization to avoid redundancy.\nThe DRY principle is stated as \u0026ldquo;Every piece of knowledge must have a single, unambiguous, authoritative representation within a system\u0026rdquo;. The principle has been formulated by Andy Hunt and Dave Thomas in their book The Pragmatic Programmer. They apply it quite broadly to include \u0026ldquo;database schemas, test plans, the build system, even documentation\u0026rdquo;.\n同一系统内的同一知识应该有明确、权威可信的唯一表示。\nWET 反模式 write everything twice write every time waste everyone\u0026rsquo;s time\n反模式：无脑拷贝、重写代码。\nAHA AHA stands for \u0026ldquo;avoid hasty abstractions\u0026rdquo;, described by Kent C. Dodds as optimizing for change first, and avoiding premature optimization.[8] and was influenced by Sandi Metz\u0026rsquo;s \u0026ldquo;prefer duplication over the wrong abstraction\u0026rdquo;.\n避免草率的抽象。面对变更先做修改，避免过早优化。\nYAGNI You aren\u0026rsquo;t gonna need it\n\u0026ldquo;You aren\u0026rsquo;t gonna need it\u0026rdquo;(YAGNI) is a principle which arose from extreme programming (XP) that states a programmer should not add functionality until deemed necessary. XP co-founder Ron Jeffries has written: \u0026ldquo;Always implement things when you actually need them, never when you just foresee that you need them.\u0026rdquo;\n只有在真正需要的时候再去实现新的逻辑，尽可能复用已有代码。\nDocument Your Code Any senior developer will stress the importance of documenting your code with proper comments. All languages offer them; you should make it a habit to write them. Leave comments to explain objects, enhance variable definitions, and make functions easier to understand.\n注释、文档与代码同样重要。使用注释解释对象、变量含义，同时使得函数更易读。\n包管理法则 前三项关注包内聚，即包内应该放什么。 后三项关注包解耦，即关注包与包之间的关系、结构。\n发布等于重用 REP The Release Reuse Equivalency Principle The granule of reuse is the granule of release.\n发布的粒度就是重用的粒度。\n封装变更 CCP The Common Closure Principle Classes that change together are packaged together.\n一起变更的类应该放在同一个包下。即关注类变更的频率、规律。使用包封装这种变化。\n封装重用 CRP The Common Reuse Principle Classes that are used together are packaged together.\n一起使用的类应该放在同一个包下。即关注类被使用的场景。\n没有循环依赖 ADP The Acyclic Dependencies Principle The dependency graph of packages must have no cycles.\n包与包之间的依赖禁止出现环。\n面向稳定的依赖 SDP The Stable Dependencies Principle Depend in the direction of stability.\n保证依赖都是稳定的。\n面向抽象的依赖 SAP The Stable Abstractions Principle Abstractness increases with stability.\n依赖抽象，提高依赖的稳定性「变化的往往是实现」。\nRef UncleBob.PrinciplesOfOod ChenYiming-面向对象编程之我见 ","permalink":"https://redolog.github.io/posts/rd/design/principle/oop/","summary":"\u003cp\u003e软件编程有多年工业界实践的经验沉淀，这些原则、法则是指导我们设计、编码的方法论。本文只针对\u003ccode\u003eOOD\u003c/code\u003e。\u003c/p\u003e","title":"OOP编程原则一览"},{"content":"Latency numbers every programmer should know.\n每位程序员应该知道的延迟数字。\n交互式界面： https://colin-scott.github.io/personal_website/research/interactive_latency.html\n黄岩微博，中文形象版： 数字版： ","permalink":"https://redolog.github.io/posts/rd/linux/data/latency/","summary":"\u003cp\u003eLatency numbers every programmer should know.\u003c/p\u003e","title":"Latency numbers every programmer should know"},{"content":"很多技术概念都是对现实的映射、类比、模拟。\n背景 我对很多技术的理解都是基于现实中的某个概念、实体、过程，因此想把这些记录下来。\n人类思考、记忆 CPU-大脑 想想人脑的思考过程，大脑相当于计算机的CPU。\n内存buffer-大脑空间 而求解一个问题时我们需要「想一想」，这个过程中产生的临时变量存放的地方则是我们的大脑空间。\n计算结果-脑突触 当一个问题有结果时，我们的大脑中产生对应的一个新的突触。\n磁盘-永久突触 随着类似逻辑运算次数叠加，我们的结果进行了缓存，形成了大脑中永久长成的突触，下次提取结果时无需重新计算，比如「1+1=2」是大部分上过小学的人都有的突触例子之一。\n耗时较长的任务执行（如定时长任务）-暗时间记忆 业务形态为：耗时较长的任务，比如一些业务补偿、延迟执行的定时任务，我们一般放在业务低峰期执行。\n而一些白天记忆的东西，计算量、记忆量可能较大，适合我们晚上复习或者睡觉时让大脑自动帮我们跑比较久去复习（利用好暗时间）。\n人类行动 提前读-预判了你的预判 通俗来说，cache是编程中一种「提前读」的通用实现。\n而打羽毛球的时候，牛逼的运动员可以根据自己将要打的球以及对手动作、习惯做出预判，提前做好接球的准备。整个过程反应极快，将预判动作训练为自己的身体反应（这是一种缓存），所以速度快。\n延迟写-脏衣篓里攒了三天的衣服 通俗来说，buffer是编程中一种「延迟写」的通用实现。\n使用内存buffer来缓冲后面开销较大的任务的冲击，比如写磁盘开销比内存操作更大（重IO）。\n而在洗衣服的场景里，我家里的脏衣篓一般会攒三天的衣服，三四天再跑去洗一次。利用脏衣篓（buffer）缓冲了洗衣任务，而洗衣服对我来说相对开销较大（懒），开销大体现在一方面有心理负担，一方面合租时候去洗的次数多容易与室友冲突。\n同时一批任务量也得到了归集内聚，也是一种批量的思路。\n某个任务背景下的铺垫-数据预热 此类比也可以描述为：前戏等同于资源初始化。\n在真正的业务处理前，我们的系统很有可能需要提前准备一些资源，一般是在PostConstruct或者进程启动阶段执行，就算是进程内部，比如JVM启动也分了好几个阶段：编译、加载、链接、初始化。\n如果将初始化、预热的动作延后到真正的任务处理时，那在实现层面就会很冗余、耦合，效率也会降低。比如开会不提前发背景、安排会议室，等到临时才安排，大家就会感受很被动。\n所以该预热的一定要提前安排。\n主从高可用-公司家两台电脑 19年的时候我买了第二台mbp放家里。在这些情况下比只有一台电脑可用性要高：\n公司的电脑坏了，19年的时候真实发生过； 突然通知要居家办公； 下了班需要oncall； 既然是主从，一定存在数据同步、一致性的问题。这块我主要依赖：\n云文档； 可以保证数据变更的实时性，飞书文档帮我保证数据读取的一致性； iCloud； 无法保证实时性的一致，不过由于物理上公司与家有一定距离，并且回到家开启电脑联网也有一定间隔，基本在我需要的时候数据已经从云端下载到了本地目录； Git仓库； 需要手动提交数据，是典型的分布式数据读取模型； 其中前两项同时保证了Mac与iPhone端的数据一致。\n衣物收纳 衣服对于我们来说，相当于信息之于计算机程序。\n排序-收纳 收纳：规整、分类衣物。\n分盒分类：Hash分桶，按照分类查找时复杂度O(1)。\n夏季将冬季衣物收进箱子：链表式。此时重点在于将暂时不用的衣物依次放进大箱子，关注收拾的速度，舍弃查找的速度（需要的时候通常全量拿出）。\n吃饭 每个人都需要吃饭，而在食堂、饭店吃饭时，我发现对我理解IO、线程池帮助很大。\nIO 数据准备； 数据复制； 阻塞、非阻塞-食堂排队时堵不堵？ 例如食堂高峰期吃饭时，餐线排队的人很多，但是由于大家打饭、结账时基本是缓慢执行的，所以此时排队并不堵，我们的餐线（IO过程）其实是不堵塞的。\n但如果有人结账或者打汤（小米食堂一般汤在最后面）时停留了比较久，比如结账卡找不到了，或者打汤洒了，那么此时餐线也就阻塞了。\n对应我们的IO阶段，也就是数据准备、复制阶段任一阶段是否发生阻塞。\n线程池-托盘回收传送带 固定长度的托盘回收传送带：线程池队列，带有界capacity。现实中不太可能产生OOM，因为物理资源明显是有限的。\n餐盘：线程池任务。 向传送带放一个餐盘：submit一个线程池任务到池。 corePoolSize：传送带背后固定的处理人数。 maximumPoolSize-corePoolSize：传送带背后流动的处理人数。或者更形象点，一个团队中的外包人数（残酷但是现实：项目用人需求变少时就裁掉外包）。 同步、异步-食堂或者高级饭店 在食堂，一般经过排队之后，我们的饭需要自己端着盘子取、送。这就是同步的，因为食物需要自己取到餐桌上。\n而在高级点的饭店（非美食广场），我们点完餐之后（告诉服务端我们要什么数据，你去准备吧！），之后就可以开始玩手机，食物准备好后，服务员会帮我们把食物端到餐桌上（服务端给我们主动发送准备好的数据、拷贝）。这就是异步的，因为当前线程（点菜、吃菜的我）不需要自己执行数据的准备、复制。\nIO多路复用-饭店迎宾员 还是在高级饭店的场景下，饭店会配备迎宾员、服务员、厨师，工作专业度强度依次提高。\n假设一个200平的饭店，一般情况下一名迎宾员就够了。因为他的任务足够轻快，使用单线程足矣完成。这就是IO多路复用，多路对应n桌客人，复用对应单个迎宾员。\nIO线程池计算型线程池分离-岗位分离 迎宾员、服务员、厨师使用不同的人员规模管理、工资，类似于不同业务线程池分离。\n迎宾员任务只配备一个线程，而服务员需要配备三个线程，而厨师需要配备五个线程。\n人类策略 服务端的大部分接口，其实就是一个func select(request) (response)函数，很多时候所谓的逻辑其实是各种业务策略。\n从接收到入参的时候，我们的接口做两件事：\n校验，看入参是否有效。 处理真正的业务逻辑。 fail-fast-快速跳过不需要的信息 在select()阶段一，RD首先要做的是：无效数据直接返回，不进入后续逻辑。 这样节省资源，也无需在后续代码中处理（clean code）。\n如同我在支撑000217中所说：人生，也是关乎各种选择。\n而快速失败是非常实用的选择策略：\n人生是一场无限游戏，快速尝试快速MVP长远来看可以帮到我们很多； 选择投资标的时，有时候需要根据「不能接受的点」进行选择，即fail-fast，通过这项校验的标的才有后续深入了解的必要； 二分法算不算一种fail-fast？\n时间换空间-通勤住得远 很多时候时间换空间是因为我们空间资源不足。\n假设我们有一个对实时性要求不高的业务，比如后台导出一份千万数据的报表，一般服务端配置的内存有限，一次查询数据会有IO、程序内存等空间瓶颈。\n这种情况下，我们需要选择节省空间（提高空间利用效率）的方案。一般，我会分批处理，比如一批一万条数据，内存可控，并且业务有可控预期（只需要执行一千次）。这就是典型的时间换空间的策略。\n人在穷的时候，往往需要使用「时间换空间」策略。比如通勤，选择住比较远的地方，通勤时间长，但是住的大、房租低。居住空间与房租是这种场景下的空间。\n反之则是「空间换时间」策略，比如住得近通勤时间短，但是居住空间小。\n时间换空间-食堂没座位 食堂打完饭，但是没有座位，怎么办？只能等，或者拿着托盘转悠一会儿。\n组织管理安排 分组\n本质上是为了内聚资源，提高资源利用率，并且起到一定的资源隔离作用。\n组织分组对应到技术可能对应这些概念：\n池化pooling 分区partition 线程池-人员分组 一般在一个公司内我们会有部门、组，把一个人放在特定的业务方向、组织节点上，而人力外包则一般是把员工当做流动劳动力，哪里需要分派到哪里。\n本质上人员分组是一种「线程池」的映射（vice versa）。一组线程池我们起了十个线程，这十个线程就负责这块业务。同时项目需求紧张的时候可以增派外包，这是我们上面提过的maximumPoolSize-corePoolSize。\n不同的组则对应我们不同的线程池，组与组之间隔离，而不同的线程池也是为了资源隔离。\n而人力外包的模式下，单个线程职责不够专注，生命周期内比较难发挥大的价值。所以做人要做重要线程池中的线程，不要做临时new出来的外包单线程。\n交通管理、设计 限流-地铁限流、预约服务 地铁在高峰期都会执行限流政策，比如：\n广州八号线的万胜围作为枢纽中转站，一般在周五下班时间会使用护栏限制走道宽度； 核酸检测、个税退税都需要预约（可以指定单日只放一万个号）； 限制宽度、服务号码预约中心思想是一致的：给服务端（地铁班次、核检网点）一个稳定、有限范围的预期。\n限流调度-信号量（红绿灯）、令牌桶、时间窗口 信号量（红绿灯）、令牌桶、时间窗口这些设计本质上是一致的：针对同一资源进行同步化访问控制。\n在十字路口，通行权就是同步访问权限。而服务端流量执行业务逻辑，则是对应的同步访问权。\n红绿灯+一定路宽在时间、通过量上做了限制，一个通行时间窗口 * 路口单位时间通行量 = 程序中Semaphore的permits量。\n排队 AQS队列-无红绿灯式环岛 AQS队列与显式加锁相比，减少了锁开销。本质上与无红绿灯式环岛减少交通冲突逻辑一致。\nAQS队列通过排队固定内部的通行速率，这一点在环岛上现实体现为进入环岛的车辆车速较低。\n非公平排队策略-插队 以清河地铁站往小米科技园步行的一段路为例，最开始有一个铁门只开了单次通行一个人的空隙。所以大家出了地铁默认都会排队。（不排队大家谁也别想过去！）\n但是总是有人插队的，这一点是我在广州、北京生活多年的观察结论。\n插队就是非公平锁获取的逻辑。对应ReentrantLock.Sync默认为NonFair。\n对于排队的人（某个节点）来说，插队可以提高自己穿行的效率（提高了吞吐throughput），但是对于整体队列来说肯定是有人受损的（被插队的节点往后都比原先慢了一个通过时间）。\n插队提高了单节点的吞吐，但是降低了满载队列通过的公平性（没有什么岁月静好，有的是后面的人给你负重前行了）。\n死锁-十字路口你不让我我不让你 以望京地区大山子某个十字路口为例，这个路口有点奇葩，红绿灯同时支持对面车直行+本侧车左转。\n碰到严重的时候，这里基本会堵死，因为左转与对侧直行车辆互不相让（owner权限无法正常流转）。\n死锁发生后，一般需要系统层面的介入，比如交管同志来介入协调（与数据库死锁后运维同学介入同理）。\n人类学习 术 VS 道 具体的技术实现-术 比如技术栈、框架、库、写法，是术。\n对应练武功时候的招式。招式需要练习、强化。\n原理、思路、方法论-道 比如理论原理：\n操作系统 网络 数据结构 思路：\n系统设计思路 优化思路 方法论：\n设计模式 管理模式 最佳实践 是道。\n对应练武功时候的心法。心法需要先学、后悟。是从低阶晋升高阶的底层支撑。\n","permalink":"https://redolog.github.io/posts/rd/mapping/reality/","summary":"\u003cp\u003e很多技术概念都是对现实的映射、类比、模拟。\u003c/p\u003e","title":"技术概念与现实的映射"},{"content":"作为RD，除了搞好技术方案、实现，也要搞好项目管理。 大部分的任务推进都是以项目为维度的，而其中的项目管理对于结果成功至关重要。 如何做好DPM（Developer Project Manager）？我认为首先要有owner心态，即我是这个项目的负责人心态，其次是要有owner配套的方法论、能力。\n背景 正统的项目管理可以学习PMP课程，但是结合我几年的工作经验，实际中我们并不需要那么完整、繁琐的流程、工具。\n我们需要的工具，只是一份简单的模板即可，关注关键点，解决关键问题即可推进进度。\n要点：\n这个项目做什么？【需求范围】 什么时候要？【关键时间点】 谁来负责？谁来参与？【干系人】 出现疑问、异常如何解决？谁来推动？【问题处理】 关键节点有无风险？【资源拉通】 实操基本原则 可根据对应的需求、项目做调整、内容丰富； 创建工单前，一定要确认需求，明确初期疑问点，资源要对齐（如多端）； 及时做好各方各端各个干系人的信息同步； 模板正文 以下是我自用的PM文档模板：\n需求背景 事 描述项目的需求背景和目标。 关联相关资料，如产品文档、技术重构文档。 卡点 关键时间点。 干系人。 设计部分 业务流程 形式：文字+流程图。 梳理、表达业务流程，如： https://www.processon.com/view/link/5f8cfe265653bb06ef068e34\n架构设计 形式：文字+架构关系图。 梳理、表达架构关系（模块、系统、层级结构）、其他领域划分设计、UML图、ER图等，如：\n架构图： 实现部分 使用的技术组件 存储选型 MySQL MQ ES Redis 框架选型 web框架 IO框架 定时任务 调度 关键技术 接口 关键对接信息，接口维护建议使用api管理系统。\n协议（http/dubbo） 接口名称 能力作用 入参 出参 mock示例 对接交付时间点 资源部分 资源申请 平台资源（数据库、缓存、队列） 量级预估、评审，扩容预估 依赖资源 接口依赖（是否需要新开发？排期资源是否OK？） 相关端是否对齐拉通？ 预案 风控 提前预估可能出现的风险，提前确认、对齐资源与方案。\n回滚 出现问题提前准备回滚预案。\n上线 checklist：\n代码改动（合并、打包） 业务数据配置 定时任务配置 依赖方资源 数据存储schema改动 对接外部是否需要注意ip黑白名单 etc 会议部分 会议记录 2022年x月xx日 xxx功能需要依赖第三方xxx\u0026hellip; xxx功能由服务端给出xxx数据，5端前端配合展示逻辑和实现交互逻辑 xxx功能避免耗费服务端带宽，前端做好相应本地缓存\u0026hellip; xxx功能存在风险点，需要看具体情况而定 ","permalink":"https://redolog.github.io/posts/rd/pm/doc/","summary":"\u003cp\u003e作为RD，除了搞好技术方案、实现，也要搞好项目管理。\n大部分的任务推进都是以项目为维度的，而其中的项目管理对于结果成功至关重要。\n如何做好DPM（Developer Project Manager）？我认为首先要有owner心态，即我是这个项目的负责人心态，其次是要有owner配套的方法论、能力。\u003c/p\u003e","title":"我自用的项目管理文档模板"},{"content":" Jay Kreps (Confluent CEO，Kafka 核心作者) 在《The Log: What every software engineer should know about real-time data\u0026rsquo;s unifying abstraction》中系统性描述了日志的价值和重要性，指出了日志特定的应用目标：它记录了什么时间发生了什么事情（they record what happened and when）。而这，正是分布式系统许多问题的核心。\n背景 年前无意间阅读公司MQ团队的一篇博文 消息队列价值思考，其中提到了本篇译作的原文，有种相见恨晚的感觉。原文讲解日志对于存储系统的关系、重要性，整个思路、思考对于我们做应用设计有非常大的借鉴价值，大致读完，发现这种文章与DDIA书籍都应该在本科学习数据库的阶段就学习掌握。因此即使网上已有公开的几篇翻译，但由于其重要性，特在自己的博客上记录、翻译一遍。 原文写于2013年底。 原文作者 Jay KrepsDecember ，以下我们简称为JK。 Ref 英文原文：Jay KrepsDecember 16, 2013: The Log: What every software engineer should know about real-time data\u0026rsquo;s unifying abstraction。 李鼎 oldratlee：OSChina译作。 foreach_break：学习笔记：The Log（我所读过的最好的一篇分布式技术文章） 译文 P0：背景 JK大佬在2013年的六年前加入了领英。彼时为何有趣？是因为当时 LinkedIn 正准备重构一个单体巨石并且是集中式的数据库，目标是转换为一系列定制的分布式系统。最后这段经历成功且有趣：JK以及 LinkedIn 构建、部署了分布式的图数据库、分布式的搜索后端、一个Hadoop集群，并且还有第一代第二代的KV存储。\n整个过程学到的一个关键点：日志是系统设计、构建的核心。log日志还有一些别名：\n预写日志 write-ahead logs 提交日志 commit logs 事务日志 transaction logs log日志，自在线系统、分布式系统存在起，就已经伴随我们左右。\n重要性 如果不懂log日志，就无法掌握：\nRDBMS NoSQL KV存储 replication 复制同步 paxos 一致性 hadoop VersionControl 版本控制 几乎所有软件系统 本文，JK会带领大家一起了解log日志的所有知识点。\nP1：什么是log日志？ log日志是最简单的存储抽象。特点：\n写入：只追加（AppendOnly）； 读取：从左往右； 全局有序； 每一行为一个entry，每一个entry记录一个唯一的、有序的编码（与时间对应）； 由以上 3 4 可得，越靠左的entry写入时间越早，因此第4点的编码与时间对应由此而来。以写入顺序来映射时间看起来有点奇怪，但是到后面我们研究分布式系统时，这项特性价值会越发明显。\n日志记录的内容、格式在本文来说不太重要。同时注意，我们不能一直追加记录到文件中，因为空间资源是有限的。这一点后面会聊到。\n日志与表、文件区别不大。表的本质是一组数据记录，而文件的本质是字节集合。日志则是记录按时间排序的表或者文件。\n这么简单的东西，有必要写一篇文章来讲解吗？一个AppendOnly的有序记录文件与数据系统有何关系？\n答案是，log日志与分布式系统有一点核心是一致的，他们解决的都是：记录了何时发生了什么（they record what happened and when）。\n有一点内容需要澄清强调：本文所说的日志，与应用日志不是一回事。\n应用日志一种非结构化的错误信息、追踪trace信息，一般使用Syslog、Log4j输出到本地文件。 应用日志是本文所指log日志的退化表现。 应用日志一般叫text log，主要用作程序员阅读、排错。而本文所指的log一般叫journal | data logs，一般用作程序解析。 大致想想，应用日志其实是一种时代错误（anachronism），当服务变多系统变复杂后，应用日志逐渐变得失控。而基于这种非结构化的日志想做一些解析的工作真是难上加难。\nP1.1：数据库中的日志 日志概念过于简单，简单到我们都不好叫它为一种发明，其最早跟随 IBM System R 出现。最开始日志在数据库中用于崩溃时保证不同数据结构、索引的同步。为了保证原子性、持久性，数据库在应用变更前会先记录要改变的信息到日志中。\n而表、索引则是经过设计的一种数据结构的映射。\n随着时间发展，log日志最开始是一种ACID的实现，变为了数据库多节点间复制的实现。到这里，日志中记录的序列号成为了保持多节点同步的重要依据。\nOracle, MySQL, PostgreSQL 都有对应的主从同步的传输协议。 比如Oracle的XStreams and GoldenGate 为非Oracle的系统提供订阅机制。MySQL, PostgreSQL类似。\n由于这种根源，最开始基于日志订阅的机制只局限在RDBMS中，巧合下才兴起。基于日志的订阅这种抽象非常适合用于：\n消息服务； 数据流； 实时数据处理； P1.2：分布式系统中的日志 日志解决了分布式系统中的两个核心问题：\n数据有序变更（对应数据更新时可对顺序达成一致）； 分发数据（对应拷贝数据时就修改的结果可达成一致）； 分布式系统中基于日志的处理方式源于状态机复制原理： 如果两个确定的进程，从相同的状态开始，并且以相同顺序进行了相同的操作，其输出一致，多个进程的最终状态一致。\nDeterministic 确定性 意思是处理过程与时间无关、不受其他外部因素影响。 举几个反例：\n多线程执行顺序不同，结果不同； 调用时间相关函数：gettimeofday； 不可重复的操作（非幂等接口重复调用）； 这些都是不确定的。 而进程的状态指的是，当操作结束后，留在机器上的任意数据（内存或者磁盘均可）。\n我们上面讲了，日志可以保证相同的顺序+相同的输入=相同的输出。这个要成为我们的直觉（启发式、公理）： 相同日志输入+确定性的处理代码逻辑=相同的输出。\n再进一步，如何应用到分布式计算的场景中？我们可以进行归约（reduce）： 转换问题 多个机器做同样的事 为 实现分布式持久化日志，以供分布式进程消费。\n此处日志作为统一的输入，将不确定性排除的同时，保证所有订阅了日志的副本（replica）消费都是同步的。\n理解了这里，也就理解了分布式系统设计中最通用的一个准则：确定性的操作带来确定性的结果。\n这里比较奇妙的一点在于，log日志记录的时间戳序号，现在可以表示每个节点的状态。每个节点记录当前已处理的最大项的时间戳，时间戳加上日志唯一确定节点完整的状态。\n根据日志记录内容的不同，我们可以有不同的原理应用方式。比如，我们可记录服务到访的请求、服务处理请求之后的状态变化、执行过的命令，甚至可以记录一系列机器指令、方法名、参数。只要两个进程以同样的方式处理同样的入参，多个副本间的结果是一定一致的。\n不同群体对日志有不同的叫法。搞数据库的一般把日志分为物理日志+逻辑日志。\n物理日志（physical logging）：记录每个改变的行。 逻辑日志（logical logging）：记录DML语句。 分布式系统一般有两种数据处理、复制的方案：\n状态机模式（state machine model == active-active model）：请求会写入一份日志，然后每个副本处理同步； 主备模式（primary-backup model == active-passive model）：选举一个副本为主节点（leader），主节点按请求到达顺序处理写请求，之后通过日志广播变更，然后其他副本执行同步追赶进度。如果主节点遭遇故障进入选举流程。 举例说明：假如现在服务端依次执行指令：+1、*2：\n状态机模式：每个副本各自执行两条执行； 主备模式：主节点执行+1、*2，日志中记录结果，副本节点依次同步变更； 所以很明显，如果顺序不同，结果也会不同。\n分布式日志可以视为共识问题（consensus）的一种建模后的数据结构。 日志：表示针对下一个值是什么的一系列决策。参考Paxos家族的算法，使用日志是最实用的手段。Paxos一般用一种叫做multi-paxos的协议来对一系列共识问题建模，日志中的每个槽（slot）都这样来处理。 同类型的协议还有：\nZAB RAFT Viewstamped Replication 这些协议都研究的是如何维护一个分布式一致的日志。 关于历史理论的发展JK有一些质疑：也许分布式计算的理论研究有些超过了现实的应用。现实中共识问题往往过于简单了。计算机系统很少只用去处理单个值，一般都需要处理一系列的请求。所以log日志，不单单是一个单个简单值登记册，而更是一种自然的抽象。\n更进一步的考虑，JK认为过度关注算法以至于我们忽视了分布式系统底层的日志抽象。未来，日志可能更有可能作为一种通用的接口而不是具体的实现被我们所使用。就像hashtable一样，我们一般使用get、put，很少去关心底层线性表、hash的具体实现。\nP1.3：Changelog101：表与事件是对偶的 回到数据库，一个带有变更的日志与表是对称的。 日志类似于一个拥有借款、赊账的银行，而数据库则表示银行卡余额。\n正向：如果我们有变更记录日志，就可以以此执行变更，最终得到一个table表。表记录的是一段时间内（日志处理时间段）某个key对应的最终状态。 逆向：如果表发生了更新，我们可以记录变更，并且发布到changelog中。changelog是我们支持近实时（near-real-time）复制的原材料！ 综上，我们的表与事件是二元对称的：\n表维护数据； 日志记录变更； 如果日志记录的是完整变更，那根据日志不仅可以还原最新版本的数据，也可以回退到任一时刻的状态。所以日志是表所有状态的备份依据。 听起来日志跟我们平时使用的代码版本控制类似了。版本控制解决的只是分布式系统解决的一部分问题：管理分布式、并发的修改。\n版本控制系统说白了是将版本记录建模为一系列有序的补丁（models the sequence of patches），这其实就是log日志。而每次我们本地检出的代码快照（checked out \u0026ldquo;snapshot\u0026rdquo; ），其实就是表的概念。分布式系统中发生变更时，副本节点需要执行复制（replicate），而在版本控制中发生变更后，我们需要先拉取补丁（拉取日志）然后应用变更到当前快照，这其实就是replication的过程。\n工业界的一个实例是 Datomic ，这家公司销售一种基于日志的数据库。理念与我们上面表达的一模一样。\n目前为止，我们讲的基本都是理论，别急，马上要进入实战环节了！\nP1.4：接下来是啥？ 本文接下来的部分，JK主要想带领大家一起过一下分布式计算的核心原理，以及分布式计算的抽象模型。分成如下几个部分：\n数据集成：旨在让所有系统（存储或者计算）都能很方便地访问任一系统的数据； 实时数据处理：计算派生数据流； 分布式系统设计：工业界如何把分布式系统的核心简化为log日志； 每个部分中，使用日志的过程无非是在演绎这句话：生成一个持久化的、可重放的历史记录。而分布式多机背景下，无非就是多台机器各自按确定性的方式、一定的速率来执行这个重放日志。\nP2：数据集成 本节结构：\n探讨什么是数据集成； 数据集成的重要性； 如何关联到log日志； 数据集成的目标是使得我们内部所有的数据在任意一台服务与系统都可以被访问到。\n数据集成这个词不太常见，以我们常见的ETL为例，作为数据集成的子集，ETL落地为了一种关系型数据仓库。如果ETL抽象的足够通用，就可以覆盖实时系统、流处理的场景。\n搞大数据的一般没怎么讲过数据集成，但是在「让我们的数据可用」这件事上，大家的关注点是一致的。\n工业界对于数据的使用符合马斯洛需求理论。\n首先最基础的需求是抓取数据，放入一个可用的处理环境中（可以是实时查询系统、文本、python脚本）； 其次是将数据建模抽象为易读易处理的格式； 最后才是考虑如何在存储组件之上如何处理数据（MapReduce、实时系统）； 所以数据平台建设的正确步骤应该是：\n抓取数据，搭建处理平台； 抽象数据建模，提供更容易理解的语义； 更复杂的处理：可视化、报表、算法处理、数据预测； 现实中，大家往往是背其道而行。所以，如何建设可靠的数据流系统，是最重要的事！\nP2.1：数据集成的两个复杂点 行业发展带来了两个有复杂度的场景：\n事件流数据 定制数据系统 P2.1.1 事件流数据 简单来讲就是各种记录性质的埋点数据、监控数据（事件、指标）。即trace、log、metric。\n科技公司由于信息化做得充分，所以这类基础搞起来趁手，而传统一点的公司也有很多这方面的应用，并且随着信息化发展，事件型数据会越来越多。\n相比业务数据，监控埋点数据量级高出不少，所有有其处理难度。\nP2.1.2 定制数据系统 第二类数据集成的难点场景则是定制的各类数据系统，比如我们需要为团队单独定制OLAP、搜索、批处理、图分析各类系统。\n多种系统共存数据带来了挑战。\nP2.2：结构化日志 数据流 日志是处理多系统间公用数据的自然抽象。使用方式很简单：将日志作为多个系统实时订阅的中心化存储。\n如上图，每个逻辑上的数据源都能抽象出自己的日志模块。数据源本质上就是为了记录事件（events-clicks-pv），而数据库表是接收变更结果。下游的每个订阅系统从日志中尽快读取数据，将每一行新记录写到自己的存储中，并且向前挪动消费位置（offset）。这里的订阅者（消费者）可以是任意一种系统：缓存、hadoop、数据、搜索系统。\nP2.2.1 逻辑时钟 上例中，日志作为一种逻辑时钟，订阅者根据这个可以算出自己的消费进度（状态、位置），offset表达了自己在消费维度上的时间进度。\n还是举个实际例子来辅助理解： 假设我们有一个数据库+若干个缓存系统。使用日志可以帮助我们进行数据同步以及推出每个缓存系统处理的时间。 首先我们写入一个X值，接着需要从缓存中读到该值。如果需要保证使用方不能读到过期数据，只需要保证不从「还未消费到X值位置的缓存节点」读数据即可。\nP2.2.2 buffer缓冲 log日志还能做数据消费的缓冲buffer。 特别是同一个系统中有多个不同的消费者，并且大家消费速率不一致时，这种设计非常好用。 好处：\n订阅系统可以宕机、挂机维护，等重启时，消费过程可以继续； 订阅者以自己的节奏进行消费； 批处理系统比如hadoop或者数仓可以一小时或者一天消费一次，而实时系统则需要在秒级别消费； 消费者只需要与日志系统交互； 消费者可以自由上下线而不影响上下游； 消费者不需要关心数据存在哪里，只需要跟日志保持一致的通信协议； Count Leo Tolstoy ：每个好的数据系统看起来都像是日志的架构；而坏的数据系统则各有各的样子。\n上面我们使用了日志log的订阅，而不是消息系统的发布订阅（pub sub），原因在于消息系统表达的与日志并不完全一致，我们可以把日志当做可以保障持久性以及强有序的消息系统。在分布式理论中，我们称之为原子广播（atomic broadcast）。\n到此，JK还是得强调，日志仅仅是基础设施。距离我们掌握数据系统还有一段路要走，接下来，我们会关注：\nmetadata 元数据； shemas 数据格式； compatibility 适配性； 以及处理数据结构的细节以及演进过程； P2.2.3 在LinkedIn领英公司的实践 回忆下我们的背景部分，JK加入公司后，经历了一系列从单体到分布式的架构重构。\n当前领英核心的系统包括：\nSearch Social Graph Voldemort Espresso Recommendation engine OLAP query engine Hadoop Terradata Ingraphs：监控系统 每一个都是特定场景下的定制系统。从JK来到领英，以日志为核心的数据流系统设计思想就贯穿始终，其中我们有一个最基础的设施服务叫做 databus ，它就是我们的日志抽象实现，最开始用于广播Oracle表变更，以满足下游数据消费的需求。\n最初的背景：JK最开始在2008年的时候参与实现一套KV存储，而下一个任务是要把我们的一些推荐系统移植到hadoop体系中。 由于经验不足，在这个项目上踩了一些坑：\n最开始计划是直接从Oracle导出数据，但是发现： 这个导出太恶心了； 报表处理时间比较久（出错就GG）； 处理过程不可逆； 所以后来抛弃了从Oracle直接导数的做法，我们直接以日志为数据源，并导入到第一代KV存储 Voldemort 中 ； 在这种导入导出的工作中，工程师的时间大大被浪费，而且中间一旦出错，你就得想办法重新处理； 尽管我们搞出了这么一套通用的方案，但是每一个数据源都需要单独配置，这个工作量不小，而且容易出错。接入方越来越多，每个系统都想要接入各种各样的系统来满足各种奇怪的需求； 到此，我们来分析一下，问题到底出在哪里？\n我们构建的管道流系统，有点乱，但很有价值； 以前难接入的计算现在变易用了； 很多新产品、分析工作，只需要接入新的数据源即可； 数据管理更强有力的支持可以帮忙我们构建可靠的数据。如果我们支持了所有的数据结构，Hadoop就可以全自动加载数据，无需人工介入。及时schema变更也可以自动跟进。 目前覆盖的数据场景还不全。以领英为例，数据场景太多了，想要全部支持难度也不小。 知道了问题，我们结合一下现状。按照我们为每个数据源与消费者定制的思路，其实是可行的，如下图。 这种做法的问题在于：为每个项目都得定制一个数据生产者与消费者。整体实施的复杂度是O(n^2)。\n将上面这种架构进行优化： 这样就是一个O(n)的解决方案。核心思想： 将生产与消费者隔离，通过中间的Unified Log做一层抽象，单点服务只需关注如何做集成，而无需关注对接方如何处理数据。\n说白了，Kafka就是这样诞生的！彼时亚马逊也有类似于Kafka的产品：Kinesis。架构类似：\n作为一个管道服务，连接其他的分布式系统（都跑在EC2机器上） DynamoDB RedShift S3 P2.2.4 ETL与数仓的关系 关于数仓，其核心概念是作为一种干净可集成的数据结构，用于业务分析。 关键流程：\n定期从源数据库提取数据 E Extract 把转换成一种易用的格式 T Transform 将上一步的产出加载到统一的中心化数仓 L Load 综上，数仓是组织内宝贵的数据资产持有者。数据价值可用于业务分析、处理。\n数仓比较适用于一些报表、整合指标（counting、aggregation、filtering），但团队仅仅只有数仓不太好满足一些实时业务、搜索、监控等需求。\nJK觉得ETL关乎两个关键：\n数据提取、清晰，将数据从特定系统解绑抽离； 为查询、使用重构数据（比如为了更好地适应RDBMS，需将数据转为星型、雪花型schema，而为了更好适应聚合，需将数据转为列式存储）； 但是耦合这两步是个问题。我们系统中的数据应该能适应各种实时、报表、搜索场景。 很多时候数仓团队自己去处理清洗数据，但理解业务的往往是业务团队。\n目标没有对齐； 业务不够熟悉； 需求响应不够快 导致的结果就是数据覆盖不够全面、数据问题多、需求响应慢。 更好的方式是：\n数仓团队只管理中心化的日志系统（同时配备操作数据的接口）； 而数据的生产方去考虑如何转换、清洗数据格式； 消费方通过接口拉取日志中的统一数据； 团队中考虑数据可拓展性是非常常见的：\n有时候我们需要在已有系统上增加搜索功能，比如全文检索； 有时候需要增加实时监控、告警； 这些需求在传统数仓、Hadoop集群环境下并不好支持，而ETL技术栈更无能为力。但是如果有了上图这种系统：有完善统一的数据结构、并且支持数据访问，那么增加上下游的能力就简单多了。\n这种理想化架构的特点：\n在往日志写数据前，可以进行数据清洗； 因为这一步的操作人最清楚数据需要如何被处理，并且这里的操作应该是无损、可逆的。 对日志可以进行实时转换，产生新的日志； 而一些添加新值一类的转换，则可以基于日志统一在业务处理之后进行处理。比如为一些事件类型数据增加session字段；或者为一些通用工鞥那增加衍生字段。 在目标系统中可以处理一些加载逻辑； 只有与消费者这里相关的聚合才适合放到加载阶段处理，比如需要把数据转为星型、雪花型的schema（支持报表、分析场景）。 以上，是经过分析后的良好的ETL架构。\nP2.2.5 日志与事件驱动 上述架构除了一些正向的好处，也带来了一些附赠的可能性：\n系统解耦； 事件驱动； 一般在web技术栈中我们通过记录日志来往数仓或者hadoop技术栈输送数据，以供后续的聚合分析、查询使用。这种做法的问题与上面我们分析ETL系统一致：就是我们整个数据流程耦合了数仓、进程调度、业务逻辑。\n而在领英，我们基于日志中心构建了一套事件处理的架构。首先以Kafka为中心，多个订阅方消费事件日志。由此，我们定义了上百种事件类型，每种类型都捕获对应的事件数据。这套架构覆盖了公司内用户画像、搜索、服务调用以及异常等业务场景。\n下面举例说明帮助大家理解一下上述架构的优势： 场景是一个展示JD岗位的页面，本来job页面应该只描述岗位相关信息，但是由于各种动态化的需求，慢慢地这个页面上会加上各种逻辑，比如：\n我们需要离线分析JD数据； 需要针对PV做count统计分析； 需要针对一些访问聚合统计结果，以供发布者了解访问走势； 需要记录用户访问记录，以此确保给用户的推荐不会重复； 需要跟踪JD页面的浏览数据，以此衡量这个岗位的流行程度； 等等。 按照这个发展，我们的一个展示岗位的页面变得越发复杂。 往往，我们的端也是多样的：手机移动端、wap端、web端，端也提高了复杂度。而当一个新人需要加新功能，那他可就头大了。而现实中、生产环境中，问题只会比这个更复杂。\n「事件驱动」架构可以简化这种场景的复杂性。 岗位展示页面只展示岗位信息，同时记录关联属性、查看用户。 而其他下游、对接系统（推荐、分析、数仓）只接入、消费日志，下游的处理与核心业务系统解耦，这样业务之间完全不影响，复杂度维度也降低了。\nP2.2.6 构建可拓展的日志系统 将发布者与订阅者解耦并不是啥新鲜事，但是在这其中，如果想搞一个支持多订阅、实时的日志系统，支持大规模拓展还没那么简单。\n大家一般会认为分布式日志是一种比较慢、重量级的实现（像ZK只负责存储元数据metadata），但如果想支撑大数据量的流式场景，日志的实现必须要快、轻（可以接入各种数据）。在领英，使用Kafka单日处理的数据量级达到了600亿的级别（算上数据中心之间的同步，单日写入量就上千亿了）。\nKafka的「秘诀」如下：\n日志分区（分片）； 通过批量读写优化吞吐； 避免无用拷贝（零拷贝 zero-copy）； 比如这就是分区日志： 每个分区都是有序的，但是分区间不保证顺序（除非业务层维护了顺序比如时间字段）。 数据写入到日志分区由生产者控制，一般通过业务字段路由到对应分区。 这种写入机制不需要协调多个节点，因此可以根据业务规模线性拓展集群。 每个分区可配置一定数量的副本节点，副本会同步追赶主节点的进度，当主节点挂掉时，副本们会选举出一个新的主节点。 不保证分区间的全序可能有点限制，但是问题并不大。因为log日志流的场景下，我们的节点往往有成百上千个，所以保证全序并没有那么要紧。像Kafaka保证的语义：从单个生产者发出的消息会以发出的顺序进行处理。 日志与文件系统类似，针对线性读写有很多通用的优化思路，比如日志可以将小的读写操作合并为大的操作，以此提高吞吐。Kafka将这个理念发挥到了极致，它的批量场景： 生产者像broker发数据时； 写磁盘时； 多节点间的数据复制时； 消费者拉取数据时； ack提交响应时； 而在内存log、磁盘、网络传输中，Kafaka仅仅使用最简单的二进制格式，所以很方便使用零拷贝（zero-copy data transfer）。 如此一来，我们的读写效率发挥到了极致，在内存超大的情况下，磁盘与网络IO也能打满。\n本文不过多讲解Kafaka，更多细节还请大家自行参阅 http://sites.computer.org/debull/A12june/pipeline.pdf 以及 Kafaka设计文档。\nP3：日志与实时流处理 到此，我们主要探讨了在系统、节点间复制数据的各种操作，但这还远不是分布式系统的全貌。下面，我们探讨下流处理，日志≈流处理（http://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/）。\nP3.1 什么是流处理？ JK认为传统的对流处理的理解是不对的，他认为流处理与SQL、实时处理毫无关系。\nJK对流处理的定义：\n进行持续数据处理的基础设施； 数据计算的模型可以抽象为像MapReduce类似的分布式处理框架，同时需保障低延迟； 数据处理的模型往往取决于数据收集的模型，比如批量收集一般就批量处理，而持续采集的数据一般会持续处理； 以1790开始的美国人口普查为例，当新一轮的普查开始后，工作人员挨家挨户进行调查、填表，收集完表格之后进行汇总；这个例子背景有点久远，但是与批量处理的模型是一致的； 在领英内部，数据处理完全不用批处理。分析业务场景，发现只有两类数据：\n活动、事件、监控类； 数据库事务类； 这种业务天生就是持续处理的。反过来想想为什么会有批处理？往往是因为数字化程度（自动化、信息化）不够！系统中有一些人工介入的操作，而且人工操作必定是有停留时段的（数据产生到处理结束有空窗期，想想A跟B通过邮件完成一个流程节点需要多久？）。\n而我们线上的很多定时任务，本质上是一种固定时间窗口的持续处理任务。整理下JK对流处理的认知：\n以一定时间维度，处理数据； 不要求使用静态数据快照，支持用户自定义处理频率； 流处理是批处理的泛化抽象（在实时业务场景下，这个意义很大）； 很多公司的实时数据收集没做好，这个影响了后续流处理的成效； 比如很多业务形式是面向文件、定期处理的，这个是与实时数据脱轨的； 在领英早期，很多数据也是以小时的维度收集的，接入流处理的时候，系统优势体现不明显，这也是很多系统常见的问题； 而在金融领域，流处理系统能大放异彩的原因就是，金融系统建设完善，实时数据早就建设好了； 流处理系统弥补了实时业务与离线批处理业务之间的gap； 而log日志，解决了流处理中的很多关键问题；可以参考开源的 Samza ，其中有很多以上设计的实现细节； P3.2 数据流向图 流处理最有意思的一点并不是流处理本身，而是关于我们在上个大部分（数据集成）中所讨论过的「什么是数据流」：feeds或者log日志本质上就是事件+多个系统间流转生产出的数据。流处理可以处理一个feeds之外的数据，而每一个feeds或者数据流在内部封装了其复杂度。\n拓展一下思路，任何一个从日志读数据并且将结果写入到其他系统的模式，我们都可以称之为流处理任务。log日志作为中枢节点连接了多个系统，如此一来，以日志为核心，你可以看到团队整个架构内所有的数据抓取、转换、流向。\n而流的处理要求并不高，你可以采用任意的框架、技术栈，如果有更完备的基础设施来帮助调度节点、流程，那自然更好。\nP3.3 日志集成系统的两大优点 支持了单一数据集之上的多订阅者，保障了有序性。 回想下我们上面提到副本复制时有序的重要性！日志同时保障了持久性，支持了容错（崩溃恢复）。 日志是天然的进程间的buffer。 在「发布订阅」模式下，很多时候数据生产者比消费者处理更快，如果以非同步的方式数据，这种情况下当前进程会阻塞，接下来我们可以选择： 使用buffer暂存数据； 丢弃数据； 如果业务不能容忍丢数据，那利用日志作为buffer是很好的选择，日志一方面够大，一方面保障持久性，并且与数据流中其他节点都解耦（不直接依赖下游）。 Storm + Samza 都基于上述理念设计，也都支持使用Kafka作为日志组件。\nP3.4 有状态的实时处理 很多事件数据流处理其实是无状态的，复杂的玩法主要是计数统计、聚合、连表操作。 但有时候我们基于事件需要记录一些状态数据，比如一个点击用户的userId。如果处理节点挂掉，我们怎么保证状态数据不丢失？\n首先最简单的做法当然是在内存中维护userId，但是进程挂掉，数据也就丢了。数据在内存中存活的窗口期越久，数据丢失的时间维度也就越长。\n另一种方式是依赖某个远程的存储服务，通过网络调用。但是会增加网络IO的开销，以及数据损失了本地的局部性（局部性原理！！！）。\n不禁发问：如何按照我们处理的流进行分区，并且实现一个类似表的东西？\n回想下上面提到的表与日志的对称性！本地日志就可以记录状态变更，而语句的执行结果就是一个表。同时表保证容错性。\n表或者索引存储了状态，举几个例子：\nbdb leveldb Lucene fastbit 通过changelog记录了状态变更，在系统崩溃或者重启时可作为恢复的依据（基于时间有序、增量记录）。 这种机制其实是很通用的抽象：通过日志作为分区，记录状态，根据入参决定不同的日志形成各自的分区。 状态自己就是日志，那其他下游的处理者订阅日志即可。\n当有数据集成的需要时，上述这个模型就非常实用。从原始数据库我们提取出一份changelog，以下游需要的方式进行index索引处理。\n更多细节可以参考 Samza 。\nP3.5 日志压缩（Log Compaction） 保留完整的日志需要很大的空间，而系统的空间资源是很宝贵并且有限的，所以，我们需要压缩日志。\n我们以Kafka为例，分两种情况：\n带key值的更新； 由于可重放的特性，可以进行重复key的压缩。 丢弃老旧淘汰数据； 合并更新同一条数据的记录，例如 +1与+2合并为+3； 这个特性为 log compaction； 事件数据； 配置化保留一定周期（时间窗口）或者一定大小（内存范围）（周期、数据条数、文件大小）。 P4：系统设计、构建 最后一个话题：日志之于存储，也对我们（工程师）在做在线系统设计时有很大的借鉴意义。\n在线系统是我们最常见的业务应用形态，所以这点非常重要。\nP4.1：分布式存储 首先日志在分布式数据流（data flow inside a distributed database）、大规模的数据集成（data integration in a larger organization）中，作用类似：\n统一抽象数据流表示（日志）； 保证数据一致性； 崩溃恢复，保证数据持久性； 然后，我们发现数据流系统与数据集成系统都可以看做是一个分布式存储系统。\n面向查询的系统（Redis, SOLR, Hive tables）无非是在我们数据之上添加index索引； 面向流处理的系统（Storm or Samza）无非是开发好的触发器、视图； 所以不同的数据系统，无非是不同的索引机制、类型。\n同时我们回溯下数据系统发展的历史，最开始的时候我们只使用一个RDBMS，比如MySQL或者Oracle（讲真，我最开始参加工作的时候就是用一个Oracle处理所有需求，那还是2016 2017年），当所有数据都放在同一个数据库里的时候，我们的在线事务处理、统计分析、批处理统统在一个存储里完成（都是SQL），这个时候，其实根本不存在数据集成的需要。 而随着业务规模、业务形态的发展，我们慢慢产生了拆分数据的需求：\n系统拓展； 地理因素（比如不同国家地区有不同业务需要）； 安全性； 性能隔离 这些是拆分数据、系统最迫切的因素。这种时候，单个系统不好满足所有场景的需求，所以数据集成的复杂度、需求，是跟随业务规模、形态早就存在的。而使用一个大的Hadoop集群完成所有业务形态数据的存储，其构建复杂度是很高的。随着工业界近些年的演变，好的设计应该是这样的：\n在多个集群中，调度编排多个服务实例； 单个服务实例不必完成所有功能，如安全性保证、性能隔离、良好拓展性； 集群服务中保证单个问题都有对应解决方案； 将单个大而全的服务拆分为上述各个小的服务，也减低了我们落地实施的成本。同时各服务（存储）内职责更明确，设计目标也更加聚焦。\nJK根据已有经验，对数据系统未来发展做出了几个预想：\n保持现状，这种情况下数据集成会留有一定复杂度，因此，一个外部可集成数据的日志系统就很重要。 一个大而全的超级系统，这个明显不符合工程最佳实践。 第三种设想对工程师来说是最有吸引力的，所有参与其中的服务、系统都是开源的。开源将我们使用的服务、系统进行了彻底的解耦。这一点可以参考目前Java技术栈： Zookeeper负责分布式服务的协调调度（得益于更高层抽象的 Helix Curator）； Mesos Yarn 负责进程虚拟化、资源管理； 三方库 Lucene LevelDB 负责索引构建； Netty Jetty 以及封装库 Finagle rest.li 负责远程通信； Avro Protocal Buffers Thrift umpteen zillion 负责序列化； Kafka Bookkeeper 负责日志挤压流转（backing log）； 总结第三种方式，本质上就是职责分离、高内聚低耦合、组合模式、DC（Divide and Conquer）分治思想。积木式服务化的架构非常利于我们做系统演进，并且由于其灵活性、可靠性，我们的实施成本变低了很多。而分治思想则将大问题拆解为了小问题，小问题无疑更容易解决。\nP4.2：日志在系统架构中的角色 一个带有外部日志的服务，可以将一些通用的复杂性交给日志模块来处理，比如：\n通过将多节点接收的并发修改序列化（sequecing），来达成数据一致性（实时的、最终的）； 提供多节点间的数据复制功能（replication）； 对数据写入方提供写入语义（commit），比如当应用可以保证数据不丢时返回一个ack响应； 面向系统提供基于外部日志的订阅流（subscription feed）； 保障崩溃恢复、节点数据追赶（restore failed replicas or bootstrap new replicas）； 支持多节点间的负载均衡（rebalancing）； 写到这里发现，这不就是一个分布式系统的职责与实现吗？剩下的，无非就是查询与索引。而查询与索引在不同系统间会有不同的设计，比如ES（基于Lucene构建索引）在做查询的时候，只需判断是单条（根据id找到对应partition或者shard）还是多条聚合（拉取所有使用到的分区数据）。\n到此，一般一个存储系统就分成了两层：\n服务层（Service）； 为对应查询创建索引，如一个KV存储可能需要b-tree sstable，而一个搜索存储则需要倒排索引（inverted-index）； 日志层（Log）； 有序捕获状态变化； 负责接收写入（可能被上述服务层代理，但写入的最终处理是由日志层负责），写入到日志时同时产生一个时间戳作为时序的索引； 假定我们这个存储系统是经过了分区的分布式系统，那么服务层与日志层分区数一致（机器数可以不同）。 如上图所示，Service节点订阅日志变更，发生写入时尽快以日志顺序同步到自己本地的索引中。此时客户端完成了read-your-write（读自己的写）语义：\n查询时携带写入时生成的时间戳； Service节点比较此时间戳与其本地索引构建时记录的时间戳，如果发生了日志-\u0026gt;服务层同步的延迟，客户端请求等待节点同步追赶，以防返回过期数据； Service层在简单情况下无需处理主从、选举问题。因为此时日志保证了数据的一致性、准确性。\n至此还有一个棘手的问题，是每个分布式系统都需要解决的：\n节点失败后恢复数据； 节点间转移分区； 一种经典做法是：\nlog中保留固定周期的数据（想想Redis中的AOF）； 对数据进行全量快照（想想Redis中的RDB）； log中保留全量数据，并对log进行整理压缩（相同key只保留最终值）（想想Redis中的AOF Rewrite）（Log Compaction），3这种做法等同于1+2的效果； 而上述在log层处理这类问题，则将负责度交给了log来处理，Service层无需关注。并且log层的处理是系统级别的，可以通用。\n基于以上描述的log系统，我们也就有了一套可存储数据的ETL数据源，并且可以为其他系统提供完备的订阅接口。\n到此，结合上图，我们发现log系统成为了其他系统处理、加载的数据流的提供者（provider of data streams）。同时，一个流处理者可接收多个输入，并且将数据提供给其他系统（以做其他类型索引的处理）。\n回到我们P4部分的标题：系统设计、构建。JK觉得这种将查询、log分层是一种通用的系统设计思路：分离查询特性、一致性、可用性。这种设计思想对于我们构建系统、理解系统很有帮助。\nKafka 或者 Bookeeper不一定必须是持久化的设计。我们完全可以设计一个 Dynamo 这样只保证 AP 的最终一致（带来的一个副作用是消息会重放，消费端需保证幂等）的系统。\n有人认为在log中保留全量数据是一种浪费。其实在很多方面，这个问题已经被弱化了：\nlog日志是一种高效的存储格式。比如linkedin在2013年的时候，kafka服务端在每个数据中心存储了75TB的数据；同时应用层面则要求使用更多的内存，所以log更节省成本； 应用对硬件要求也更高，比如使用了SSD，而log只需要线性顺序写，使用普通磁盘进行存储即可； 结合上图，应用架构复杂后，参与其中的应用变多，每个业务形态中的索引都需要单独存储，这样整体上平摊了单个节点的费用（amortized over multiple indexes）； 上述几点说明，log日志的存储成本并非问题。\n围绕日志的这种架构模式也是领英在内部构建自己系统的方式，所有的服务订阅一个存储（使用databus作为一种日志抽象 或者 使用kafka来做日志系统）。应用层处理分区、索引、查询逻辑。领英就这样实现了他们的搜索、社交关系图、OLAP查询系统。\n使用log的程序也需要根据情况适应调整，一个完全可靠的系统（如RDBMS）可以借助log来完成：\n数据分区； 节点恢复； 负载均衡； 一致性； 数据广播； 如此一来，应用层（Service 服务层）变成了处理索引、查询的缓存模块。\nP5：结束 If you made it this far you know most of what I know about logs. Everyone seems to uses different terms for the same things so it is a bit of a puzzle to connect the database literature to the distributed systems stuff to the various enterprise software camps to the open source world. Nonetheless, here are a few pointers in the general direction.\n如果你读到了这里，JK大佬觉得，你已经了解了他对log的所有知识。最后JK大佬给大家提供了一些值得关注的资源：\n学术论文、博客、talk 概述：state machine 状态机 primary backup replication 主从备份 PacificA 是微软基于日志实现的一个通用分布式存储框架 Spanner 是Google实现的基于物理时间并且将时间戳视为一个范围来应对时钟漂移的一个存储系统 Datanomic 结构数据库是Rich Hickey（clojure的作者） 在他一个创业公司的产品 A Survey of Rollback-Recovery Protocols in Message-Passing Systems 在消息传输系统中关于回滚-恢复协议的报告，其中对容错、基于日志的恢复有很好的讲解 Reactive Manifesto 有些事件驱动的讲解 Martin Odersky Scala大佬在coursera开的课 Event Sourcing state machine replication状态机复制的正确说法 Change Data Capture 对日志最友好的数据解析方式 Enterprise Application Integration 教你如何在企业应用背景下解决数据集成问题 Complex Event Processing (CEP) Paxos 原始paper 发展历史 加入了一些有趣细节的改良版本 Fred Schneider Butler Lampson 使用 Paxos构建一个可拓展的持久性存储 Using Paxos to Build a Scalable Consistent Data Store Paxos的竞品，也许更适于实践： 最早实现日志复制的算法 Viewstamped Replication 更好理解的共识算法 RAFT 企业级软件面临类似的问题 事件源：企业软件中的「状态机复制」Event Sourcing 在不同的语境下，相同的理念被重复了一次 开源组件 Kafka 基于日志实现的一套流系统 Bookeeper 专注于事件数据的日志系统 ","permalink":"https://redolog.github.io/posts/rd/log/translation/what-every-software-engineer-should-know-about-real-time-datas-unifying/","summary":"\u003cblockquote\u003e\n\u003cp\u003eJay Kreps (Confluent CEO，Kafka 核心作者) 在《The Log: What every software engineer should know about real-time data\u0026rsquo;s unifying abstraction》中系统性描述了日志的价值和重要性，指出了日志特定的应用目标：它记录了什么时间发生了什么事情（they record what happened and when）。而这，正是分布式系统许多问题的核心。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"译---日志：每个软件工程师都应该知道的有关实时数据的统一抽象"},{"content":"解答：ES为什么建议使用32或者26GB的堆？并且了解JVM中的指针压缩设计。\n结论 Elasticsearch推荐使用的堆大小不超过32GB，主要是因为Java的垃圾收集器（Garbage Collector）和对象指针压缩（Compressed Object Pointers）的工作方式。\n对象指针压缩：在64位的JVM中，如果堆大小小于32GB，JVM可以使用一种叫做\u0026quot;Compressed Ordinary Object Pointers\u0026quot;（COOP）的技术，将64位的指针压缩为32位。这样可以节省大量的内存，因为在JVM中，大部分的内存都是用来存储对象指针的。但是，一旦堆大小超过32GB，JVM就不能使用COOP，这会导致内存使用率显著增加。 而推荐26GB堆的原因则是JVM内部内存计算并不是严格的1024进位，即1GB==1000MB，因此使用26GB可以确保使用到COOP技术。 垃圾收集器：Java的垃圾收集器在处理大堆时，可能会导致长时间的停顿。这是因为垃圾收集器需要遍历整个堆来查找和清理无用的对象。如果堆太大，这个过程可能会花费很长时间，导致Elasticsearch无法响应请求。 因此，Elasticsearch推荐的堆大小是物理内存的一半，最大不超过32GB。这样可以在保持对象指针压缩和避免长时间的垃圾收集停顿之间找到一个平衡。\n本文下方记录研究此问题的过程。\n前提 针对ES文档设置堆大小产生疑问。 我想了解下JVM中的指针压缩设计，也就是上述疑问的答案。 JVM自动帮开发者管理内存（开发者无需自行操作内存指针）。 指针压缩是JVM针对内存管理所做的优化技巧之一。 本文提到的JVM默认均为HotSpot JVM。\n本文使用到的JVM参数：\n1 2 3 4 5 6 7 8 # 打开指针压缩，jdk7之后只要 -Xmx 的值少于32GB在64位机器上都默认打开 -XX:-UseCompressedOops # 打印指针压缩日志 -XX:+PrintCompressedOopsMode # 指定对齐填充字节，默认对齐8字节 -XX:ObjectAlignmentInBytes=16 # 打开诊断JVM选项 -XX:+UnlockDiagnosticVMOptions JVM对象结构 我们先看看JVM中的对象在内存中是如何表示的：\n对象头（object header） mark word 存放了： 锁信息（biased locking pattern, locking information） hashCode（identity hashcode） GC信息（GC metadata） 指向类的指针（klass word） Java7之前指向的是永久代，从Java8开始指向元空间。 类名（class name） 修饰符（modifiers） 父类信息（superclass info） 数组长度（仅针对数组对象，4字节） 实例数据（instance data 或者 array data） 对象实际信息，比如字段值 对齐填充（alignment paddings） 占用32位4字节，对齐的设计是为了硬件友好。 同时在JVM中，使用Ordinary Object Pointers (OOPS)数据结构表示指向对象的指针。而指向对象与数组的指针的数据结构叫oopDesc。这个oopDesc包含了上面我们提到的两个内容：\nmark word klass word 这部分是可以被压缩的。 如下源码：\n1 2 3 4 5 6 7 8 9 class oopDesc { friend class VMStructs; private: volatile markOop _mark; union _metadata { Klass* _klass; narrowKlass _compressed_klass; } _metadata; } 以上，普通对象对应instanceOop，而数组对象对应arrayOop。\n32位机器到64位机器的变化 32位机器的一个限制：堆内存最大4GB。原因是内存只有2^32bit==2^10(KB)*2^10(MB)*2^10(GB)*4即4GB大小，OS级别也受此限制。而64位机器内存则高达TB级别。\n在64位机器上。 普通对象长度至少16字节，其中包括：\nmark word 8字节 klass word 4字节 padding对齐填充 4字节 数组对象长度至少16字节，其中包括：\nmark word 8字节 klass word 4字节 数组长度 4字节（32位的word） 当我们从32位机器迁移到64位机器上时，肯定是希望性能更好，但是这个问题没有这么简单。\n如上面的描述，我们的对象有指针（klass word），而指针在64位机器上占用了大约是32位机器上1.5倍的空间。同时也带来了新的问题：\n更多的内存消耗 更频繁的GC（导致我们的应用线程占用更少的CPU时间片） 所以这引出了JVM内存优化的方案：指针压缩。\n指针压缩 指针压缩即Compressed Oops，简称COOP。\nJVM默认帮我们开启了指针压缩，作用是在64位的机器上也可以存储32位的指针（对象头中的类型指针以及引用类型的字段），从而节省内存、提高性能。\n在实现上，压缩的是oopDesc中的_metadata。\n64位转32位：encode；32位转64位：decode。\nencode过程的原理我们可以这么理解：\n时间换空间（利用CPU计算少许逻辑，达到节省内存的作用），因为需要计算，所以叫做encode 由于JVM默认对齐填充8字节，所以我们的oops永远是8的倍数，而8的倍数使用二进制表示时，最后三位永远是0，所以我们多出来三位，decode时可以补三位零进行还原。 综上，64位机器上我们使用32位的空间，实际可以寻址到2^35bit==2^10(KB)*2^10(MB)*2^10(GB)*2^5==32GB。\n所以使用更少的空间，我们可以寻址更多地址。\n对应decode时，我们需要将其左移 3 位，再加上一个固定偏移量，便可以得到能够寻址 32GB 地址空间的伪 64 位指针了。\n基于0虚拟地址的指针压缩 Zero-Based Compressed Ordinary Object Pointers (oops) Zero-Based指针压缩的意思就是从32位decode64位地址时无需加一个Java堆的基础地址。\n当堆小于4GB时，JVM内可以使用一个字节的偏移量，避免使用一个对象的偏移量，这也就节省了一个8位的偏移量空间。同时将64位地址encode为32位地址时也非常高效。\n在Solaris, Linux, Windows这些系统上，堆空间小于26GB时，一般可以用到Zero-Based指针压缩（相比普通指针压缩更加高效）。\n小结 通过本文，我们从理论上研究了JVM指针压缩以及基于0虚拟地址的指针压缩设计原理。\n也明白了为什么ES建议JVM堆调优设置为32GB：使用指针压缩性能更好，而使用26GB时可以应用到基于0虚拟地址的指针压缩，性能更佳。\nRef anatomy-quarks/23-compressed-references Compressed OOPs in the JVM HotSpot/CompressedOops ES:ECE:JVM Heap Sizes elasticsearch:Setting the heap size javase/7/docs/technotes/guides/vm/performance-enhancements-7.html ","permalink":"https://redolog.github.io/posts/rd/jvm/compressed_oops/","summary":"\u003cp\u003e解答：ES为什么建议使用32或者26GB的堆？并且了解\u003ccode\u003eJVM\u003c/code\u003e中的指针压缩设计。\u003c/p\u003e","title":"ES为什么建议使用32或者26GB的堆？"},{"content":"踩坑MySQL主从延迟背景下，流程业务卡住无法流转，问题得以暴露。\n背景 今天这个问题我们的监控没有提前观测到，属于业务处理时未考虑到主从同步，用户反馈过来时，距离问题发生其实已经过去了几天。\n我们有个业务流程表：t_apply，基本流程：\n我们的服务端应用biz-server接收到用户提交的数据，创建一条申请单数据。 触发外部依赖的一个校验服务validation-server，这个校验动作一般耗时：10ms-10s。 校验服务处理完成后回调biz-server接口，更新我们的申请单状态。 很清晰、简单的一个流程。\n其中1-2隐含了MySQL主从复制的过程。\n而今天爆出的业务问题时：数据提交了，申请单状态却卡在了INIT状态，也就是校验状态未成功回写。\n正常的状态变化：INIT-\u0026gt;NODE1。同时，我们的业务MySQL一个实例被多个业务方共享。\n问题排查 查日志 出现问题先查日志： 发现回调是有触发的，而回调内的更新状态的sql却没有执行（对应日志没打出来）。\n疑问 什么情况会导致select出的数据是空的呢？正好我们前两天刚好解决过一个主从同步延迟导致的问题，所以经验告诉我：发生了主从同步延迟。\n验证 查问题还是应该有依据才能得出可靠的结论。对照了两个时间点：\n数据创建的时间点：Thu Dec 16 2021 18:47:25 GMT+0800 (China Standard Time) 校验服务回调的时间点：Dec 16, 2021 @ 18:47:33.387 耗时秒级别，属于前期接入此校验服务时正常预期范围。\n观察数据库监控大盘： 对比16号的TPS与最近一周的TPS数据，发现平时TPS大约为500，而16号达到了差不多7k的量。到这里已经可以看到12.16下午六点多开始出现数据库流量尖刺。\n如上通过数据库的监控大盘，我们找到了问题（主从复制延迟）以及问题原因（DML尖刺带来TPS激增）。\n综上，我们画出问题对应时序图： 时序图帮助我们更形象地表达问题过程。\n问题解决 到此我们定位到了问题：主从复制延迟。 以及分析出了问题原因：DML尖刺带来TPS激增。\n同时为了更好地解决问题，我拜托了DBA大佬帮忙导出了16号18点到19点的DML语句（解析binlog行），这一步可以帮助我们分析是否为其他公用MySQL资源的业务方带来的DML流量尖刺。\n资源充足 经分析对应时段DML语句，我们发现有很多数据写入是其他业务带来的，如果资源充足，我们可以隔离多个业务间的DB资源。\n资源不太充足 | 业务要求不高 DB资源没那么充足或者业务要求并没那么高（这里主要指MySQL主从复制的延迟容忍度）的情况下，比如今天排查的这个应用属于集团toB业务，资源要求不高，那么我们不单独申请DB资源。\n在这个框架内，我们依然可以改善、解决问题：\ncallback业务方法或者对应select语句处，强制指定主节点数据源：@DS(\u0026quot;master\u0026quot;) callback业务方法处：指定事务：@Transactional(rollbackFor = Throwable.class)，使用的mybatis-plus主从切换如果在事务内默认读主节点数据 同时排除带来DML尖刺的业务操作（超出了当前数据库处理的能力）。\n数据库优化 主库的TPS并发越高，产生的DML数量就越超过slave单线程的处理能力，延迟也就越大。新版本MySQL主从复制已支持多线程，因此升级MySQL是一个优化点。如果此时slave查询锁等待过多，也会影响复制进度。\n反思 纵观整个过程，你会发现，这又是一例因为错误实现导致的问题。所以：\n提高团队的战斗力志在必行。 问题解决后带来的是宝贵的经验，珍惜问题，把每个问题解决好。 实现一个方法的时候，最好同时能有整体架构的高度思考。 Ref MySQL主从数据库同步延迟问题解决 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/db-replicate-delay/apply_callback_miss/","summary":"\u003cp\u003e踩坑MySQL主从延迟背景下，流程业务卡住无法流转，问题得以暴露。\u003c/p\u003e","title":"MySQL主从延迟业务数据缺失问题一例"},{"content":"踩坑通过消费MQ，从MySQL同步Redis时，并发时暴露消费无序的问题，导致数据被覆盖。\n背景 对接组的一个服务A有这么一个数据流向，一份数据写入三个地方：\nMySQL 存储原数据 Redis 存储数据快照备份，字段与DB保持一致 ES 存储原数据涉及的查询字段 问题出现在MySQL同步Redis这一步，因此本文暂忽略ES的部分。同时MySQL无从节点介入，因此只考虑DB主节点即可。\n数据流向：\n数据变更，更新MySQL 这里假设更新 S表，其中有字段： id name status 生产RocketMQ消息 消费者收到消息，从MySQL拉取此时最新数据，写入Redis 问题： 两个操作依次（即使是并发也会有先后顺序）触发变更动作：\n其中A操作更新name：oldName-\u0026gt;newName 其中B操作更新status：1-\u0026gt;2 按道理如果是顺次执行，最后结果应该是：\nid=0 name=newName status=2 最终结果为：MySQL存储正常（因为加了事务），而Redis中数据变为了：\nid=0 name=newName status=1 其中B操作的结果在Redis丢失。表现为最新操作的数据被旧的操作数据覆盖。\n问题分析 此问题应该画个时序图：\n如上ThreadA代表操作A生产者对应的MQ消费者，ThreadB代表操作B生产者对应的MQ消费者，理想情况下我们认为AB是顺次执行（无程序逻辑保证），当实际运行时，如上图所示，ThreadA的操作被阻塞（如进程暂停）时，操作无法保证多组件间的全序。\n新数据因此被一个迟到的旧操作携带的旧数据覆盖。\n问题解决 治标 方案一 首先应用当前整体架构无法做大调整，因此优先在此基础上解决问题。\n问题产生的原因是RocketMQ的多个消息消费无顺序保证，当前序执行的消息未消费成功时，后续消息不应并行执行，因此将此处的消息生产消费改为顺序消费，即可解决当前问题。\n方案二 可在消费入口增加全局锁（分布式锁），这样当前序消费者拿到消息后，后续消息访问时只能阻塞。\n缺点：\n中间件从逻辑上无法保证前序的消息一定先到达消费者服务，如发生网络延迟时，依然有问题。 引入了分布式锁，有可用性风险，并且增加了开销。 并发高且资源出现瓶颈的情况下，有流量尖刺。 方案三 缓存数据增加版本标识，利用乐观锁的思想控制全序。\n缺点：需应用实现版本判断。\n治本 跳出应用历史实现的框架，我们发现，此服务逻辑本质上是在处理数据库、缓存的双写数据一致性。\n既然是要保证数据多写的一致性，一般情况我们不应该使用先更新数据库再更新缓存的方案，通常情况下使用更新数据库然后删除缓存的方案更加可靠，如果应用存在主从同步延迟，可适度增加延时双删的逻辑。\n除此之外，此流程中的MQ组件，个人认为可以去掉。\n小结 方案设计应考虑全面，优先实现数据准确性一致性，其次才是可用性等方面的考量。 出现问题尽量治本，单纯治标是为以后挖坑。 数据多写的情况使用通用可靠方案，切勿自创逻辑不严谨的方案。 Ref RocketMQ - 如何实现顺序消息 缓存更新的套路 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/consistency/mysql_sync_redis_with_process_pause/","summary":"\u003cp\u003e踩坑通过消费MQ，从\u003ccode\u003eMySQL\u003c/code\u003e同步\u003ccode\u003eRedis\u003c/code\u003e时，并发时暴露消费无序的问题，导致数据被覆盖。\u003c/p\u003e","title":"消费RocketMQ无序同步缓存，导致数据覆盖问题一例"},{"content":"踩坑MySQL主从延迟背景下，生产消费未正确处理此类场景，数据消费第一次失败报警，问题得以暴露。\n背景 前几天我就收到了这个报警，报警内容：\n[恢复][部署平台][我负责的服务PPP][接口响应状态码异常][接口：接口a响应状态码：500看图：https://grafana.be.company.com/d/i4jgtCN7c/kan-ban-ppp?editPanel=141\u0026amp;orgId=1] 系统: 部署平台 策略名称: 接口响应状态码异常 报警描述: 接口：接口a响应状态码：500看图：https://grafana.be.company.com/d/i4jgtCN7c/kan-ban-ppp?editPanel=141\u0026amp;orgId=1 时间: 2021-12-07 10:25:58 持续时间: 3m11s 服务节点: 服务节点全链名 合并数: 1 合并主机: 这个接口是a模块数据变更后，B系统向A系统拉取a模块数据的查询接口。\n问题分析排查 首先查日志，根据个人习惯与团队设施操作即可，一般我们使用ELK技术栈或者Linux文本技术工具。\n可以看到使用的消息中间件侧的消息索引，这个可以 用来定位消息。\n使用的消息中间件侧看到消息进行了重试，第一次失败，而第二次成功。重试容错，对业务侧没有影响。\n下一步需要确认下第一次报错的原因。初步在本地用postman调用生产这个入参的接口： 调用多次，平均耗时\u0026lt;100ms。当时基于此得出结论：接口没有问题，监控为误报【为多次排查埋下伏笔，这也是一个错误的惯性思维】。\n20211207初步处理 初步得出结论：以后出现问题继续观察下，此问题无继续排查下去的必要。\n20211209再次分析 问题看起来非偶发，需要重新排查。\n发现NPE，根据异常栈可直接定位到报错代码行数。\n疑点 对应代码初上线时并无问题，说明是后面某个变更导致的新问题。 出现了多条这种数据（消息重试第二次才会成功），说明不是巧合，是必现问题。 排查 突然想到项目中前几周上了读写分离的插件，这个动作立即与当时负责的RD确认，MR记录还存在。\n到这里问题经过变得清晰：\n消息生产时，数据写入了主库节点。 消费者通过生产者服务拉数据时，此时MySQL从节点正在异步写数据，所以此时查出的数据为空。 问题解决 将此NPE异常封装下，错误返回到接口的Response中，消费者可拿到响应码做判断、重试即可。 如果消息组件支持延迟消费，可以通过配置的方式处理此问题（RocketMQ支持了延迟消费功能）。 反思 同类问题多次报出，说明不是巧合。 业务可用性只是问题判断的依据之一，应用中的异常、报警指标也是问题的判断依据，需要兼顾这几个方面。 每个问题都值得重视。 Ref MySQL主从数据库同步延迟问题解决 DynamicDataSource读写分离实现 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/db-replicate-delay/msg_consumer_no_delay/","summary":"\u003cp\u003e踩坑MySQL主从延迟背景下，生产消费未正确处理此类场景，数据消费第一次失败报警，问题得以暴露。\u003c/p\u003e","title":"MySQL主从延迟消费重试问题一例"},{"content":"踩坑事务内循环调用sleep，通过调整线程池参数的方式问题得以暴露。\n背景 12.07晚我负责的应用发布了一批需求相关的变更，乍一想如上报错接口与那批需求无关。\n问题排查 遇到问题首先查具体日志，grafana这里只是告诉哪里有问题，但是应用内部的情况，这里并没有打出来。\n定位问题代码位置 可以通过kibana查下打到es中的应用日志： 发现对应时段有大量MySQLTransactionRollbackException报错。 进入异常栈，简单分析可得，这行代码有问题。\n如果es日志目前还不支持上下文（前后内容）的展示，我们可以通过最基础的方式把相关日志找出来，进终端：\n1 cat log.2021-12-08.log | grep MySQLTransactionRollbackException -C50 \u0026gt; tmp.log 必要情况下可以多打些内容出来，根据自己需要指定需要的行数。\n通过python起个http-server，暴露80端口进行日志文件的下载：\n1 python -m SimpleHTTPServer 80 如图： 日志拿到本地，我们就可以更方便得浏览、分析问题。\n分析问题代码 初探 由上可知，A.java:60 这里负责更新A关系表（这里关系到数据库事务锁）。而调用的地方是 BServiceImpl.java:179 a方法，翻开源码（伪代码）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // BServiceImpl a方法 @Transactional(rollbackFor = Exception.class) @Override public void a(Req req) { req.getShops().forEach(s -\u0026gt; { // 这里调用了sleep的方法 b(); }); } // 另一个service中的方法b public void b(){ // 延时10s发消息，避免一定程度的主从延时 ThreadUtil.sleep(10000); // 执行一系列落表操作 } 初步看没有什么问题（跑了好久没问题），发现附近最新的变动来自10.18，我们点进去看看：\n发现调用了java Thread的sleep方法（也就是上面的方法b）。这里注释写得很清晰，是为了解决数据库主从延迟的问题，因此这里延迟10s执行。\n但是！sleep不会让出monitor的ownership，同时这里业务层加了事务注解，事务持有的锁资源也不会释放。\n通过 DataSourceTransactionManager.doBegin() 我们可以看到，本质上spring这种框架是通过java.sql包下的接口与各种数据库进行交互的。 连接、锁资源均绑定到执行当前事务的线程上。\n疑点 但是！我们这里的 b() 贴了 @Async注解，这里明明是通过新的线程池去执行的，所以理论上sleep不应该阻塞、占有当前事务线程的资源。 但是巧就巧在，我们在昨天上线的代码里，把默认的线程池配置改了，现在的Executor配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Configuration public class ExecutorConfig { @Primary @Bean public Executor taskExecutorA() { return new ThreadPoolExecutor(1, 5, 5L, TimeUnit.MINUTES, new LinkedBlockingQueue\u0026lt;\u0026gt;(2000), new ThreadFactoryBuilder().setNameFormat(\u0026#34;Async任务默认线程池-%s\u0026#34;).build(), new ThreadPoolExecutor.CallerRunsPolicy()); } } 线程池coreSize是1，maxSize是5，队列容量指定了200，而拒绝策略指定了callerRuns，这些参数以我对代码的了解（之前不知道发消息的业务中使用了sleep）下经过了预估，预估是可行的。而原先默认的线程池使用了spring默认配置，可参考TaskExecutionProperties，其中coreSize是8，maxSize是Integer.MAX_VALUE，队列容量也是Integer.MAX_VALUE，默认拒绝策略为AbortPolicy，因此原来使用此线程池会不停创建线程进行sleep，通过消耗线程、内存资源将此问题覆盖了。\n破案 经过上面对sleep代码的分析，我们目前可知：请求数比较多，并且是循环调用的情况下，线程池200队列长度的限制很快会被突破，业务此时触发CallerRuns策略，回到原线程执行，原线程经过多个sleep10s操作，一直持有事务资源，此时业务方法大概率会超过事务等待时间。\n1 2 # 经与DBA确认，事务锁等待时间为默认的60s SHOW VARIABLES LIKE \u0026#39;innodb_lock_wait_timeout\u0026#39;; 回到我们上面出错业务方法的代码：\n注意req.getShops()，只要这里循环超过六次，这里的循环内的sleep10s累积起来一定会超过innodb_lock_wait_timeout的60s。至此，抛出MySQLTransactionRollbackException: Lock wait timeout exceeded。\n问题解决 针对性处理： 调整线程池配置，可适当增加队列长度、线程数（根据具体业务调整，并且隔离不同业务线程池）。 去掉sleep操作，使用ScheduledExecutorService进行定时、延迟业务的执行。 长期优化： 优化上线流程，增强CR。 制定编码规范，增加条目：禁止事务中使用sleep这类不释放资源的方法。同时必须使用sleep的时候使用TimeUnit类进行调用。 加强、丰富监控告警。 ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/sleep-in-transaction/","summary":"\u003cp\u003e踩坑事务内循环调用sleep，通过调整线程池参数的方式问题得以暴露。\u003c/p\u003e","title":"事务中调用sleep问题一例"},{"content":"踩坑reflections反射库突然无法通过反射正常加载类，原因竟是Guava未做新老版本兼容。\n背景 我的代码没有变动，测试突然找过来，说一个接口无法正常运行了。通过报错发现是一个工具类无法通过反射加载类。\n问题描述： 其他同学负责的分支并入了测试环境后，测试反映我的接口调用出现反射问题【reflections依赖版本覆盖导致reflections反射出错】。\n关键代码，仅仅是下述伪代码的样子：\n1 Util.check(); 报错：\n1 2 3 4 5 6 java.lang.NoSuchMethodError: com.google.common.collect.Sets$SetView.iterator()Lcom/google/common/collect/UnmodifiableIterator; at org.reflections.Reflections.expandSuperTypes(Reflections.java:380) at org.reflections.Reflections.\u0026lt;init\u0026gt;(Reflections.java:126) at org.reflections.Reflections.\u0026lt;init\u0026gt;(Reflections.java:168) at org.reflections.Reflections.\u0026lt;init\u0026gt;(Reflections.java:141) 问题分析： 可以看到是 Util 初始化动作使用了 reflections 进行反射操作。\n同时，可以看到我们的工程中有多个 Reflection 版本，如图：\n查找reflection github仓库issue，发现已有该问题反馈：https://github.com/ronmam o/reflections/issues/194 。\n可看到原因是 Guava 新旧版本不兼容导致的问题。\n问题解决：【二选一即可】 找到出现问题的reflection版本源，将其exclude掉。此处我们使用 1 mvn dependency:tree 打印maven依赖树，如图，我们可以看到是哪个module依赖了不同的reflections包： 由于前期测试使用0.9.9-RC1版本是不出错的，我们本次将0.9.11版本exclude掉，如图： 明确指定Guava版本，避免此处的版本不兼容。 1 2 3 4 5 6 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.guava\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guava\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;28.0-jre\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; ","permalink":"https://redolog.github.io/posts/rd/troubleshoot/reflections/guava_version_incompatible/","summary":"\u003cp\u003e踩坑reflections反射库突然无法通过反射正常加载类，原因竟是Guava未做新老版本兼容。\u003c/p\u003e","title":"Reflections中反映的Guava版本不兼容问题一例"},{"content":"解答组内技术分享时抛出的一个疑问：buffer跟cache的区别是什么？\n背景 目前的团队开完周会后会有一个简单的技术分享环节，本周的话题中主要是同事聊了下Java中的IO流，其中就涉及到了BufferedInputStream、BufferedOutputStream、BufferedReader、BufferedWriter这几个基于buffer数组的实现类。\n其中的成员大概长这样：\n1 2 3 4 // inputstream与outputstream负责字节类io的处理 protected byte buf[]; // write与reader负责字符类io的处理，此处应该是charBuffer的缩写 private char cb[]; 那个时候我脑子里突然蹦出了之前与网友们探讨过的一个问题：buffer跟cache的区别是什么？\n这几个实现类里面的成员名字叫buffer，那么为什么叫buffer呢？跟我们应用里面使用比较多的cache即缓存有什么区别跟联系呢？\n组内解答 大家各抒己见，领导也说了点他的理解：\n存储结构不同 设计意图不同 cache是提前把需要频繁使用的数据都读出来 而buffer则是把数据分段分批读写到一个结构中，供之后的计算使用 自己的理解 buffer buffer中文为缓存区，缓冲存储区，缓冲存储器。\nan area in a computer\u0026rsquo;s memory where data can be stored for a short time\n一般是将一些需要使用（读+写）的数据放到内存的一块区域，作为读写动作的中间层，不直接与底层存储交互，以批量、分页的形式将数据归到一个逻辑单位内，减少数据与磁盘、网络等设备的频繁IO次数。是一种延迟写思路的实现，这里的写泛指读写动作。\n结合生活中的例子，比如工位离垃圾桶很远，那么我们可以放一个小一点的垃圾桶，等小的垃圾桶满了之后统一扔到的大的垃圾桶内，这样就减少了我们直接去大垃圾桶的次数，提高了扔垃圾的效率。这里的小垃圾桶就是一个buffer的实现。\ncache cache中文为缓存，高速缓冲存储器。\na part of a computer\u0026rsquo;s memory that stores copies of data that is often needed while a program is running. This data can be accessed very quickly.\n一般将频繁读的数据放到读取效率更高的存储中，比如内存相比磁盘读取效率更高，比如CPU对应的L1、L2多级缓存之间也有读取速度差异，以此提高读性能，是一种提前读思想的实现，以此提高系统性能。\n结合生活中的例子，比如开发与产品同学的工位离得比较远，而做同一个项目时交流过于密切次数过于多，那么我们可以把工位拉近，背靠背工作，这样就减少了大家面对面沟通的成本，提高了沟通协作效率。这里的工位背靠背就是一个cache的实现。\n类比 除了buffer与cache，我们以同样的思路可以对比下Linux中的page与swap设计。\npage 比如page主要提供了数据读写逻辑单位的机制，比如Linux默认一个页是4KB，当然这个是可配置的，核心解决的问题是：内存有限的情况下，无需加载所有数据到内存中，而是按页逐页加载。\n用多少加载多少，并且按页读取也是一种局部性（挨着近使用上应该也是相近的）的设计。\nswap 又比如swap区主要是提供了一种磁盘物理中间区域，解决的核心问题是：内存有限的情况下，OS依然可以运行多进程，只不过有些后台进程的数据被切到了swap中，程序读写内存通过swap作为中间层转换。\n比如生活中几个人合租房子，合租可以租一个卧室，也可以租两个卧室，按照你的量来进行选择，这就是一个屋子一个page，而整租对于少的人来说就是浪费。而AB两个人同住一个卧室，其中A白天睡，B晚上睡，就是一种swap的设计，谁需要睡觉就把谁的被子拿进来住。\n小结 以上我们针对buffer、cache、page、swap几个IO相关的概念做了介绍以及类比。\n其中buffer、cache都是缓冲，前者主要是延迟写的思路，而后者主要是提前读的思路。\npage、swap，前者解决数据按需读取的问题，后者解决内存有限时运行多进程的问题。\n","permalink":"https://redolog.github.io/posts/rd/faq/buffer-cache/","summary":"\u003cp\u003e解答组内技术分享时抛出的一个疑问：buffer跟cache的区别是什么？\u003c/p\u003e","title":"buffer跟cache的区别是什么？"},{"content":"理解Tomcat主要的架构设计、核心组件设计、功能实现以及性能优化方案，细数我目前对Tomcat掌握的知识。\n原先打算按照深入浅出系列的结构来讲解本文，后来想想这种文章别人早就已经写过了，那倒不如我们就抱着一个简单的想法「从Tomcat中我们学到什么？」来试试把本文填满。\nServlet之于Java，相当于wsgi之于Python。我之前看到微博有人发牢骚「不懂Python的wsgi为什么要把web开发搞的这么复杂？！」，其实这个问题很好解答，如果我们只做简单的项目，我们就不需要Tomcat，甚至不需要Spring、SpringMVC，简单到一定程度连Http协议都不需要了。\n简单的需求我们直接用原生@WebServlet的方式处理接口逻辑，足矣。\n但是我们的Web后端可以这么简单吗？\n肯定不能！\n1.架构组件抽象 常见的中间件、技术组件单独拿出来做的一个原因是：分离了服务中变化与不变的内容，而技术组件只负责不变的内容。这就是解耦！\n比如本文要聊的Tomcat作为HTTP服务器实现了网络协议的连接、转换、传输功能，同时作为Servlet容器实现了接入业务接口的功能。\n针对Tomcat的核心组件画了个图： 结合一下我们平时的应用系统设计，特别是需求多、系统庞杂的情况下，针对组件做一定的解耦、抽象是必须的，这一点完全可以跟着Tomcat的样子学习：\n职责不同的组件抽出来 层级关系、节点关系可以通过组件表达出来 通过合适的设计模式设计组件，比如Tomcat中大量使用了模板方法模式来实现骨架公用逻辑 2.线程使用 线程池类自定义 线程池主类 Tomcat中自定义了jdk原生的java.util.concurrent.ThreadPoolExecutor，对应类为org.apache.tomcat.util.threads.ThreadPoolExecutor。\n关于jdk原生线程池主类的介绍参考美团技术博客：Java线程池实现原理及其在美团业务中的实践。其中图四描述了任务调度流程。\n而Tomcat的自定义子类重写了execute方法的部分逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public void execute(Runnable command, long timeout, TimeUnit unit) { submittedCount.incrementAndGet(); try { super.execute(command); } catch (RejectedExecutionException rx) { // 这里判断是否为Tomcat自定义队列 if (super.getQueue() instanceof TaskQueue) { final TaskQueue queue = (TaskQueue)super.getQueue(); try { // 尝试往队列里硬塞任务 if (!queue.force(command, timeout, unit)) { // 塞失败了再执行本来的拒绝策略 submittedCount.decrementAndGet(); throw new RejectedExecutionException(sm.getString(\u0026#34;threadPoolExecutor.queueFull\u0026#34;)); } } catch (InterruptedException x) { submittedCount.decrementAndGet(); throw new RejectedExecutionException(x); } } else { submittedCount.decrementAndGet(); throw rx; } } } 这里Tomcat针对原先已经触发拒绝策略的节点，做了再尝试往队列中塞任务的操作。\n所以结合我们的业务，如果有必要，也可以自己实现ThreadPoolExecutor，通过execute方法可以定义我们需要的执行逻辑。\n队列 任务队列：org.apache.tomcat.util.threads.TaskQueue。\n这个队列类中也有自定义逻辑，比如写入任务的offer方法：\n1 2 3 4 5 6 7 8 9 10 11 12 public boolean offer(Runnable o) { //we can\u0026#39;t do any checks if (parent==null) return super.offer(o); // 当前队列中的任务量已经达到了最大线程数，往队列中塞任务吧 if (parent.getPoolSize() == parent.getMaximumPoolSize()) return super.offer(o); // 任务提交量小于等于当前队列中的线程数，也塞到队列中待执行 if (parent.getSubmittedCount()\u0026lt;=(parent.getPoolSize())) return super.offer(o); // 队列中的任务量小于最大线程数，不往队列塞，直接创建新的线程执行即可 if (parent.getPoolSize()\u0026lt;parent.getMaximumPoolSize()) return false; // 执行原塞任务的逻辑 return super.offer(o); } 自定义的TaskQueue中维护了一个submittedCount计数字段，用来表示提交到队列中的任务量，所以最后一个if判断可以到达的前提就是：提交量大于了当前线程池的线程数，在这种情况下再判断，如果线程数还没到达最大线程数设置，就创建新的线程。\n当然，业务中我们使用队列时，针对这种默认无界的情况，我们建议根据场景来设置一个容量，防止堆积过量请求造成OOM。\n线程工厂类 线程任务工厂类：org.apache.tomcat.util.threads.TaskThreadFactory。\n这个类主要是一个工厂模式的简单实现，内部维护了例如namePrefix的字段，方便Tomcat在创建线程时定义名称前缀。\n线程模型 Tomcat中针对不同的任务使用不同的线程组，比如：\nAcceptor线程组负责连接请求 Selector线程组负责I/O 事件监听 专用线程池负责业务处理 不同类型的任务使用不同的线程组，是我们业务实践中也遵循的原则，这样隔离了互相之间的影响（配置、运行时状态），也提高了任务处理性能。\n3.合理并发容器 LifecycleBase中使用CopyOnWriteArrayList维护了生命周期事件监听器LifecycleListener列表。这类对象创建后状态基本不会发生变化，所以使用这种为读多写少优化过的并发容器可以大大提高性能。\n这里有我实际用到的例子：有一个服务负责维护SKU原数据，而这类数据经过配置后，改动的频率特别低，当时第一版的缓存设计就是使用了CopyOnWriteArrayList。\n4.延迟写 一般我们把网络传输、磁盘读写视为相对消耗性能的动作。因而针对这种操作，服务端代码都会采用「延迟写」的策略。\n我们看看org.apache.tomcat.util.net.NioChannel中的write方法，这里就会调用写操作。\n而使用到write的地方都会进行buffer缓冲，以此来减少系统调用，这是一种延迟写的思路，我们在各种技术组件中都可以看到这类使用方式。\n5.提高锁的使用效率 缩小锁的范围 在synchronized本身已经针对竞态做了无锁-\u0026gt;偏向锁-\u0026gt;轻量级锁-\u0026gt;重量级锁的优化基础上，Tomcat在工程实践中尽量缩小了锁的范围，比如org.apache.catalina.core.StandardService的方法startInternal()负责启动Service相关的子组件：\nEngine Executors Connectors 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 if (engine != null) { // 只锁定 engine 成员变量 synchronized (engine) { engine.start(); } } // 只锁定 executors 成员变量 synchronized (executors) { for (Executor executor: executors) { executor.start(); } } // 只锁定 connectorsLock 成员变量 synchronized (connectorsLock) { for (Connector connector: connectors) { if (connector.getState() != LifecycleState.FAILED) { connector.start(); } } } 这里的例子中，并没有在整个方法上加锁，而是将锁的粒度细分，来synchronized对应的成员变量，在多线程并发执行时，无需在方法维度等待，只有在访问同个成员时才需要进入线程同步逻辑，提高了并发执行效率。\n使用无锁技术 以org.apache.tomcat.util.threads.LimitLatch为例，这个类负责了AprEndpoint网络组件中的连接数限制功能。\n其内部的计数成员count使用了AtomicLong类型，底层使用CAS的实现，减少了线程切换上下文的成本，提升并发性能。\n成员sync则实现了aqs中定义的共享锁接口，内部排队使用了aqs中实现的CLH队列，相比加重量级锁，队列技术可以提高并发性能。\n6.使用堆外内存提高IO效率 以AprEndpoint为例，这个端点组件实现了APR协议相关功能。\nAPR（Apache Portable Runtime Libraries）是用 C 写的 Apache 可移植运行时库，为应用提供了跨平台的系统接口。\n首先NioEndpoint使用Java提供的 NIO 接口实现非阻塞IO模型，而AprEndpoint则使用JNI调用C库，底层使用堆外内存提升网络IO性能。\n我们关注下这部分涉及的几个关键类：\norg.apache.tomcat.jni.Socket 实现网络连接JNI相关接口。 org.apache.tomcat.util.net.SocketWrapperBase 封装了网络socket涉及的（模板方法）通用操作，比如连接过程的关键动作（生命周期）、连接基本信息设置。 org.apache.tomcat.util.net.SocketBufferHandler 管理IO过程读写使用到的buffer，IO过程涉及数据传输，过程中通过buffer来存放以及操作数据。这里可以通过参数指定具体的buffer实现类，可选： HeapByteBuffer 对象分配在JVM堆上，对应的数据byte[]同样在JVM堆上管理，网络IO过程中，需要先将内核数据拷贝到一个临时的native内存中，再从这个native内存拷贝到对应的byte[]。这里的原因简单可理解为如果直接从native区域拷贝，JVM碰到GC时对象数据会进行移动，对应buffer可能失效，最终这里的限制可理解为JVM的一个权衡设计。具体可参考R大的解释：Java NIO中，关于DirectBuffer，HeapBuffer的疑问？。 DirectByteBuffer 对象本身分配在JVM堆上，而对应的byte[]则直接位于native内存。成员中的long address记录了native内存地址（映射的作用）。因此相比HeapByteBuffer，数据拷贝少了一次，因此效率更高。 org.apache.tomcat.util.net.AprEndpoint.AprSocketWrapper 该类继承自SocketWrapperBase，构造器中主要初始化了数据拷贝对应的SocketBufferHandler，其中direct传为true表示使用更高效的DirectByteBuffer进行数据拷贝，同时创建了负责ssl连接的sslOutputBuffer，同样使用ByteBuffer.allocateDirect创建DirectByteBuffer实例，使用native内存拷贝提高ssl的执行效率。 这里我大概画个图，说明从网络中传输数据的关键流程、组件，以及对比下两个ByteBuffer的区别： 7.使用零拷贝技术 Tomcat作为一个Http-Server最常见的一个应用场景是：为客户端提供静态文件。 客户端调用接口后服务端操作分两步：\n从磁盘加载文件 将数据传给网卡 传统的做法如下图： 可以看到这个过程一共有六次拷贝数据操作。对应到org.apache.catalina.servlets.DefaultServlet中的doGet方法，跟下内部的调用逻辑，发现最终调到了copy(WebResource resource, ServletOutputStream ostream,Iterator\u0026lt;Range\u0026gt; ranges, String contentType)方法上，其中就做了读取磁盘文件，写到web层输出流中，返回的时候就是把数据扔到了网卡上。与我们上述图描述的一致。\n而AprEndpoint中的processSendfile表示底层通过sendFile的方式使用了零拷贝技术：\n1 2 3 4 5 long nw = Socket.sendfilen(data.socket, data.fd, data.pos, data.length, 0); public static native long sendfilen(long sock, long file, long offset, long len, int flags); 可以看到其内部是一个native方法，这里同样是JNI调用，使用了系统提供的零拷贝特性，对应的过程图如下： 从磁盘到网卡，传统的复制过程我们可以看到一共有六次拷贝动作，并且伴随着内核态到用户态的切换，这里有着比较重的CPU以及内存开销。而sendFile零拷贝的过程，我们只需要一次JNI调动，只需要两次拷贝动作（磁盘到内核buffer，buffer再复制到网卡），从次数上减少了四次，并且无需切换用户态。性能大大提高。\n8.自定义ClassLoader Tomcat利用JDK提供的类加载器机制，自定义了一套类加载逻辑（重写loadClass()），核心类结构如下图：\nBootstrapClassLoader：加载 JVM 启动时所需要的核心类，比如rt.jar、resources.jar。 ExtClassLoader：加载\\jre\\lib\\ext目录。 AppClassLoader：加载 classpath。 自定义类加载器，用来加载自定义路径。 在org.apache.catalina.startup.Bootstrap这个启动类中，如下片段可以看到上述结构图中的几个层次成员，均为WebappClassLoader实例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ClassLoader commonLoader = null; ClassLoader catalinaLoader = null; ClassLoader sharedLoader = null; private void initClassLoaders() { try { commonLoader = createClassLoader(\u0026#34;common\u0026#34;, null); if( commonLoader == null ) { // no config file, default to this loader - we might be in a \u0026#39;single\u0026#39; env. commonLoader=this.getClass().getClassLoader(); } catalinaLoader = createClassLoader(\u0026#34;server\u0026#34;, commonLoader); sharedLoader = createClassLoader(\u0026#34;shared\u0026#34;, commonLoader); } catch (Throwable t) { handleThrowable(t); log.error(\u0026#34;Class loader creation threw exception\u0026#34;, t); System.exit(1); } } 创建了几个不同的实例的主要考虑是：各自管理不同的目录（如createClassLoader传入不同的路径配置）。路径关系：\nCommonClassLoader /common/* CatalinaClassLoader /server/* SharedClassLoader /shared/* WebAppClassloader /webapps//WEB-INF/* 而加载逻辑均定义在WebappClassLoaderBase.loadClass(String name, boolean resolve)中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // (0) 检查之前加载过的本地class缓存，Check our previously loaded local class cache clazz = findLoadedClass0(name); // (0.1) 检查之前系统加载过的class缓存，Check our previously loaded class cache clazz = findLoadedClass(name); // (0.2) 尝试用ExtClassLoader加载，Try loading the class with the system class loader, to prevent // the webapp from overriding Java SE classes. This implements // SRV.10.7.2 ClassLoader javaseLoader = getJavaseClassLoader(); // (0.5) Permission to access this class when using a SecurityManager // (1) delegate模式，尝试用AppClassLoader加载，Delegate to our parent if requested clazz = Class.forName(name, false, parent); // (2) 从本地目录查找，Search local repositories clazz = findClass(name); // (3) 尝试用AppClassLoader加载，Delegate to parent unconditionally clazz = Class.forName(name, false, parent); // 抛异常 加载过程 0.2中尝试用ExtClassLoader的逻辑主要是基于安全考虑，部分沿用了双亲委派的逻辑：JRE中的核心类会通过ExtClassLoader委派给BootstrapClassLoader加载，防止了应用中自行加载类名与核心类或者ext中冲突的类。\n而自定义的逻辑中，主要是增加了本地目录、缓存查找的功能。\n隔离机制 Tomcat在线程级别，即Context层提供了隔离功能，即多个web应用使用独立的WebappClassLoader实例进行加载类，这样可以保证多个应用间同个Servlet类同样可以正常加载。\n每个context创建的时候内部线程会进行类加载器绑定：\n1 Thread.currentThread().setContextClassLoader(webApplicationClassLoader); 同样context处理完也会将加载器切换为原线程上下文加载器。\n这个过程我截了下源码的图： 而针对共用类库，比如两个web应用使用到了同个框架，那我们的jar可以共用，通过SharedClassLoader实例管理的\u0026lt;Tomcat \u0026gt;/shared/*目录来存放共用的jar。\n而针对Tomcat与应用的隔离，通过CatalinaClassLoader实例管理的\u0026lt;Tomcat \u0026gt;/server/*目录来专门管理Tomcat本身的类。\n如果是Tomcat与应用共用的类，则通过SharedClassLoader实例管理的\u0026lt;Tomcat \u0026gt;/shared/*目录来共用。\n类加载小结 通过以上设计，Tomcat自定义了各部分类加载的逻辑，通过WebappClassLoader几个实例分别管理了多个目录下class文件的加载，也达到了应用间隔离、容器应用隔离、容器应用共用的设计目的。\n9.热加载与热部署 Ref：\n死磕Tomcat系列(6)——Tomcat如何做到热加载和热部署的 Java服务器热部署的实现原理 ","permalink":"https://redolog.github.io/posts/rd/web/tomcat/essence/","summary":"\u003cp\u003e理解Tomcat主要的架构设计、核心组件设计、功能实现以及性能优化方案，细数我目前对Tomcat掌握的知识。\u003c/p\u003e","title":"Tomcat精粹"},{"content":"为什么Elasticsearch/Lucene全文查询这么快？\n本文要研究的问题：ES/Lucene的索引为什么这么快？\nElasticsearch本质上是基于Lucene增加了分布式高可用管理功能：\n副本 分片 集群 而我们今天要研究的，一段数据查的快不快，指的是数据结构以及索引模块的设计。本文暂不探讨es分布式相关的设计。涉及到的数据结构细节描述会使用其他文章进行分析，本文不做数据结构内部实现、过程的分析。\nES的NRT写入过程 首先我们需要了解es提供的near-real-time近实时查询的写入机制。\n在es单点中，一条数据写入遵循以下流程：\n内存 第一步写入两个位置 写入in-memory-buffer，此时通过get接口可实时拿到单条数据 同时写入translog，这是es中提供的wal事务日志，保证数据的持久性 稍后调用refresh接口写入段文件 磁盘文件 调用flush将translog落盘 调用flush将segment落盘 我们今天要研究的索引数据结构，就是为了定位上述的segment。\n段segment一旦写入就具备不可变性。immutable的优势在于并发操作的时候无需加锁，并且可以常驻内存，但是内存占用过大的时候这里会是一个优化点。数据新增时创建新的段文件，删除时标记状态，延迟实际的删除动作。\n索引设计 全文索引对字段首先要经过分词的处理，也就是把一个字段按照分词的规则拆分成多个词条，前端查询的时候这么多词条都可以命中最原始的写入字段。\n我们先写入几条数据：\nid:1 name:a1 age:11 desc:\u0026lsquo;1这个人住在广州市琶洲村\u0026rsquo; id:2 name:a2 age:22 desc:\u0026lsquo;2住在北京回龙观\u0026rsquo; id:3 name:a3 age:33 desc:\u0026lsquo;3住在上海静安寺\u0026rsquo; 这个结构天然就是一个forward index正排索引，即id直接指向数据行。\n一般RDBMS中的索引分两类：\n聚簇索引 id指向具体行数据 非聚簇索引 索引关键词指向id RDBMS的索引只保留原始的字段值，比如（1、a1、11、1这个人住在广州市琶洲村）。\n根据不同的场景，我们可以使用hash-table、有序数组（二分、跳表）、平衡树来完成底层数据结构的设计。\nES索引经过分词，就会在词条term的维度来创建查询key，比如（1、a1、11、广州市、琶洲村、1这个人住在广州市琶洲村）。\nES索引我们叫它inverted index倒排索引，一个segment就是一个完整的索引，包含三个模块：\nTerm index Term dictionary Posting list 按照上述对RDBMS索引的数据结构设计我们同样可以在ES中实现term-\u0026gt;数据的查询结构。但是为什么ES使用了目前的设计呢？我们来分析一下：\nPosting list 首先Posting list存储了数据的id，拿到id（在这里还有多个词条命中合并的过程）后我们就回到了传统存储系统中查数据的过程。\n这里内部使用了跳表结构，支持快速查询（时间复杂度低），并且可以支持多个列表间做交集操作。\nTerm dictionary ES中就维护了term词条到Posting list的映射关系，这个结构就叫Term dictionary。\n大体上我们可以考虑使用以下数据结构实现Term dictionary：\nmap 数据不需要有序写入。但是不支持范围查询。 有序数组二分 支持范围查询，数据需要有序写入 跳表 支持范围查询，数据同样需要有序，相比数组更吃内存，查询效率高 B+树 MySQL索引选用的结构 ES分词后的查询key可想而知量是很多的，按照上述的几种结构的设计，我们无法直接把Term dictionary常驻于内存。如果落盘，我们倾向于这几个设计原则：\n减少查询磁盘IO 尽量顺序写 遵循数据局部性原理 B+树中通过扩充单个节点上数据的量（n叉）来降低树高度，以此达到了上述1+3点的设计原则。单节点存储更多相邻数据遵循了局部性原理，而控制树高度则有效减少了磁盘寻道次数。\n通过选用适当的数据结构，我们可以降低这里查询的时间复杂度，并且可以减少磁盘寻道次数。\nTerm dictionary实际存储为以block为单位的.tim文件。\nTerm index ES在上面思路基础之上做了加速的设计，即：Term index，也就是为查询哪个（哪些）词条所做的索引。\nTerm dictionary加速了查哪些数据id的过程，Term index加速了查哪些词条term的过程。\n与B+树降低树高度来减少磁盘随机IO的策略类似，ES底层使用了一种特殊的特里树trie来进行term的压缩存储，我们叫他FST Finite Status Transducer。\n我们首先简单看看特里树的特征：\n公用前缀 符合ES多词条多重合的场景 查询时间复杂度O(n)，n表示查询key的字符串长度 时间复杂度很低，查询快 问题：实现消耗内存 而Lucene底层使用的FST本质上是一个图结构，相比trie树，增加了公用后缀的特性，同时，使用压缩技术解决了消耗内存的问题。关于FST的设计解析查看这里：关于Lucene的词典FST深入剖析。\n读者可使用这个工具创建已知词条集合组成的有限状态机树：Build your own FST。\n实际上Term index只会存储term的前缀（再进一步是上述Term dictionary存储block的前缀），效果类似于以xxx开头的数据，一定程度上减少了这部分数据占用的空间，配合FST的压缩设计，使得Term index可以常驻内存。因此内存中直接可以定位Term dictionary的大概位置（.tim文件上的block指针），以此减少磁盘寻道次数。\n综上，从Term index通过FST结构，实现了加速定位Term dictionary的位置（反过来也支持fail fast）。从Term dictionary又是一层索引结构，加速了定位Posting list数据id的过程。\n存储压缩 数据查询过程快是索引设计的一大目标，而另一个目标则是过程中使用的存储结构占用的内存需要尽可能小。需要在内存占用、IO次数、CPU占用之间做权衡。Lucene中关于数据压缩有以下设计实践。\nFOR 关于Posting list中id集合的存储，Lucene从4.1版本开始通过增量编码的方式进行了压缩，术语叫做Frame Of Reference(FOR)。这里我们直接通过图示进行说明：\n数据经过增量拆解、分块（分组block），然后根据最后分好的组内最大值计算二进制最大所需位数，比如图中的227对应二进制11100011则使用8位空间，图中的30对应二进制11110则使用5位空间。因此增量、分块、按需分配内存可有效做到无损压缩、节约内存。\nRoaring bitmaps 对于有序整数id集合（比如filter对应缓存，记录了查询条件到命中docId的映射），Lucene使用咆哮位图Roaring bitmaps进行压缩。\nPosting list存储于磁盘，而这里的缓存则直接常驻内存，所以对应的压缩策略也有所不同。由于是对查询过滤缓存所做的压缩，那么编码解码的速度一定不能慢于多执行一次查询的速度，所以这里的编解码一定要足够简单（因为ES很吃CPU，可能没有太多的CPU分配给编解码的任务，复杂任务就会导致编解码过慢）。\nLucene从5版本开始，最终的方案是结合了short数组+位图：\n数据量少于4096时，使用short（-32768~32767）数组进行存储，此时内存占用比位图更少，遍历使用也很直接 数据量多于4096时，使用bitmap存储，位操作是效率最高的指令（编解码效率最高），存储100M文档数据仅需12.5MB内存，相比int数组使用的400MB缩小到了3.125%的大小 这个方案下，无论是查询匹配的docId还是针对多个查询条件匹配id交集，存储与计算效率都很高。\n这里画个图来更直观地说明过程： 小结 综上，我们了解到es的写入存在近实时的过程，底层文件存储、数据结构使用了Lucene的实现。\n数据查询快，主要是有Term index``Term dictionary``Posting list几个索引模块的加持。而Term index这种常驻内存的结构使用了FOR``roaring bitmap等压缩技术。结果就是省内存、查询数据快。\nRef Elasticsearch: 权威指南 » 基础入门 » 分片内部原理 » 段合并 Elasticsearch: 权威指南 » 管理、监控和部署 » 部署后 » 索引性能技巧 B树、B+树索引算法原理（上） Lucene 倒排索引原理探秘 (1) Apache Lucene - Index File Formats Frame of Reference and Roaring Bitmaps ","permalink":"https://redolog.github.io/posts/rd/storage/es/index-structure/","summary":"\u003cp\u003e为什么Elasticsearch/Lucene全文查询这么快？\u003c/p\u003e","title":"简析ES/Lucene索引的基本设计原理"},{"content":"了解Unix、Linux系统下的进程控制fork函数。\n工作中经常会碰到需要研究一下的问题，而这些问题恰好暴露出自己基础知识的不完备，因此博客中对需要「研究」的知识归类成文。\n定义 Unix/Linux操作系统提供fork函数，用于创建当前进程的子进程。\n返回值 这个函数会返回两次值【区分父子进程】，原因是内核拷贝当前进程形成新的子进程，而在两个进程中都会做出返回动作。\n子进程返回0，pid0这个在内核中分配给了内核交换进程【特殊值】，所以这里的0表示是子进程返回的，调用方在此返回分支下执行子进程逻辑。同时子进程可以在任意地方调用getppid()获取父进程id 父进程返回子进程的id，原因是父进程只能通过这种方式记录子进程id 返回错误 fork调用失败会返回-1，因此调用方需要针对错误进行容错处理。\n调用错误的可能原因：\n进程数超限 内核内存紧张 系统没实现fork 使用的例子 我们看一下Redis中后台生成rdb文件（rdbSaveBackground）中对fork使用的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int rdbSaveBackground(char *filename, rdbSaveInfo *rsi) { pid_t childpid; if ((childpid = redisFork()) == 0) { // 这个分支下处理了子进程逻辑 } else { // 这里则处理父进程，并且判断返回-1的错误情况 if (childpid == -1) { // 错误处理 return C_ERR; } return C_OK; } return C_OK; } 可以看到Redis中是正确处理了fork返回值。\n调用影响 fork调用之后，创建的子进程会拷贝父进程的地址空间，包括：\n堆 栈 数据 一般来说子进程创建之后，内核通过glibc中的exec函数执行拷贝逻辑（基于execve），这个过程不一定会用到这些数据的全部，也不一定是立刻会用到（修改）。所以考虑到这点，内核提供了COW写时复制的机制。\n简单来说copy on write，就是在修改一块数据的时候，我们才复制一份原数据进行修改（延迟、惰性写），这是这个机制的基本思路。这种思路同样可应用在应用系统设计中。\nfork调用后子进程会立即持有一份 指向与父进程相同的物理内存页 的页表（PTE:Page Table Entry），页表相对物理内存来说轻量很多。此时如果物理内存页没有发生变化，页表中会标记为只读状态，而一旦父子进程中有一个需要修改物理内存数据，则触发缺页异常（page fault标识数据一致性需要同步）。\n页表（PTE:Page Table Entry）是一个列表，内部维护了每一虚拟页与物理内存页的映射关系。\n此时内核就执行COW逻辑：\n创建一个新的物理页 拷贝内容到新的物理页 分配父子进程各自的页表 页表中页项状态修改为可写（表示这一页更新成最新状态了） 通过上面过程的描述，fork刚刚调用后，只要对应物理内存没有修改动作，则子进程只需要拷贝轻量级的页表，对于调用方来说，性能大大提升。而物理内存拷贝的动作，发生在数据被父子进程修改的时刻，这里才是性能消耗的时候。\n到此，我们可以考虑下，比如在Redis中，为什么fork有时候会阻塞主进程（线程）呢？考虑Redis大多数时候作为缓存存储服务，前端的请求量是比较高的，那么fork后数据发生了变化（更新动作），主进程对应的某一页数据就发生了变更，此时就进入了COW拷贝动作的逻辑内。如果此时系统配置了大页（比如1GB），又恰好修改的数据命中到这一页时，拷贝的数据量大阻塞时间就会更久。不过互联网缓存业务中，读多写少，因而这种情况发生概率较低。\n因此，我们小结下，fork调用后，子进程立刻持有页表，而拷贝动作的影响大小，取决于父子进程的物理数据是否被修改，这个决定了拷贝动作发生的时刻。\n小结 以上，我们过了一遍fork的定义、处理过程、影响。\n从这一函数内部的设计中，我们可以学习延迟写COW的思路，在读多写少的场景下，COW可以有效减少没必要的数据复制，提高系统性能。\nRef 聊聊并发-Java中的Copy-On-Write容器 Linux环境编程：从应用到内核 UNIX环境高级编程（第3版） ","permalink":"https://redolog.github.io/posts/rd/linux/fork/","summary":"\u003cp\u003e了解Unix、Linux系统下的进程控制fork函数。\u003c/p\u003e","title":"研究一下fork函数"},{"content":"讨论fsync函数细节。\n工作中经常会碰到需要研究一下的问题，而这些问题恰好暴露出自己基础知识的不完备，因此博客中对需要「研究」的知识归类成文。\n背景 在MySQL5.6-8.0版本中，InnoDB RedoLog数据落盘默认使用fsync。MySQL落盘方式调参。\n在Redis的AOF中，也使用fsync进行日志文件落盘。Redis持久化文档。\n在Elasticsearch中使用Translog保证数据的持久性，而Translog默认持久化选项index.translog.durability为request，其语义中也包含了fsync调用。关于ES的Translog模块说明。\n疑问 那么我有一个疑问：Linux提供的fsync会保证数据一定落盘吗？\n函数定义 首先我们先看看fsync的语义。\n根据POSIX中对fsync的定义，在上面这些需要持久化的场景下，调用fsync是为了保证数据在当前处理过程完成前写入到文件中，动机与用法是正确的。\n但在下方rationale原理解释的部分文档也有说明：必要的情况下，具体实现可以有所不同，比如不是NVM（非易失存储）的情况下，fsync无需保证数据一定写入到文件系统中。\n因而到这一步，我们得知，fsync不一定保证数据落盘。\n再进一步 其实在fsync背后有一个write barrier写屏障机制，通过这个机制，我们可以明确fsync落盘的策略，保证将硬件存储的缓存落盘issues a storage cache flush。\nwrite barrier可以保证文件系统元数据正确、有序写入（包括断电的时候）。当然在某些系统的实现上，开启此功能会对性能有所影响。\n结论 通过df -T可查看文件系统格式，而在Linux/ext4文件系统实现上，默认开启了写屏障。所以在此类的服务端上，fsync会保证数据一定落盘。\n","permalink":"https://redolog.github.io/posts/rd/linux/fsync_durability/","summary":"\u003cp\u003e讨论fsync函数细节。\u003c/p\u003e","title":"研究一下fsync函数"},{"content":"译作：了解下DPDK技术基本原理。\nDPDK用户态网络通信。\n全称Data Plane Development Kit，DPDK是在用户空间操作的一项完全开源的技术。其架构支持多商户、异构，并且定位于实现高IO性能以及高网络处理速率（都是网络领域的重要指标）。其被Intel首创于2010年，后于2017年四月划归给了Linux基金会。这一举措将其定位为了Linux顶级项目。本身DPDK是为了电信架构设计的，但是如今却应用在了各个领域，包括不限于云架构、数据中心、基础设施、容器。本文，我们就来探究下2017年八月后发布的DPDK17.08版本的特性以及设计。\n无疑，这个技术是为了实现高速、高性能的网络通信。\n而DPDK为了实现这个大目标有这么几个基本的做法：\nDPDK完全在用户态下进行网络通信，无需切换内核态，少了切换成本。 利用了内存大页机制，比如使用2MB或者1GB的大页，在通信的时候相比Linux标准页（默认为4KB）仅需要更少的页即可完成工作，因此TLB页表快取不命中的概率大大降低了（性能自然就提升了）。 更底层的优化就是代码优化了。比如共享内存缓存行（避免伪共享），这样可以更高效地使用缓存，以及可以提前读缓存，优化策略诸如此类。 DPDK技术近些年变得流行起来，被应用在了很多开源项目中。许多Linux发行版本（Fedora、Ubuntu）增加了内置支持。\nDPDK核心包括了库、驱动（也叫PMD Poll Mode Drivers）。本文撰写时为35个核心库。并且通过接口的形式抽象化了技术能力，将底层实现与其发布屏蔽，这样对于多个厂商来说更加灵活。\nDPDK开发模式 DPDK主要是C写的，其中也有部分Python写的工具。代码贡献、补丁、讨论都通过邮件的方式进行。补丁主要通过RFC的方式获取反馈。为了尽可能保证代码的稳定，我们倾向于尽可能保留ABI二进制接口。开发者们需要严格遵守ABI退出过程，包括需要提前通过邮件列表获取ABI变更的通过。发布页面会标出ABI的变更。如果有一些新功能存疑，但是被合并到了主分支，就会标记为实验性（将来可能会被移除）。举个实验性接口的例子：17年八月的时候，一个新的rte_bus接口被加入。\n同时需要说明：当有新的补丁时，支持多硬件、多厂商的通用接口必须要通过邮件发出来。必须保证至少有一个硬件或者厂商是支持新特性的。\n跟许多其他的开源项目一样，DPDK每年也会开很多会议，通过这种方式可以获取用户、参与者的反馈。除了会议也会有各种在线访问调查。（增强了社区互动性）\nDPDK官网维护了主分支源码，而其他的仓库专注于提供新特性。dpdk-devbind.py脚本使用DPDK协议连接网络、加密设备，还有testpmd是一个多功能工具，提供了转发、监控统计等功能。同时sample目录下提供了五十多个demo（都有完备的文档说明）。\n除了DPDK本身，网站还托管了其他的项目，比如DTS测试组件（一个基于Python的测试框架），内部有着一百多个测试组件。内部还使用了IXIA、Scapy框架，提供功能性、性能测试功能。这部分开箱即用。\nDPDK例常三个月发布一次。这种发布周期可以保证开发团队在比较快的节奏下进行产出，工作流程包括了review、讨论、改进代码。每个正式发布前会有3-5个release candidates发布待定。比如17.08这次发布，一共有125个作者参与，提供了1023个补丁（有来自不同系统的补丁）。长期稳定版LTS会维护两年（偶数年的11月起始）。\n最近的功能特性 过去一年有很多新的功能提供。我们举一个例子，DPDK 17.05中为Intel I40E driver提供的DDP（Dynamic Device Personalization）动态设备定制化功能。\n这个功能支持在I40E硬件上动态使用每个设备的信息。通过ddp add命令运行一个测试，ddp del进行移除。在网络流量波动的时候，可以动态调整profile信息的加载、卸载。这些profile信息是Intel创建的，与用户无关，使用此特性需要精通I40E设备内核知识。\nDPDK以及网络相关的项目 DPDK在许多网络相关的项目上有所应用。简单看几个：\nOpen vSwitch (OvS)：这个项目实现了虚拟网络切换功能 Contrail vRouter：这是一家创业团队开发的SDN控制台 pktgen-dpdk：基于DPDK制作的开源流量生成器 使用DPDK 对于新手用户、开发者，我们建议查看官方文档进行上手。\n小结 本文简单解析了DPDK技术原理、使用、功能特性。DPDK在工业界的使用越发广泛，前途也是一片光明。\n","permalink":"https://redolog.github.io/posts/english/translation/dpdk/","summary":"\u003cp\u003e译作：了解下DPDK技术基本原理。\u003c/p\u003e","title":"【译】用户态网络协议栈之DPDK"},{"content":"Ref 原文：LINUX – IO MULTIPLEXING – SELECT VS POLL VS EPOLL。\n正文 对于Unix（Linux）系统我们有一个基本的设定：系统中的任何对象都是个文件everything in Unix/Linux is a file。每个进程都维护了指向文件、socket、设备以及其他对象的描述指针列表。\nIO资源处理的基本模式：\n资源有一个初始化阶段 接着进入待命模式 等待客户端处理请求、响应 最简单的实现是：为每个客户端都创建一个线程（或者进程），阻塞一直等到请求过来并且把响应发出去。客户端少的时候这个方式是可行的，但是一旦我们想拓展到成百上千请求时，这个方案就很低效了。\nUnix内核中取出一堆文件描述符的机制主要有主流的三种思路：\nselect(2) poll(2) epoll 三个方法目标一致：\n创建一个文件描述符集合 告诉内核每个描述符对应的操作（读还是写） 使用一个线程阻塞在函数调用上，直到有可处理的操作返回 Select系统调用 select()函数实现了同步多播multiplexing I/O。\n1 int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select()调用后会一直阻塞等待，直到文件描述符可以处理此事件，或者超时停止。\n被监听的描述符有三个状态：\nreadfds set监听读事件 writefds set监听写事件 exceptfds set监听异常事件，负责处理异常或者out-of-band带外数据（只有网络socket会有） 上述状态集合可以为NULL，此时select()不做处理。\n事件成功返回后，集合更新状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;wait.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/select.h\u0026gt; #include \u0026lt;sys/time.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define MAXBUF 256 void child_process(void) { sleep(2); char msg[MAXBUF]; struct sockaddr_in addr = {0}; int n, sockfd,num=1; srandom(getpid()); /* Create socket and connect to server */ sockfd = socket(AF_INET, SOCK_STREAM, 0); addr.sin_family = AF_INET; addr.sin_port = htons(2000); addr.sin_addr.s_addr = inet_addr(\u0026#34;127.0.0.1\u0026#34;); connect(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(addr)); printf(\u0026#34;child {%d} connected \\n\u0026#34;, getpid()); while(1){ int sl = (random() % 10 ) + 1; num++; sleep(sl); sprintf (msg, \u0026#34;Test message %d from client %d\u0026#34;, num, getpid()); n = write(sockfd, msg, strlen(msg));\t/* Send message */ } } int main() { char buffer[MAXBUF]; int fds[5]; struct sockaddr_in addr; struct sockaddr_in client; int addrlen, n,i,max=0;; int sockfd, commfd; fd_set rset; for(i=0;i\u0026lt;5;i++) { if(fork() == 0) { child_process(); exit(0); } } sockfd = socket(AF_INET, SOCK_STREAM, 0); memset(\u0026amp;addr, 0, sizeof (addr)); addr.sin_family = AF_INET; addr.sin_port = htons(2000); addr.sin_addr.s_addr = INADDR_ANY; bind(sockfd,(struct sockaddr*)\u0026amp;addr ,sizeof(addr)); listen (sockfd, 5); for (i=0;i\u0026lt;5;i++) { memset(\u0026amp;client, 0, sizeof (client)); addrlen = sizeof(client); fds[i] = accept(sockfd,(struct sockaddr*)\u0026amp;client, \u0026amp;addrlen); if(fds[i] \u0026gt; max) max = fds[i]; } while(1){ FD_ZERO(\u0026amp;rset); for (i = 0; i\u0026lt; 5; i++ ) { FD_SET(fds[i],\u0026amp;rset); } puts(\u0026#34;round again\u0026#34;); select(max+1, \u0026amp;rset, NULL, NULL, NULL); for(i=0;i\u0026lt;5;i++) { if (FD_ISSET(fds[i], \u0026amp;rset)){ memset(buffer,0,MAXBUF); read(fds[i], buffer, MAXBUF); puts(buffer); } }\t} return 0; } 示例中我们创建了五个子进程，每个进程连接到服务端并且发送消息。服务端进程使用accept(2)为每个客户端创建一个不同的文件描述符。select(2)中的第一个参数是三个集合中最大的数字。\n主循环中创建一组文件描述符，调用select(2)有结果返回时检查是否可读。这里为了简化代码未做异常检查。\n有结果返回的时候，select改变了集合，只包含就绪的描述符。因此每次迭代我们需要重置set。\n这里我们需要告诉select函数最大的文件描述符数字的原因是fd_set内部实现决定的。每个文件描述符声明为一位bit，所以fd_set是个int32的数组。函数内判断当前集合是否达到了最大值。比如我们有五个文件描述符但是最大值是900，函数就会监听0-900的任意位。POSIX(可移植操作系统接口)中提供了pselect这个选项，这个等待的时候增加了一个信号掩码。\n小结select 每次调用前需要创建描述符集合 函数会检查最大值 我们需要遍历每个文件描述符，以此来检查是否有数据就绪待处理 select主要的优势是可移植，每个Unix系统上都有对应实现 Poll系统调用 跟select()中低效的三位掩码文件描述符集合不同，poll()使用了一个nfds pollfd的数组结构，方法定义更加简洁：\n1 int poll (struct pollfd *fds, unsigned int nfds, int timeout); pollfd结构中定义了不同事件的字段以及响应事件的字段：\n1 2 3 4 5 struct pollfd { int fd; short events; short revents; }; 我们使用的时候也很简单，为每个fd都创建一个pollfd对象，放到对应事件中，返回时检查对应事件对象。代码实例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 for (i=0;i\u0026lt;5;i++) { memset(\u0026amp;client, 0, sizeof (client)); addrlen = sizeof(client); pollfds[i].fd = accept(sockfd,(struct sockaddr*)\u0026amp;client, \u0026amp;addrlen); pollfds[i].events = POLLIN; } sleep(1); while(1){ puts(\u0026#34;round again\u0026#34;); poll(pollfds, 5, 50000); for(i=0;i\u0026lt;5;i++) { if (pollfds[i].revents \u0026amp; POLLIN){ pollfds[i].revents = 0; memset(buffer,0,MAXBUF); read(pollfds[i].fd, buffer, MAXBUF); puts(buffer); } } } 跟select使用时一样，我们需要检查每个pollfd对象看是否有数据就绪，但是每轮迭代我们不需要创建fd集合了。\nPoll对比Select poll不需要用户这端维护当前读到的描述符指针数，fd+1 poll在处理更多fd的时候更加有效率。 select的fd集合时静态声明大小的 poll入参使用数组传递，内部对象可以重用，无需每次创建 select的超时参数在返回的时候是未定义的，我们需要自己编码来重新初始化它。而pselect则没有这个问题 select更加轻便，因为有些系统不支持poll Epoll系统调用 使用select与poll的时候，我们在自己的代码中管理状态，并且我们需要在等待调用的时候需要传递fd集合。如果需要添加一个新的socket事件，我们就需要把它传递给fd集合并且再调用一次select或者poll。\n而Epoll可以帮助我们创建、管理内核上下文。我们把一个任务的步骤分成三步：\n使用epoll_create创建一个内核上下文 使用epoll_ctl将fd（文件描述符）从上下文添加或者移除 使用epoll_wait等待事件 epoll的代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 struct epoll_event events[5]; int epfd = epoll_create(10); ... ... for (i=0;i\u0026lt;5;i++) { static struct epoll_event ev; memset(\u0026amp;client, 0, sizeof (client)); addrlen = sizeof(client); ev.data.fd = accept(sockfd,(struct sockaddr*)\u0026amp;client, \u0026amp;addrlen); ev.events = EPOLLIN; epoll_ctl(epfd, EPOLL_CTL_ADD, ev.data.fd, \u0026amp;ev); } while(1){ puts(\u0026#34;round again\u0026#34;); nfds = epoll_wait(epfd, events, 5, 10000); for(i=0;i\u0026lt;nfds;i++) { memset(buffer,0,MAXBUF); read(events[i].data.fd, buffer, MAXBUF); puts(buffer); } } 首先我们创建了epfd上下文（参数必须为正数）。当有客户端连接时，我们创建epoll_event对象，并且把它丢到上下文中，在主循环中我们等待其返回响应。\nEpoll vs Select/Poll 我们在等待的时候可以添加或者移除fd epoll_wait只会返回就绪的fd epoll有更好的性能：O(1) epoll只有Linux实现 小结 可以看到epoll的实现更加高效，用户端使用也更加便捷。\n三者作为操作系统不同的IO多路复用的实现，可以总结为以下区别与联系。\n区别： 连接数量限制： select：在大多数系统中，最大可同时监控的文件描述符数量有限制，通常为1024（取决于系统配置）。 poll：理论上，poll没有固定的最大文件描述符数量限制，但受限于系统资源和内存大小。 epoll：同样没有固定的最大文件描述符数量限制，只受限于系统内存。 数据结构和效率： select：使用一个由用户空间传递到内核空间的固定大小的数组来存储需要监控的文件描述符集合，每次调用都需要进行全集扫描，效率较低。 poll：与select类似，但使用链表存储文件描述符，避免了固定大小数组的限制，但仍需要全集扫描。 epoll：引入了水平触发和边缘触发两种模式，并且内核维护了一个红黑树来存储文件描述符， epoll_wait() 只返回就绪的文件描述符，不需要全集扫描，大大提高了效率。 通知机制： select 和 poll：在轮询过程中，需要遍历所有文件描述符来检查是否有事件发生。 epoll：提供了 epoll_ctl() 函数，可以注册和修改对特定文件描述符的关注事件，当有事件发生时，epoll会通过回调函数或者返回就绪的文件描述符来通知用户空间，减少了不必要的上下文切换。 操作方式： select 和 poll：都需要在每次调用时将全部关注的文件描述符集合传递给系统调用。 epoll：只需要在初始化时创建一个epoll实例，然后通过epoll_ctl()添加或删除关注的文件描述符，epoll_wait()则用于等待事件发生。 联系： 它们都是为了实现I/O多路复用，提高网络编程的效率和性能。 都支持水平触发（Level Triggered，只要有数据可读/写，就会一直通知）模式。 在Linux系统中，epoll是作为select和poll的替代品出现的，旨在解决它们在处理大量并发连接时的效率问题。 总的来说，epoll在处理大量并发连接和高负载场景下具有更高的性能和效率，而select和poll更适合连接数量相对较小的情况。\n类比 本质上我对IO这个场景可以类比到饭店服务上。\n我们将一个饭店的工作人员细化分开：\n迎客员（两个） 服务员（三个） 厨师（十个） 迎客员就是做了IO事件监听的事，只需要少量人员即可完成事件的循环、监听动作，而服务员的任务相对重一点，耗时多一点，而我们用户的程序则是厨师的角色，用于响应IO处理，做菜耗时比服务点菜、迎宾要多，所以在顾客多的时候（IO多）我们的厨师需要多一点，但是迎客员只需要少量即可应付任务。\n不同的角色分开，是职责上解耦的设计，厨师不必一个人顾及前端IO的事件状态变化，可以更专注的后端逻辑的处理。\n看了多路复用，可以多对比一下AIO。\n","permalink":"https://redolog.github.io/posts/english/translation/linux_io_multiplexing/","summary":"Ref 原文：LINUX – IO MULTIPLEXING – SELECT VS POLL VS EPOLL。 正文 对于Unix（Linux）系统我们有一个基本的设定：系统中的任何对象都是个文件everyt","title":"【译】简析Linux IO多路复用模型"},{"content":"原文：All about Linux swap space\nLinux将物理RAM内存分割成不同的内存块，我们把这个块叫做页page。而交换swapping这个动作发生在将一页内存拷贝到提前配置好的硬盘区域，这块区域就叫交换区swap space，以此来释放内存页的占用。物理内存+交换区=可用虚拟内存大小。\n优点 swapping交换有两个存在的必要：\n当系统需要比可用物理内存更大的空间时，内核换出一些用完的页，以此给当前这个应用分配更多的内存。 有的程序初始化的时候回用到大量的页，但是后来这些空间都不再使用了。内核会把这些页换出，以此为其他需要的程序（或者是硬盘缓存disk cache）提供内存。 缺点 但是，交换区也会有出问题的时候。与内存相比，磁盘读写是比较慢的。一般内存速度以纳秒计，而磁盘需要到毫秒，所以访问磁盘会很慢。当交换越多，系统也就越慢（因为在读写磁盘）。\n当一个交换变慢，交换动作变多，会进入恶性循环，越交换系统越慢可用内存越少越需要交换。这种时候系统忙于寻找可用内存以此来保证应用运行。这种情况下，只能增加RAM配置。加机器加配置大法好！\nLinux中有两种交换区：\n交换分区 硬盘中一个单独的区域，仅供交换使用，其他文件无法访问 交换文件 一种特殊的系统、应用文件 使用swapon -s可查看当前的交换区状况：\n1 2 Filename Type Size Used Priority /dev/sda5 partition 859436 0 -1 可通过fdisk管理交换分区。\n先确保分区已标记为交换分区 然后创建文件区 多大合适？ 如果RAM够大，理论上可以不需要交换区。但是内存用尽的时候系统会崩溃。\n老版本的Unix系统需要物理内存的2-3倍配置。现代系统实现不需要那么多。大致原则如下：\n桌面系统，配置两倍于物理内存的大小，这种情况允许我们启动一些大应用（空闲的时候内存可以被换出），这样对使用中的应用的RAM使用更有效。 服务端，配置一半于物理内存的大小，这样当需要更多内存的时候可以更灵活换出。但是同时也需要进行监控，必要时升级RAM。 结论 管理好交换区是系统管理很重要的一点。好的规划与正确使用，可以有更好的效果。当然配置不是固定的，要敢于测试使用，动态调整自己的配置策略。\n","permalink":"https://redolog.github.io/posts/english/translation/all_about_linux_swap_space/","summary":"原文：All about Linux swap space Linux将物理RAM内存分割成不同的内存块，我们把这个块叫做页page。而交换swapping这个动作发生在将一页内存","title":"Linux中的swap空间"},{"content":"Linux内核文档：What is NUMA?。\n关于NUMA架构是什么的问题，可以同时从硬件、软件的不同角度进行解释。\n硬件角度 从硬件来看，NUMA系统是一个组合了多个内部包含多核CPU、本地内存、以及IO总线组件的计算机平台。为了简短表达以及防止歧义，我们在文档中称这些组件为cells。\n每一个cells都可以看做是NUMA系统下的一个SMP（对称多处理器）子集，同时要注意，有些独立的SMP系统并不位于任何cell上。NUMA系统下的cell通过连接组件连接彼此，比如交叉开关或者点对点连接。这些类型的连接组件都可以进行组合用于创建与其他cell的连接。\n在Linux系统上，NUMA主要是指Cache Coherent NUMA 缓存一致NUMA，简称ccNUMA。在系统内部，只要CPU与cell连接，所有内存都是可见并且可访问的。同时，缓存一致性问题会被缓存处理器以及各个连接组件处理。\n内存访问时长与有效带宽因CPU访问的不同cell间的距离不同而不同。比如访问同个cell下的内存会明显快于离得远的不同的cell间的内存。NUMA平台架构允许多种距离组合的cells。\n硬件平台厂商不直接实现NUMA架构，让软件实现的部分更加有意思了。准确来说，NUMA架构是为了提供一种可拓展的内存带宽。为了达到这个目标，操作系统以及软件就必须要让大部分内存引用都位于本地cell的部分（local memory），或者说，越近越好。\n软件角度 上面的描述正好引入了软件部分的视角：\nLinux把系统硬件资源都抽象为了软件中的nodes，硬件物理核（physical cells）与nodes间形成mapping映射，这层抽象屏蔽了一些架构上的细节。软件中的nodes（可对应到物理核）这时候就能对应到0-多个CPU、内存、IO设备上。并且，访问离得近的nodes（其实是映射到离得近的cells（物理核））就会比访问远程cells更快（时长更短、带宽更有效）。\n比如在X86架构上，Linux会把一些没有内存分配的node（映射到物理核cell上）给隐藏起来，同时会二次分配，把CPU分配到有内存资源的node上。因此，在这种架构下，我们可以看到，分配到某个node上的不同CPU，有可能有着不同的本地内存访问时间与带宽效率。\n除此之外，还是以X86为例，Linux支持附加nodes的模拟。Linux会划分已有的nodes（或者是非NUMA系统的内存）到更多的nodes上。每个模拟出来的node都管理者底层cells物业内存的部分。当测试非NUMA架构上的NUMA内核与软件特性时，这个模拟非常实用。并且跟cpusets一起使用时，这是一种内存资源管理机制。\n针对每个带内存的node，Linux有一套独立的内存管理子系统，里面包含了free page lists, in-use page lists, usage statistics and locks to mediate access。另外，Linux把每个内存区域（DMA, DMA32, NORMAL, HIGH_MEMORY, MOVABLE）都放到一个叫zonelist的结构里。一个zonelist定义了当选中的zone、node不满足分配请求时可二次访问的zones或者nodes资源。这种情况，我们称之为溢出或者退路。\n因为有的nodes中包含了多种内存资源的zones，所以Linux需要决策：不同node但是相同zone类型碰到退路分支时（或者同个node上要分配不同的zone类型），是否要对zonelist进行排序。对于DMA``DMA32这类稀缺资源来说，这是很重要的策略。Linux默认给定一个有序的zonelist。在连接到远程node之前，系统会自动帮你找同个node上的不同zone，以NUMA中的距离排序，先找近的再找远的。\n这个内存分配查找的过程大概是这样：\nLinux默认先找执行本次请求的CPU资源。\n先本地分配：先尝试分配请求源zonelist中的第一个node资源。\n如果本地分配不成功，内核会按照顺序（NUMA距离排序）找list中最近满足要求的node。\n本地分配倾向于使得后续请求都访问本地的物理资源，并且倾向于远离系统连接组件（这一步需要保证分配的内存后续没有发生migrate动作）。Linux的调度器这里使用NUMA拓扑图策略，可访问[see Documentation/scheduler/sched-domains.txt]，调度器的目标是尽可能减少远程调度（task migration）。但是，调度器不能直接拿到一个任务的NUMA细节数据。因此，在足够多的不平衡发生时，多个任务会在nodes间、从原始node到远程node进行migrate。\n系统管理员可以限制上述这个migration的发生，以此来提升NUMA架构下的本地效率。比如可使用CPU affinity cli：\ntaskset(1) numactl(1) 也可以使用程序接口：\nsched_setaffinity(2) 当然也可以修改内核默认分配策略的配置，可参考： [see Documentation/admin-guide/mm/numa_memory_policy.rst.]\n我们也可以通过使用control groups以及cpusets来限制非特权用户所使用的CPU以及nodes内存。参考：[see Documentation/cgroup-v1/cpusets.txt]\n在不隐藏无内存node的架构上，Linux在zonelist中只保留有内存的zone（node）资源。这也就意味着在一个CPU对应的zonelist中，本地内存第一个node节点将不是它本身，而是创建zonelist时内核选中的有内存的最近的node。因此默认逻辑是，本地分配的结果是分配一个最近可用的内存块。发生退路、溢出等异常情况时，选择逻辑依旧是选当前离得最近的node。\n部分内核分配的情况不适用这种退路逻辑（比如当子系统在每个CPU内分配内存资源）。相反，他们想要确定：\n当前分配发生在特定node上 同时可以拿到分配失败的结果（分配的node没有内存了） 一个例子：\n通过numa_node_id() CPU_to_node()获取当前CPU关联的nodeId 接着通过上述nodeId获取对应内存 当上面这种分配失败的时候，可以退到自定义的逻辑内。\n","permalink":"https://redolog.github.io/posts/english/translation/what_is_numa/","summary":"Linux内核文档：What is NUMA?。 关于NUMA架构是什么的问题，可以同时从硬件、软件的不同角度进行解释。 硬件角度 从硬件来看，NUMA","title":"什么是NUMA架构？"},{"content":"原文：Storing hundreds of millions of simple key-value pairs in Redis 。\n背景：本文讲的是Instagram团队2011年时碰到的一个技术场景。新旧系统迁移，需要做一些脚手架的工作，其中有一项是需要缓存userId与图片之间的关系，有这么几个要求：\n根据key查value，要快 内存占用不能太大，要节省机器资源，参考当时EC2高配版本（17或者34GB） 适用于当前基础架构、设施 数据持久化，服务端down掉数据不能丢 如果用RDBMS来存储，编码层面是简单的，但是考虑数据使用的场景，使用RDBMS未免有些不合适：\n数据只有写入动作，没有更新动作 我们也不需要事务 与其他维度的数据也没有关系需要存储 考虑至此，我们想到了Redis，当时在Ins团队，feed流已经在大范围使用Redis了。相比Memcached，Redis的优势：\n提供了更丰富的数据类型，比如zset、list 并且支持配置持久化 并且支持主从，从节点可用来支持更加重的备份、大批量任务 最开始的时候，我们尝试了使用mediaId作为key，对应的userId作为value：\n1 2 3 SET media:1155315 939 GET media:1155315 \u0026gt; 939 发现存储1,000,000条数据会消耗70 MB内存，推断下我们总共的300,000,000数据量，对应的内存去到了21GB，这个已经超出了EC2的17GB容量。\n于是我们咨询了Redis的核心开发者Pieter Noordhuis，他建议我们使用Hash结构。底层可以使用zipmap（现在已经是ziplist了）进行编码压缩，对应配置项hash-zipmap-max-entries（现在则对应hash-max-ziplist-entries），参数含义为在配置条数内的情况下，hash内的数据会进行压缩。经过测试我们把该参数设为1000，过高的配置会引起更多的CPU消耗。\n为了利用这个特性，我们将mediaId进行分桶处理，按照数据总量除以1000（保证每个分桶内存数据量不超过1000），这里测试的数据总量为1155315，也就是我们会有1155 (1155315 / 1000 = 1155)个bucket：\n1 2 3 HSET \u0026#34;mediabucket:1155\u0026#34; \u0026#34;1155315\u0026#34; \u0026#34;939\u0026#34; HGET \u0026#34;mediabucket:1155\u0026#34; \u0026#34;1155315\u0026#34; \u0026gt; \u0026#34;939\u0026#34; 内存占用的区别相当明显了，我们使用1,000,000个key，放到不同的bucket中，每个key对应的hash量不超过1000，Redis用了16MB就搞定了。使用这种办法拓展到3亿个key，一共使用5GB，也就是我们使用更便宜的m1实例就可以搞定这个需求。\n这个办法让我们保持了O(1)的查找效率，同时让我们省了至少2/3的成本。\n小结 虽然是2011年的老文章，对应Redis中的某些术语都有过时的表述，但是思路上对我的启发还是挺大的。\n我们很多人都了解过Redis的设计，比如知道不同的数据结构对应到Redis底层的实现，但是在应用层的设计却不一定能做到如此巧妙，所以学到技术原理还不够，落实到具体的方案设计上才是关键。\n","permalink":"https://redolog.github.io/posts/english/translation/storing_hundreds_of_millions_of_simple_key_value_pairs_in_redis/","summary":"原文：Storing hundreds of millions of simple key-value pairs in Redis 。 背景：本文讲的是Instagram团队2011年时碰到的一个技术场景。新旧系统迁移，需要做一些脚手架","title":"译---在Redis中存储亿万级的简单KV数据"},{"content":"第一次来长沙还是2014年的时候，彼时我还不怎么会编程（那时候想做PM而不是RD），对计算机的理解还过于肤浅。但是2014年认识了几个对我之后几年起了关键作用的朋友，其中一位，我们的第一次会面，就是在长沙。\n这一次来长沙，主要的原因是冀哥目前在这里工作，剩下几个人，一个北漂（赟）一个沪漂（雒）一个广漂（我）。\n此行我一共拍了183张照片，选取几张留作纪念。\n长沙此行一共拍了183张图 爬了爬岳麓山 海信超级文和友账单 海信超级文和友商场入口招牌 来长沙一定要喝茶颜悦色 祝各位老友前程似锦 🙏 长沙菜连虾都是辣的 此行发觉，大家在不同的环境下经历了不同的生活，思想上也生出了各自的分支，向各位学习。\n与老友相聚，找一找旧时的感觉，品味一番长沙当地的特色菜，足矣。\n长沙，再会。\n","permalink":"https://redolog.github.io/posts/note/changsha20201213/visit/","summary":"第一次来长沙还是2014年的时候，彼时我还不怎么会编程（那时候想做PM而不是RD），对计算机的理解还过于肤浅。但是2014年认识了几个对我之","title":"20201213长沙老友相聚"},{"content":"踩坑Apollo配置namespace加载顺序优先级问题，更具体点应该表述为【踩坑Spring框架中针对多配置属性源取值的逻辑】。\n背景 SprintBoot项目中使用了携程开源的Apollo组件完成分布式配置功能。其中有一个common包负责公用一些基础配置，工程特有的配置通过namespace进行区分。\n配置项长这样：\n1 2 apollo.bootstrap.enabled = true apollo.bootstrap.namespaces = common,application 问题表现： 在管理界面修改namespace为application的某一个项时，发现配置不生效。\n定位 去找了下中文文档，针对Java客户端使用，集成到Spring的描述中针对此项有具体解释：注入多个namespace，并且指定顺序。\n此处规则很明确：在前面的namespace优先级最高，谁在前面哪个值生效。\n源码解析 Apollo中通过ApolloApplicationContextInitializer这个类implements了Spring框架中的ApplicationContextInitializer类，内部实现了initialize(ConfigurableApplicationContext context)方法。这里负责把各命名空间中的配置项加载到java对象（PropertySources）中，交给spring管理起来。\n我们详细看看initialize(ConfigurableEnvironment environment)方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 /** * 环境变量初始化动作 * * @param environment */ protected void initialize(ConfigurableEnvironment environment) { if (environment.getPropertySources().contains(PropertySourcesConstants.APOLLO_BOOTSTRAP_PROPERTY_SOURCE_NAME)) { // 这里判断apollo启动配置中是否已注入 ApolloPropertySources，fail-fast逻辑 return; } // 从 apollo.bootstrap.namespaces 中取配置的命名空间，默认使用 application。此处其实已经表明下游自定义的项应该配在application空间中，但是要保证namespace顺序application在前面。 String namespaces = environment.getProperty(PropertySourcesConstants.APOLLO_BOOTSTRAP_NAMESPACES, ConfigConsts.NAMESPACE_APPLICATION); logger.debug(\u0026#34;Apollo bootstrap namespaces: {}\u0026#34;, namespaces); List\u0026lt;String\u0026gt; namespaceList = NAMESPACE_SPLITTER.splitToList(namespaces); CompositePropertySource composite = new CompositePropertySource(PropertySourcesConstants.APOLLO_BOOTSTRAP_PROPERTY_SOURCE_NAME); for (String namespace : namespaceList) { // 每个命名空间会对应到管理界面中配置的数据，java中使用 Config 表示这些配置 Config config = ConfigService.getConfig(namespace); // 通过命名空间与config对象拿到PropertySource（可遍历的），添加到apollo启动配置对象中。 composite.addPropertySource(configPropertySourceFactory.getConfigPropertySource(namespace, config)); } // 将组装好的组合配置源扔进spring的环境对象中，置于首位 environment.getPropertySources().addFirst(composite); } 我们需要关注下CompositePropertySource.addPropertySource(PropertySource\u0026lt;?\u0026gt; propertySource)方法：\n1 2 3 4 5 public void addPropertySource(PropertySource\u0026lt;?\u0026gt; propertySource) { this.propertySources.add(propertySource); } // 而这里的propertySources可以观察到是一个链表结构，保证了有序。 private final Set\u0026lt;PropertySource\u0026lt;?\u0026gt;\u0026gt; propertySources = new LinkedHashSet\u0026lt;\u0026gt;(); propertySources在spring中表示多配置属性源，而这个initialize(ConfigurableEnvironment environment)本质上就是把我们在Apollo界面中配置的数据加载到spring中了。到这里，数据都加载了进来。\n多个命名空间的配置配置源封装到了名为ApolloBootstrapPropertySources的CompositePropertySource中。根据上面注释说明，propertySources添加子项的时候通过LinkedHashSet保证了有序。\n而获取某个key对应属性值的方法是：\n1 2 3 4 5 6 7 8 9 10 11 12 @Override @Nullable public Object getProperty(String name) { for (PropertySource\u0026lt;?\u0026gt; propertySource : this.propertySources) { // 有序的集合中进行迭代，找到就返回，因此先添加进来的 PropertySource 会作为最终的值 Object candidate = propertySource.getProperty(name); if (candidate != null) { return candidate; } } return null; } 小结 所以我们当时那个工程中关于apollo.bootstrap.namespaces的配置应该改为application,common。\n而这个实现的逻辑位于Spring中对于环境配置源的CompositePropertySource中，其中通过链表添加多个命名空间对应的PropertySource对象，获取同个key值时，取最先加入的命名空间对应的配置。\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/apollo/namespace-priority/","summary":"\u003cp\u003e踩坑Apollo配置namespace加载顺序优先级问题，更具体点应该表述为【踩坑Spring框架中针对多配置属性源取值的逻辑】。\u003c/p\u003e","title":"Apollo命名空间加载优先级"},{"content":"踩坑WebService接入外部非法数据（非打印字符）。\n由于hugo渲染的时候会检查标题title内的非打印字符，所以这里把带有非打印字符的标题写到正文中。 标题：晋\u0008西 != 晋西 ？？？ hugo渲染报错：Error: Error building site: \u0026ldquo;/Users/dragonsong/Downloads/code-repo/dragonsong031100/content/posts/rd/troubleshoot/invalid_nonprinting_char.md:2:1\u0026rdquo;: failed to unmarshal YAML: yaml: control characters are not allowed\n背景 19年开始我作为一个数据同步服务的owner开始整合各种外部数据接入的架构落地、编码设计。其中有一个客户提供老式的WebService协议的接口，我开发了一个针对该协议的agent客户端，负责把数据拉过来、同步（处理后置逻辑）。\n听起来很简单的过程：调用外部接口，拿到数据就可以扔给后面的服务。\n我们在前置的dev、sit环境都经过了测试，完全没有问题，接下来就往prod环境推。\n一推，就出问题了。\n使用了cxf框架处理协议的逻辑，框架内报了下面这个错误：\n:[failed to localize] Failed to deserialize the response.(javax.xml.bind.UnmarshalException - with linked exception: [com.ctc.wstx.exc.WstxUnexpectedCharException: Illegal character ((CTRL-CHAR, code 8)) at [row,col {unknown-source}]: [1,169325]]) 定位 首先我当时考虑，一样的程序（检查下仓库分支、发布在不同环境使用的hash码）在前置环境未出问题，那么不同的环境下有哪些东西是不一样的呢？（这种情况肯定是变化的内容导致了问题）\n机器环境不同 但是我们这个程序只是作为客户端，机器环境只负责将应用跑起来，并不会影响这里拉数据的运行时状态 数据不同 这点取决于外部，分析下来也只有这里有问题 其次，顺着异常栈，找一下cxf中抛异常的逻辑。\n而其源码中的抛错逻辑很明确：碰到ctrl-char就直接抛出异常。目的是排除这类非法数据，在编写本文时，hugo中也有类似逻辑（不允许我直接在标题中使用非打印字符）。\n当时我定位问题的时候走了一些弯路，标题中的「晋\u0008西 != 晋西 ？？？」是数据中的字符，而我当时完全忘记了有非打印字符这回事，我尝试把接口拿到的数据在本地打开，使用ctrl+f匹配字符时才发现，明明我两只眼睛都看到了晋西这两个字，但是就是匹配不上。明白原因之后才恍然大悟，中间有一个\\b非打印字符，所以开始用眼睛来查问题，查错了方向。\n此次之后，我们一定要铭记非打印字符的存在，这些字符用眼睛是看不到的，切忌用错误的方式来排查问题。\nJava中使用Character.isISOControl(char)进行判断。\n关于ctrl-char的范围、概念可以参考这里：List of Unicode Characters of Category “Control”。\n解决 web相关的框架都提供了拦截器机制，我们利用cxf提供的这类机制自定义一个FilterCtrlCharInterceptor，内部方法将此类非法字符排除。\n其中排除的逻辑可以用正则直接匹配替换：\n1 string.replaceAll(\u0026#34;([\\b|\\f|\\n|\\r|\\t])\u0026#34;, \u0026#34;\u0026#34;) 也可以直接使用guava库：\n1 2 3 CharMatcher.JAVA_ISO_CONTROL.removeFrom(string); CharMatcher.javaIsoControl() 小结 本文通过经验判断、异常栈分析的方式排查了cxf框架中数据反序列化的异常检查问题。解决方式非常简单，直接正则匹配替换非法字符即可。\n同时，我们可以控制的数据范围内，一定要有主动排除非法字符的逻辑，防止这类数据写库。\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/invalid_nonprinting_char/","summary":"\u003cp\u003e踩坑WebService接入外部非法数据（非打印字符）。\u003c/p\u003e","title":"晋西 != 晋西 ？？？"},{"content":"Golang笔记。\nRef https://github.com/redolog/hello-go 语法 basics if 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import \u0026#34;fmt\u0026#34; func main() { var number int = 5 if number += 6; 10 \u0026gt; number { number += 3 fmt.Print(number) } else if 10 \u0026lt; number { number -= 2 fmt.Print(number) } fmt.Println(number) } defer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) func main() { bytes, err := readFile(\u0026#34;README.md\u0026#34;) if err != nil { panic(err) } for _, v := range bytes { fmt.Println(v) } deferIt() deferIt2() deferIt3() deferIt4() for i := 0; i \u0026lt; 10; i++ { defer func(n int) { // 后执行 fmt.Printf(\u0026#34;%d \u0026#34;, n) }(func() int { // 调用即求值 n := fibonacci(i) fmt.Printf(\u0026#34;%d \u0026#34;, n) return n }()) } } func readFile(path string) ([]byte, error) { file, err := os.Open(path) if err != nil { return nil, err } defer file.Close() return ioutil.ReadAll(file) } func deferIt() { defer func() { fmt.Print(1) }() defer func() { fmt.Print(2) }() defer func() { fmt.Print(3) }() fmt.Print(4) } func deferIt2() { for i := 1; i \u0026lt; 5; i++ { defer fmt.Println(i) } } func deferIt3() { f := func(i int) int { fmt.Printf(\u0026#34;%d \u0026#34;, i) return i * 10 } for i := 1; i \u0026lt; 5; i++ { // Possible resource leak, \u0026#39;defer\u0026#39; is called in a for loop less... (⌃F1) // Inspection info: Reports defer statements inside loops. // 调用匿名函数的时候就已经进行了内部的执行 defer fmt.Printf(\u0026#34;%d \u0026#34;, f(i)) } } func deferIt4() { for i := 1; i \u0026lt; 5; i++ { // 语句携带的表达式语句中的那个匿名函数包含了对外部（确切地说，是该defer语句之外）的变量的使用。 // 注意，等到这个匿名函数要被执行（且会被执行4次）的时候，包含该defer语句的那条for语句已经执行完毕了。此时的变量i的值已经变为了5 defer func() { fmt.Println(i) }() } } func fibonacci(num int) int { if num == 0 { return 0 } if num \u0026lt; 2 { return 1 } return fibonacci(num-1) + fibonacci(num-2) } error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) func innerFunc() { fmt.Println(\u0026#34;Enter innerFunc\u0026#34;) defer withRecover() panic(errors.New(\u0026#34;Occur a panic!\u0026#34;)) fmt.Println(\u0026#34;Quit innerFunc\u0026#34;) } func outerFunc() { fmt.Println(\u0026#34;Enter outerFunc\u0026#34;) innerFunc() fmt.Println(\u0026#34;Quit outerFunc\u0026#34;) } func main() { fmt.Println(\u0026#34;Enter main\u0026#34;) outerFunc() fmt.Println(\u0026#34;Quit main\u0026#34;) } func readFile(path string) ([]byte, error) { file, err := os.Open(path) if err != nil { return nil, err } defer file.Close() return ioutil.ReadAll(file) } func withRecover() { if p := recover(); p != nil { fmt.Printf(\u0026#34;error msg: %s\u0026#34;, p) } } for 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import \u0026#34;fmt\u0026#34; func main() { for i := 0; i \u0026lt; 10; i++ { fmt.Println(i, \u0026#34; \u0026#34;) } // 遍历字符串 for i, v := range \u0026#34;Go语言\u0026#34; { fmt.Printf(\u0026#34;%d: %c\\n\u0026#34;, i, v) } map1 := map[int]string{1: \u0026#34;Golang\u0026#34;, 2: \u0026#34;Java\u0026#34;, 3: \u0026#34;Python\u0026#34;, 4: \u0026#34;C\u0026#34;} for i, v := range map1 { fmt.Printf(\u0026#34;%d: %s \\n\u0026#34;, i, v) } } goroutine 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;sync\u0026#34; ) func main() { gosched() waitGroup() } func waitGroup() { // 异步转同步，类似于Java中的CountDownLatch var wg sync.WaitGroup wg.Add(3) go func() { fmt.Println(\u0026#34;Go!\u0026#34;) wg.Done() }() go func() { fmt.Println(\u0026#34;Go!\u0026#34;) wg.Done() }() go func() { fmt.Println(\u0026#34;Go!\u0026#34;) wg.Done() }() wg.Wait() } func gosched() { go fmt.Println(\u0026#34;Goroutine!\u0026#34;) // 让当前正在运行的Goroutine（这里是运行main函数的那个Goroutine）暂时“休息”一下 runtime.Gosched() } interface 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import \u0026#34;fmt\u0026#34; type Animal interface { Grow() Move(start string) string } type Cat struct { Name string Age uint8 Gender string Start string End string } func (cat *Cat) Grow() { cat.Age++ } func (cat *Cat) Move(start string) string { fmt.Printf(\u0026#34;cat move from %s name: %s\u0026#34;, start, cat.Name) return \u0026#34;cat move result\u0026#34; } func main() { myCat := Cat{Name: \u0026#34;Little C\u0026#34;, Age: 2, Gender: \u0026#34;Male\u0026#34;} animal, ok := interface{}(\u0026amp;myCat).(Animal) fmt.Printf(\u0026#34;%v, %v\\n\u0026#34;, ok, animal) } // func(dog *Dog) Name() string{ // return dog.Name //} //func(dog *Dog) Age() uint8{ // return dog.Age //} iota 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import \u0026#34;fmt\u0026#34; func main() { // iota在const关键字出现时将被重置为0 const cons1 = iota const cons2 = iota fmt.Println(cons1) fmt.Println(cons2) // const中每新增一行常量声明将使iota计数一次 const ( cons3 = iota // 跳值 _ // 插值 cons4 = 666 _ cons5 = iota + 2 // 隐式赋值 cons6 cons7 cons8, cons9 = iota, iota + 2 cons10, cons11 ) fmt.Println(cons3) fmt.Println(cons4) fmt.Println(cons5) fmt.Println(cons6) fmt.Println(cons7) fmt.Println(cons8) fmt.Println(cons9) fmt.Println(cons10) fmt.Println(cons11) } pointer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import \u0026#34;fmt\u0026#34; type MyInt struct { n int } // 修改结构体中属性值时，使用指针类型传递 func (myInt *MyInt) Increase() { myInt.n++ } func (myInt MyInt) Decrease() { myInt.n-- } type Pet interface { Name() string Age() uint8 } type Dog struct { name string age uint8 } // 返回结构体中属性值时，直接用值类型传参 func (dog Dog) Name() string { return dog.name } func (dog Dog) Age() uint8 { return dog.age } func main() { mi := MyInt{} mi.Increase() mi.Increase() mi.Decrease() mi.Decrease() mi.Increase() fmt.Printf(\u0026#34;%v\\n\u0026#34;, mi.n == 3) // ======= myDog := Dog{\u0026#34;Little D\u0026#34;, 3} _, ok1 := interface{}(myDog).(Pet) _, ok2 := interface{}(\u0026amp;myDog).(Pet) fmt.Printf(\u0026#34;%v, %v\\n\u0026#34;, ok1, ok2) } select 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import \u0026#34;fmt\u0026#34; func main() { // select语句属于条件分支流程控制方法， // 不过它只能用于通道。 // 它可以包含若干条case语句， // 并根据条件选择其中的一个执行。 // 进一步说，select语句中的case关键字只能后跟用于通道的发送操作的表达式以及接收操作的表达式或语句。示例如下： ch1 := make(chan int, 1) ch2 := make(chan int, 1) // 省略若干条语句 select { case e1 := \u0026lt;-ch1: fmt.Printf(\u0026#34;1th case is selected. e1=%v.\\n\u0026#34;, e1) case e2 := \u0026lt;-ch2: fmt.Printf(\u0026#34;2th case is selected. e2=%v.\\n\u0026#34;, e2) default: fmt.Println(\u0026#34;No data!\u0026#34;) } // 另一方面，对于包含通道发送操作的case来讲， // 其执行条件就是通道中至少还能缓冲一个数据（或者说通道未满）。 // 类似的，当有多个case中的通道未满时，它们会被随机选择。请看下面的示例： ch3 := make(chan int, 100) select { case ch3 \u0026lt;- 1: fmt.Printf(\u0026#34;Sent %d\\n\u0026#34;, 1) case ch3 \u0026lt;- 2: fmt.Printf(\u0026#34;Sent %d\\n\u0026#34;, 2) default: fmt.Println(\u0026#34;Full channel!\u0026#34;) } ch4 := make(chan int, 1) for i := 0; i \u0026lt; 4; i++ { select { case e, ok := \u0026lt;-ch4: if !ok { fmt.Println(\u0026#34;End.\u0026#34;) return } fmt.Println(e) // 操作完关闭channel close(ch4) default: fmt.Println(\u0026#34;No Data!\u0026#34;) // 发数据到channel ch4 \u0026lt;- 1 } } } switch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; ) func main() { var name string = \u0026#34;Golang\u0026#34; switch name { case \u0026#34;Golang\u0026#34;: fmt.Println(\u0026#34;A programming language from Google.\u0026#34;) case \u0026#34;Rust\u0026#34;: fmt.Println(\u0026#34;A programming language from Mozilla.\u0026#34;) default: fmt.Println(\u0026#34;Unknown!\u0026#34;) } // 类型switch ia := []interface{}{byte(6), \u0026#39;a\u0026#39;, uint(10), int32(-4)} intn := rand.Intn(4) fmt.Println(intn) switch v := ia[intn]; interface{}(v).(type) { // rune is an alias for int32 and is equivalent to int32 in all ways. It is // used, by convention, to distinguish character values from integer values. case rune: fmt.Printf(\u0026#34;Case A.\u0026#34;) case byte: fmt.Printf(\u0026#34;Case B.\u0026#34;) default: fmt.Println(\u0026#34;Unknown!\u0026#34;) } } cmd concurrent-channel 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import ( \u0026#34;fmt\u0026#34; ) func main() { // 通过channel通信 ch := make(chan string) // 开启goroutine go printConcurrent(666, ch) // 开启多个协程 // := 声明赋值 for i := 0; i \u0026lt; 5; i++ { // ch获取输入 go printConcurrent(i, ch) } // ch读取输出 for { msg := \u0026lt;-ch fmt.Println(msg) } // 休眠保留运行 //time.Sleep(time.Millisecond * 3) } func printConcurrent(i int, ch chan string) { for { ch \u0026lt;- fmt.Sprintf(\u0026#34;Hello Concurrent Go %d \\n\u0026#34;, i) } } pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 import ( \u0026#34;encoding/binary\u0026#34; \u0026#34;io\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;sort\u0026#34; ) // empty file：不包括main()，用于他方调用的库编码 // 接收一个int数组，扔进channel，返回 func ArraySource(srcArr ...int) chan int { // 创建channel out := make(chan int) // channel内容只能通过goroutine进行传送 go func() { for _, v := range srcArr { // 数组内容扔进channel out \u0026lt;- v } // 关闭通道 close(out) }() return out } // 增加\u0026lt;- 表示只进不出 func ArraySource2(srcArr ...int) \u0026lt;-chan int { out := make(chan int) go func() { for _, v := range srcArr { out \u0026lt;- v } close(out) }() return out } // 内存排序 func InMemorySort(in \u0026lt;-chan int) \u0026lt;-chan int { out := make(chan int, 1024) go func() { arr := []int{} // 输入的chan，读取内容到slice中 for v := range in { arr = append(arr, v) } // 排序 sort.Ints(arr) // 扔进chan for _, v := range arr { out \u0026lt;- v } close(out) }() return out } // 合并节点，归并操作 func Merge(in1, in2 \u0026lt;-chan int) \u0026lt;-chan int { out := make(chan int, 1024) go func() { v1, ok1 := \u0026lt;-in1 v2, ok2 := \u0026lt;-in2 // 接收方：ok判定有无元素 for ok1 || ok2 { // in2无值取in1 // in1中有值，且小于in2中的值 if !ok2 || (ok1 \u0026amp;\u0026amp; v1 \u0026lt; v2) { // 将v1扔进out out \u0026lt;- v1 // 继续读in1 v1, ok1 = \u0026lt;-in1 } else { // 将v2扔进out out \u0026lt;- v2 // 继续读in2 v2, ok2 = \u0026lt;-in2 } } // 发送方：channel关闭 close(out) }() return out } // 归并多个节点 func MergeN(inputs ...\u0026lt;-chan int) \u0026lt;-chan int { size := len(inputs) if size == 1 { return inputs[0] } middle := size / 2 return Merge(MergeN(inputs[:middle]...), MergeN(inputs[middle:]...)) } // 文件系统读取源数据 func ReaderSource(reader io.Reader, chunkSize int) \u0026lt;-chan int { out := make(chan int, 1024) go func() { // 指定八位 buffer := make([]byte, 8) // 已读的字节数 bytesRead := 0 for { // 读到的字节数 n, err := reader.Read(buffer) bytesRead += n if n \u0026gt; 0 { // 转换文件字节到channel out \u0026lt;- int(binary.BigEndian.Uint64(buffer)) } // 处理错误 // 超出了预设的块大小 exceedChunk := chunkSize != -1 \u0026amp;\u0026amp; chunkSize \u0026lt;= bytesRead if err != nil || exceedChunk { break } } close(out) }() return out } // 写数据 func WriterSink(writer io.Writer, in \u0026lt;-chan int) { for v := range in { buffer := make([]byte, 8) binary.BigEndian.PutUint64(buffer, uint64(v)) _, err := writer.Write(buffer) if err != nil { panic(err) } } } // 生成随机数据源 func RandomSource(count int) \u0026lt;-chan int { out := make(chan int) go func() { for i := 0; i \u0026lt; count; i++ { out \u0026lt;- rand.Int() } close(out) }() return out } sort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) func main() { // 初始化一个int类型的slice intSlice := []int{ 1, 3, 9, 2, 4, } fmt.Println(intSlice) // 调用库排序 sort.Ints(intSlice) fmt.Println(intSlice) // for range for index, value := range intSlice { fmt.Println(index, value) } fmt.Println(\u0026#34;===============\u0026#34;) // 不使用变量时，用_声明 for _, value := range intSlice { fmt.Println(value) } } sort-external 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;hello-go/pipeline\u0026#34; \u0026#34;os\u0026#34; ) func main() { p := createPipeline(\u0026#34;small.in\u0026#34;, 512, 4) writeToFile(p, \u0026#34;small.out\u0026#34;) printFile(\u0026#34;small.out\u0026#34;) } func printFile(fileName string) { file, err := os.Open(fileName) if err != nil { panic(err) } defer file.Close() p := pipeline.ReaderSource(file, -1) for v := range p { fmt.Println(v) } } func writeToFile(p \u0026lt;-chan int, fileName string) { file, err := os.Create(fileName) if err != nil { panic(err) } defer file.Close() writer := bufio.NewWriter(file) defer writer.Flush() pipeline.WriterSink(writer, p) } func createPipeline(fileName string, fileSize, chunkCount int) \u0026lt;-chan int { chunkSize := fileSize / chunkCount sortResults := []\u0026lt;-chan int{} for i := 0; i \u0026lt; chunkCount; i++ { file, err := os.Open(fileName) if err != nil { panic(err) } file.Seek(int64(i*chunkSize), 0) source := pipeline.ReaderSource(bufio.NewReader(file), chunkSize) sortResults = append(sortResults, pipeline.InMemorySort(source)) } return pipeline.MergeN(sortResults...) } pipeline-demo 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; // 上面pipeline中实现了内存排序、归并操作 \u0026#34;hello-go/pipeline\u0026#34; \u0026#34;os\u0026#34; ) const ( fileName = \u0026#34;small.in\u0026#34; count = 50 ) func main() { //const FILE_NAME = \u0026#34;small.in\u0026#34; //const count = 50 //testMerge() file, err := os.Create(fileName) if err != nil { panic(err) } defer file.Close() randomSrc := pipeline.RandomSource(count) writer := bufio.NewWriter(file) pipeline.WriterSink(writer, randomSrc) writer.Flush() file, err = os.Open(fileName) if err != nil { panic(err) } defer file.Close() // 逐行处理文件 reader := bufio.NewReader(file) fileCh := pipeline.ReaderSource(reader, -1) for v := range fileCh { fmt.Println(v) } } func testMerge() { ch1 := pipeline.ArraySource2(3, 1, 4, 2, 66) ch2 := pipeline.ArraySource2(4, 2, 5, 3, 77) // 排序 sortedCh1 := pipeline.InMemorySort(ch1) sortedCh2 := pipeline.InMemorySort(ch2) mergedCh := pipeline.Merge(sortedCh1, sortedCh2) //for ; ; { //\t// ok表示source channel还未关闭 //\t// 该写法不易读 //\tif v, ok := \u0026lt;-ch1; ok { //\tfmt.Println(v) //\t} else { //\tbreak //\t} //} // 直接用for range，提高可读性 for v := range mergedCh { fmt.Println(v) } } web web-server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(writer http.ResponseWriter, request *http.Request) { fmt.Fprintf(writer, \u0026#34;\u0026lt;h1\u0026gt;Hello Golang: %s\u0026lt;/h1\u0026gt;\u0026#34;, request.FormValue(\u0026#34;name\u0026#34;)) }) serve := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if serve == nil { fmt.Print(serve) } } 完","permalink":"https://redolog.github.io/posts/rd/language/go/note/","summary":"\u003cp\u003eGolang笔记。\u003c/p\u003e","title":"Go学习手记"},{"content":"译作。对原文做核心理念的意译。\nlinkedin-code-review\nLinkedIn 针对高效代码评审的几条建议 \u0008Preface LinkedIn 刚刚完成了百万行代码审查的里程碑式的工程。\u0008其团队负责开发工具（效率）的领导将为大家分享一路的收获。\n在每个工程师日常工作中，阅读以及审查代码是大家一直在做的事。但作为正式环节的一部分，则会有所不同：它要求每个代码变更都要在上线前被另一个团队成员进行正式复审。而在领英，从2011年起，\u0008代码审查就成为了强制的一个环节。他们相信，做好了代码审查，有助于提升整个团队的工程能力。分开来讲，首先可以保证编码质量，其次可以互相之间做知识共享。除此，做好代码评审也从方方面面改善了他们的工程文化。\nSzczepan Faber 从2015年起，Szczepan Faber 就作为领英开发工具组的领导，这个团队为提升工程效率负责。2011年到201\u00085年期间，他是 Gradle 1.x-2.x版本的核心工程师。在2007年，他创建了 Mockito 这个 Java 框架，预估有两百万的用户量。\n在公司级别实行审查，最大的好处就是提高了开发工作流的标准性。领英的所有团队使用同样的工具、方式进行代码审查，这避免了诸如”我可以修复这个问题，但我不知道如何\u0008上手“之类的问题。工作流做了标准化，也间接提高了不同小组间的合作效率。\n作为强制要求，在公司内，领英也建立了良好的反馈分为：\u0013团队欢迎每位工程师\u0008给予反馈，不仅限于代码。工程师将代码审查视为提升专业度的一个机会，而非消极的互相批评。事实上，高质量的代码审查已经成为了领英内部晋升的一大依据，这本身就佐证了工程师的工程能力。\nQuestions \u0026amp; Tips 经过多年演变，领英总结出了一些最佳实践。\u0008像下面这样，多问自己一些问题，就可以挖掘出代码评审的价值。\n我真的理解为什么吗？ 为了做好评审，每个提交都应该包含一个概要设计，以此说明编码变更的意图。在提交代码后，提交者也有义务来解释自己的动机。进一步，这也会促使提交者在commit message中规范化，进一步规范了编码文档。\n我在做正向反馈吗？ 工程团队往往充满了聪明的伙伴，大家都认为简洁的编码以及精干的测试覆盖率是理所应当的。这带来一个问题：每个人都专注于寻找你的问题。继而导致：问题背后的作者一定是被打击的。但实际上每个人都需要被鼓励，\u0008以此来提高团队战斗力，并且鼓励具有传播的作用。所以，领英鼓励，好的地方也有明确用书面语表达出来。\n我的代码评审的注释写得够明确吗？ 不管反馈是正面还是负面，对应的注释都应该是能自我解释的。当你反馈的信息不够明确时，代码作者也会有所疑惑（这里到底哪里写得不够好？），对于评审的注释、反馈，\u0008宁可多写，也不要写得过于简陋。\n我尊重提交者的工作成果吗？ 辛勤的工作永远值得大家尊敬（不管产出如何）。这一点有利于培养健壮、士气高的团队。有些编码存在问题较多，可能需要返工，\u0008这种情况下，我们仍然要尊重、认同作者的投入。作为反馈，我们可以给予提交者得体的说明：\n\u0008指出做得好的地方 说句谢谢 评审注释对我有帮助吗？ 多问一句这个问题，有利于辨明反馈是否有必要。我们应该视代码审查为有帮助的开发流程，而非冗余的无用功。如果觉得此处无用，那就直接移除。一个典型的无用评审案例就是编码风格规范。代码风格规范应该使用自动化工具进行强化推广，而非工程师。\n测试完备够充足吗？ 在领英团队，每个代码变更的提交都强制要修测试完备。比如使用 GitHub 为例，在每个 Pull Request 的描述中必须附带testing done的字样。而这个完备程度取决于每次变更的严重程度。如果变更包含了新的、变动的条件复杂性，那在单测中就应该覆盖。如果集成测试不够完备，那就需要人肉测试。在上面说到的案例中，testing done就应该包含测试前置情况、产出。\n我是否在评审时\u0008过于深究细节？ 有些评审注释过多，以至于把真正需要修复的问题给覆盖了。评审过于深究不要紧的细节会拖慢评审进度，并且给评审双方增加合作阻力。大家有共同的评审目标、正面例子、以及积极的评审文化，可以有效避免冗余、耗费精力的问题。\n##\u0008 Summary 将代码审查引入为\u0008工作流程有助于提高编码质量、知识共享度。当工程师意识到有人要阅读我的代码时，则会更加谨慎，也就会在首次提交时就做到最好（节约后续返工成本）。日常工作中经常收到反馈时，代码的精进也就成为一个习惯。\n在领英，从百万行级别的代码评审中我们学到了很多，在以后更多的评审中，我们也渴望学到更多。编码质量越高，我们所创作的产品质量也就越高。高质量的评审是传播性的。\n","permalink":"https://redolog.github.io/posts/english/translation/linkedin_code_review/","summary":"译作。对原文做核心理念的意译。 linkedin-code-review LinkedIn 针对高效代码评审的几条建议 \u0008Preface LinkedIn 刚刚完成了百万行代码审查的里程碑式的工程。\u0008其团队负责开发工具（效率）的领导","title":"译---LinkedIn如何高效进行代码评审"},{"content":"香港科技大学真漂亮。\n","permalink":"https://redolog.github.io/posts/note/hongkong20190405/visit/","summary":"香港科技大学真漂亮。","title":"香港两日游"},{"content":"Int numbers every programmer should know.\n1 2 3 4 5 6 7 8 uint8 : 0 to 255 uint16 : 0 to 65535 uint32 : 0 to 4294967295 uint64 : 0 to 18446744073709551615 int8 : -128 to 127 int16 : -32768 to 32767 int32 : -2147483648 to 2147483647 int64 : -9223372036854775808 to 9223372036854775807 ","permalink":"https://redolog.github.io/posts/rd/linux/data/int/","summary":"\u003cp\u003eInt numbers every programmer should know.\u003c/p\u003e","title":"Int numbers every programmer should know"},{"content":"大约是2018-12在imageDT内部简单分享的一则Slide：从零开始微服务。介绍微服务出现的背景、历程以及基本的技术栈体现。\n","permalink":"https://redolog.github.io/posts/rd/share/learn-micro-service-from-scratch/","summary":"\u003cp\u003e大约是2018-12在\u003ccode\u003eimageDT\u003c/code\u003e内部简单分享的一则Slide：从零开始微服务。介绍微服务出现的背景、历程以及基本的技术栈体现。\u003c/p\u003e","title":"从零开始微服务"},{"content":"AWK笔记。\nAWK是Linux系统下著名的字符串处理工具。取名逻辑：三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。\nRef Runoob Linux awk 命令 CoolShell AWK 简明教程 实例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # in.txt 中每一行第一项拼接逗号输出到 out.txt awk \u0026#39;{print $1\u0026#34;,\u0026#34;}\u0026#39; in.txt \u0026gt; out.txt # 每行按空格或TAB分割，输出文本中的1、4项 awk \u0026#39;{print $1,$4}\u0026#39; log.txt # 使用\u0026#34;,\u0026#34;分割 #-F相当于内置变量FS, 指定分割字符 awk -F, \u0026#39;{print $1,$2}\u0026#39; log.txt # 或者使用内建变量 awk \u0026#39;BEGIN{FS=\u0026#34;,\u0026#34;} {print $1,$2}\u0026#39; log.txt awk -v # 设置变量 # a=1 输出第一项 第一项+1 awk -va=1 \u0026#39;{print $1,$1+a}\u0026#39; log.txt # 过滤第一列大于2的行 awk \u0026#39;$1\u0026gt;2\u0026#39; log.txt # 过滤第一列等于2的行 awk \u0026#39;$1==2 {print $1,$3}\u0026#39; log.txt 完 ","permalink":"https://redolog.github.io/posts/rd/language/awk/note/","summary":"AWK笔记。 AWK是Linux系统下著名的字符串处理工具。取名逻辑：三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。 Ref Runoob Linux awk 命令 CoolShell AWK 简","title":"AWK学习手记"},{"content":"JavaScript笔记。我大概是16年前后开始看廖雪峰的教程，之后工作中只写过一些基础的jQuery前端代码，很基础。但是工作几年越发觉得JavaScript其实很实用，在web环境中做一些工具用是最方便的。\nMacOS直接使用Option+Command+i打开控制台，Window使用F12打开控制台。\n语法 输出 换行输出 1 2 document.write(\u0026#34;你好 \\ 世界!\u0026#34;); 包 json 使用js可以直接在浏览器环境下处理一些json。\n1 2 3 4 5 6 7 8 9 let jsonStr=\u0026#39;{\u0026#34;header\u0026#34;:{\u0026#34;code\u0026#34;:\u0026#34;200\u0026#34;,\u0026#34;desc\u0026#34;:\u0026#34;success\u0026#34;},\u0026#34;body\u0026#34;:{\u0026#34;total\u0026#34;:489,\u0026#34;pageIndex\u0026#34;:3,\u0026#34;msg\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;pageSize\u0026#34;:200,\u0026#34;dataList\u0026#34;:[{\u0026#34;key\u0026#34;:666,\u0026#34;other\u0026#34;:888},{\u0026#34;key\u0026#34;:666,\u0026#34;other\u0026#34;:888},{\u0026#34;key\u0026#34;:666,\u0026#34;other\u0026#34;:888},{\u0026#34;other\u0026#34;:888},{\u0026#34;key\u0026#34;:null,\u0026#34;other\u0026#34;:888}]}}\u0026#39;; var jsonObj=JSON.parse(jsonStr); jsonStr=JSON.stringify(jsonObj); // 我们查找下上述json结构dataList的每一项，查看不存在 \u0026#34;key\u0026#34;的dict项，或者值空的数据 var list=jsonObj.body.dataList; let target=list.filter(data=\u0026gt;data.key==undefined); console.log(JSON.stringify(target)); // 输出 [{\u0026#34;other\u0026#34;:888},{\u0026#34;key\u0026#34;:null,\u0026#34;other\u0026#34;:888}] 仓库 静态生成器 react-static vercel next.js EventLoop JS EventLoop 可视化 pengdi关于EventLoop的notion笔记 完","permalink":"https://redolog.github.io/posts/rd/language/js/note/","summary":"\u003cp\u003eJavaScript笔记。我大概是16年前后开始看廖雪峰的教程，之后工作中只写过一些基础的\u003ccode\u003ejQuery\u003c/code\u003e前端代码，很基础。但是工作几年越发觉得\u003ccode\u003eJavaScript\u003c/code\u003e其实很实用，在web环境中做一些工具用是最方便的。\u003c/p\u003e","title":"JavaScript学习手记"},{"content":"Python笔记。\n语言特性 参数解包 PEP 448 \u0026ndash; Additional Unpacking Generalizations：将参数从集合中拿出来参与计算。\n1 2 3 4 5 6 7 8 \u0026gt;\u0026gt;\u0026gt; print(*[1], *[2], 3) 1 2 3 \u0026gt;\u0026gt;\u0026gt; dict(**{\u0026#39;x\u0026#39;: 1}, y=2, **{\u0026#39;z\u0026#39;: 3}) {\u0026#39;x\u0026#39;: 1, \u0026#39;y\u0026#39;: 2, \u0026#39;z\u0026#39;: 3} \u0026gt;\u0026gt;\u0026gt; {\u0026#39;x\u0026#39;: 1, **{\u0026#39;x\u0026#39;: 2}} {\u0026#39;x\u0026#39;: 2} \u0026gt;\u0026gt;\u0026gt; {**{\u0026#39;x\u0026#39;: 2}, \u0026#39;x\u0026#39;: 1} {\u0026#39;x\u0026#39;: 1} 运算符 算术 **\t幂 - 返回x的y次幂\ta**b 为10的20次方， 输出结果 100000000000000000000 //\t取整除 - 返回商的整数部分（向下取整）\t9//2 输出结果 4 , 9.0//2.0 输出结果 4.0 比较 !=\t不等于 - 比较两个对象是否不相等\t(a != b) 返回 true. \u0026lt;\u0026gt;\t不等于 - 比较两个对象是否不相等\t(a \u0026lt;\u0026gt; b) 返回 true。这个运算符类似 != 。 逻辑 and\tx and y\t布尔\u0026quot;与\u0026quot; - 如果 x 为 False，x and y 返回 False，否则它返回 y 的计算值。\t(a and b) 返回 20。 or\tx or y\t布尔\u0026quot;或\u0026quot;\t- 如果 x 是非 0，它返回 x 的值，否则它返回 y 的计算值。\t(a or b) 返回 10。 not\tnot x\t布尔\u0026quot;非\u0026quot; - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。\tnot(a and b) 返回 False 成员 in\t如果在指定的序列中找到值返回 True，否则返回 False。\tx 在 y 序列中 , 如果 x 在 y 序列中返回 True。 not in\t如果在指定的序列中没有找到值返回 True，否则返回 False。\tx 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 身份 is\tis 是判断两个标识符是不是引用自一个对象\tx is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False is not\tis not 是判断两个标识符是不是引用自不同对象\tx is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 字符串前使用u,r,b标记 u/U:表示unicode字符串 1 2 3 4 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;你好\u0026#39;) 你好 \u0026gt;\u0026gt;\u0026gt; print(u\u0026#39;你好\u0026#39;) 你好 r/R:正则表达式，使用原始字符串，不转义 1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;\\n \\t sss\u0026#39;) sss \u0026gt;\u0026gt;\u0026gt; print(r\u0026#39;\\n \\t sss\u0026#39;) \\n \\t sss b:bytes，表示该字符串为字节，即每个字符占用一个字节 比如IO接收到的内容就是字节流，此时的内容就应当是经过编码后的字节： 1 2 \u0026#39;内容\u0026#39;.encode(\u0026#39;utf-8\u0026#39;) \u0026gt;\u0026gt;\u0026gt;b\u0026#39;\\xe5\\x86\\x85\\xe5\\xae\\xb9\u0026#39; 三元运算符 条件表达式。\n1 2 3 \u0026gt;\u0026gt;\u0026gt; word=\u0026#39;before\u0026#39; if 1\u0026gt;2 else \u0026#39;after\u0026#39; \u0026gt;\u0026gt;\u0026gt; print(word) after 列表生成式 列表解析式。比如现在有一个需求，对（1,11）的列表每个元素进行-1操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 首先生成一个列表 \u0026gt;\u0026gt;\u0026gt; container=list(range(1,11)) \u0026gt;\u0026gt;\u0026gt; print(container) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # 传统思路是对列表进行循环 \u0026gt;\u0026gt;\u0026gt; empty=[] \u0026gt;\u0026gt;\u0026gt; for x in container: ... empty.append(x-1) ... \u0026gt;\u0026gt;\u0026gt; empty [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # 使用列表生成式，对每个元素直接处理 \u0026gt;\u0026gt;\u0026gt; [x-1 for x in container] \u0026gt;\u0026gt;\u0026gt; [x-1 for x in range(1,11)] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # 后面还可以对元素进行条件判断，比如3的倍数才进行处理 \u0026gt;\u0026gt;\u0026gt; [x-1 for x in container if x%3==0] [2, 5, 8] # 还可以对多个`iterable`进行操作，如下可以生成两个集合的笛卡尔积 \u0026gt;\u0026gt;\u0026gt; [x+y for x in \u0026#39;123\u0026#39; for y in \u0026#39;abc\u0026#39;] [\u0026#39;1a\u0026#39;, \u0026#39;1b\u0026#39;, \u0026#39;1c\u0026#39;, \u0026#39;2a\u0026#39;, \u0026#39;2b\u0026#39;, \u0026#39;2c\u0026#39;, \u0026#39;3a\u0026#39;, \u0026#39;3b\u0026#39;, \u0026#39;3c\u0026#39;] # 同理可以将\u0008`dict`一次转为`list` \u0026gt;\u0026gt;\u0026gt; dictVar={\u0026#39;1\u0026#39;:\u0026#39;a\u0026#39;,\u0026#39;2\u0026#39;:\u0026#39;b\u0026#39;} \u0026gt;\u0026gt;\u0026gt; print(dictVar) {\u0026#39;1\u0026#39;: \u0026#39;a\u0026#39;, \u0026#39;2\u0026#39;: \u0026#39;b\u0026#39;} \u0026gt;\u0026gt;\u0026gt; listVar=[k+\u0026#39;-\u0026gt;\u0026#39;+v for k,v in dictVar.items()] \u0026gt;\u0026gt;\u0026gt; listVar [\u0026#39;1-\u0026gt;a\u0026#39;, \u0026#39;2-\u0026gt;b\u0026#39;] lambda函数 匿名函数。\n1 2 3 4 5 6 7 8 # 还是以上述列表生成式中的需求为例，借助\u0008`map`处理每个元素 \u0026gt;\u0026gt;\u0026gt; list(map(lambda x:x-1, container)) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # 等同于 \u0026gt;\u0026gt;\u0026gt; list(x-1 for x in container) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \u0026gt;\u0026gt;\u0026gt; [x-1 for x in container] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 包 SimpleHTTPServer Python自带的一个简易版web-server。\n官方文档\n1 2 3 4 5 6 7 8 9 10 11 import SimpleHTTPServer import SocketServer PORT = 8000 Handler = SimpleHTTPServer.SimpleHTTPRequestHandler httpd = SocketServer.TCPServer((\u0026#34;\u0026#34;, PORT), Handler) print \u0026#34;serving at port\u0026#34;, PORT httpd.serve_forever() 不推荐在生产环境使用，只适用于小型或者基本服务的搭建试用，所以可以直接用-m命令启动：\n1 2 3 python -m SimpleHTTPServer 8000 # python3 python3 -m http.server 8000 之后即可直接访问：http://IP:端口号/，web根路径就是执行命令的那个目录。\njson 使用python处理一些json的问题检查很方便。注意，从java日志中贴过来的json，需要把null替换为None。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 #!/usr/bin/python3 import json # Python 字典类型转换为 JSON 对象 data = { \u0026#34;header\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;success\u0026#34; }, \u0026#34;body\u0026#34;: { \u0026#34;total\u0026#34;: 489, \u0026#34;pageIndex\u0026#34;: 3, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;pageSize\u0026#34;: 200, \u0026#34;dataList\u0026#34;: [ { \u0026#34;key\u0026#34;:666, \u0026#34;other\u0026#34;:888 }, { \u0026#34;key\u0026#34;:666, \u0026#34;other\u0026#34;:888 }, { \u0026#34;key\u0026#34;:666, \u0026#34;other\u0026#34;:888 }, { \u0026#34;other\u0026#34;:888 }, { \u0026#34;key\u0026#34;:None, \u0026#34;other\u0026#34;:888 } ] } } json_str = json.dumps(data) # print (\u0026#34;Python 原始数据：\u0026#34;, repr(data)) # print (\u0026#34;JSON 对象：\u0026#34;, json_str) # print(data[\u0026#39;body\u0026#39;][\u0026#39;dataList\u0026#39;]) # 这里我们检查上述json结构dataList的每一项，查看不存在 \u0026#34;key\u0026#34;的dict项，或者值空的数据 [print(dataItem) for dataItem in data[\u0026#39;body\u0026#39;][\u0026#39;dataList\u0026#39;] if (\u0026#34;key\u0026#34; not in dataItem.keys() or dataItem[\u0026#34;key\u0026#34;]==None)] qrcode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/usr/bin/python3 # -*- coding:UTF-8 -*- ############################################################################### # pip install qrcode # 命令行二维码工具 # # :author:: DragonSong 📅: 2022/3/4 # import qrcode def printQR(str): qr = qrcode.QRCode() qr.border = 1 qr.add_data(str) qr.make() qr.print_ascii(out=None, tty=False, invert=False) if __name__ == \u0026#34;__main__\u0026#34;: str = input(\u0026#34;请输入你要encode为二维码的字符:\u0026#34;) printQR(str) 文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 with open(\u0026#39;/Users/dragonsong/Downloads/code-repo/dragonsong031100/content/posts/methodology/work/make-a-great-meeting/freemind.txt\u0026#39;) as fp: line = fp.readline() while line: str_arr = line.split(\u0026#39; \u0026#39;) for ele in str_arr: if \u0026#34;TEXT\u0026#34; in ele: print(ele.replace(\u0026#34;TEXT=\u0026#34;,\u0026#34;\u0026#34;).replace(\u0026#34;\\\u0026#34;\u0026#34;,\u0026#34;\u0026#34;)); line = fp.readline() #conding=utf8 import os directory = os.walk(r\u0026#34;/Users/dragonsong/Desktop/dir\u0026#34;) for path,dir_list,file_list in directory: for file_name in file_list: print(file_name) suffix = os.path.splitext(file_name)[1] if suffix == \u0026#39;.record_dat\u0026#39;: print(file_name) os.rename((path+ os.sep + file_name),(path+ os.sep + file_name+\u0026#39;.mp4\u0026#39;)) # concat 拼接操作 content = [] with open(\u0026#39;/Users/dragonsong/Downloads/zeus_2022_2_23.csv\u0026#39;, \u0026#39;r\u0026#39;) as f: for line in f.readlines(): line_list = line.strip(\u0026#39;\\n\u0026#39;).split(\u0026#39;,\u0026#39;) # 去除换行符，以制表符分隔 new_c = line_list[2]+\u0026#39;(\u0026#34;\u0026#39;+line_list[1]+\u0026#39;\u0026#34;,\u0026#34;\u0026#39;+line_list[0]+\u0026#39;\u0026#34;),\u0026#39; print(new_c) 完","permalink":"https://redolog.github.io/posts/rd/language/python/note/","summary":"\u003cp\u003ePython笔记。\u003c/p\u003e","title":"Python学习手记"},{"content":"短时间上手，ES实战。新手接触新东西，最重要的是理清学习路径，这里的目录，就是我个人的简化路径。深入还需要再多投入时间精力，慢慢研究才能做到深钻，本文目标是「快速上手」。\n喜欢通过视频学习的同学可以看慕课：ElasticSearch入门。\n推荐：\n全文搜索之 Elasticsearch Elasticsearch 权威指南（中文版） Elasticsearch实战 基础 Elasticsearch 是一个分布式、可扩展、实时的搜索与数据分析引擎。 它能从项目一开始就赋予你的数据以搜索、分析和探索的能力，这是通常没有预料到的。 它存在还因为原始数据如果只是躺在磁盘里面根本就毫无用处。基于Apache Lucene构建的开源搜索引擎。 Kibana 是一款开源的数据分析和可视化平台，它是 Elastic Stack 成员之一，设计用于和 Elasticsearch 协作。您可以使用 Kibana 对 Elasticsearch 索引中的数据进行搜索、查看、交互操作。 简单讲，ES是一款数据库(server)，Kibana是可视化客户端(client)。相比Lucene，ES易于使用、应用。应用优势：易于横向拓展，可支持PB级别的结构化与非结构化的数据处理。\n适用场景：\n海量数据分析 站内搜索引擎 数据库 应用安装 首先安装Elasticsearch，Installation。\n然后安装Kibana，安装 Kibana。\n推荐英文文档。\n单实例 官网复制tar下载链接：https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.tar.gz 进入本地安装目录，wget下载 1 2 3 4 cd /Users/DragonSong/es_learn wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.tar.gz # 解压tar包 tar -zxvf elasticsearch-6.3.2.tar.gz 插件 Head提供了友好的操作界面，可以辅助开发者操作ES。\n进入GitHub搜索elasticsearch-head，获取zip下载链接，wget下载：https://github.com/mobz/elasticsearch-head/archive/master.zip 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cd /Users/DragonSong/es_learn wget https://github.com/mobz/elasticsearch-head/archive/master.zip # 解压zip包 unzip master.zip # 进入master目录，检查node版本 cd elasticsearch-head-master/ node -v # npm装载环境 npm install # 启动node服务 npm run start # 浏览器查看界面 http://localhost:9100 # 修改es配置，解决跨域问题 cd ../elasticsearch-6.3.2 vi config/elasticsearch.yml # shift+g跳到末行，添加内容： http.cors.enabled: true http.cors.allow-origin: \u0026#34;*\u0026#34; # 保存退出 此时启动ES，开启Head node服务，进入localhost:9100即可看到工具已连接到ES 多节点 分布式环境安装。\n修改elasticsearch.yml，将当前节点设置为master 1 2 3 4 5 6 7 vi config/elasticsearch.yml # 末尾增加配置： cluster.name: dragon node.name: master node.master: true # 绑定ip network.host: 127.0.0.1 重启服务（作为master） 1 2 3 4 5 # 找到原后台服务，并kill ps -ef | grep `pwd` kill 20660 # 重启 ./bin/elasticsearch -d 外部新建目录，存放slave节点服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 mkdir es_slave # 复制tar包 cp elasticsearch-6.3.2.tar.gz es_slave cd es_slave # 解压 tar -zxvf elasticsearch-6.3.2.tar.gz # 复制两个节点内容 cp -r elasticsearch-6.3.2 es_slave1 cp -r elasticsearch-6.3.2 es_slave2 # 进入slave1，修改配置 cd es_slave1 vi config/elasticsearch.yml # 配置内容： cluster.name: dragon node.name: slave1 network.host: 127.0.0.1 # 端口需区分于master http.port: 8200 discovery.zen.ping.unicast.hosts: [\u0026#34;127.0.0.1\u0026#34;] # 上述过程重复与slave2即可 # 配置内容： cluster.name: dragon node.name: slave2 network.host: 127.0.0.1 # 端口需区分于master http.port: 8100 discovery.zen.ping.unicast.hosts: [\u0026#34;127.0.0.1\u0026#34;] \u0008扩充服务节点方式就是重复上述过程，易于操作。\nElasticsearch ./bin/elasticsearch启动 ./bin/elasticsearch -d后台启动 Ctrl+C停止 curl 'http://localhost:9200/?pretty'测试连接 Kibana Kibana默认配置放在config/kibana.yml ./bin/kibana启动 localhost:5601 或者 http://YOURDOMAIN.com:5601访问 装好跑起来，加载示例数据，使用Kibana\u0008操作下常规动作，了解下能做什么(what)，想一下如何做到(how)，以及结合应用的业务场景何时使用(when)。\n用户手册 对安装好的应用进行功能操作。\n这里重点看Kibana 用户手册，\u0008从基础入门到可视化进行初步了解即可。\n以及Elasticsearch: 权威指南，从基础入门，到聚合，参考自己要真实编码应用的方面，选择\u0008章节花费两个小时研究一下即可。\n基本概念 集群中包含了多个节点，服务启动后，head可以很方便地查看集群状态 索引：含有相同属性的文档集合 类型：索引可以定义多个类型，文档必属于一个类型 文档：可被索引的基本数据单位 分片：每个分片是一个Lucene索引，每个索引有多个分片，只能在创建索引时指定 备份：拷贝一份分片，就完成了分片备份，后期也可以自由修改 索引创建 ES通过RESTFul API调用服务：\nAPI基本格式： http://\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;index\u0026gt;/\u0026lt;type\u0026gt;/\u0026lt;doc_id\u0026gt; 常见HTTP动词： GET/PUT/POST/DELETE 非结构化的索引 可以通过head插件界面创建索引。成功后返回：\n1 2 3 4 5 { \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;index\u0026#34;: \u0026#34;index_first\u0026#34; } 创建index成功后，head首页即可看到不同节点存储的分片。（粗线表示主分片，细线表示备份）\n索引信息中的mappings为空时，即为非结构化的索引。\n结构化的索引 在head插件，复合查询tab中，输入：\n1 2 3 4 5 6 7 8 9 10 http://localhost:9200/ languages/java/mappings POST { \u0026#34;type_first\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;text\u0026#34; } } } 提交这个POST请求，右侧返回如下信息，说明对应的mappings规则已成功建立：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;_index\u0026#34;: \u0026#34;languages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;mappings\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1 } 也可以到概览tab页查看索引信息，发现languages这个index中已经保存了对应的mappings结构化信息。这一步可以使用更加易用的Postman进行操作，发送一个PUT将数据（index）写到es中，在body中选择raw-\u0026gt;json，可以更好地定义json数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \u0026#34;settings\u0026#34;:{ \u0026#34;number_of_shards\u0026#34;:3, \u0026#34;number_of_replicas\u0026#34;:1 }, \u0026#34;mappings\u0026#34;:{ \u0026#34;java\u0026#34;:{ \u0026#34;properties\u0026#34;:{ \u0026#34;name\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; }, \u0026#34;ranking\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;keyword\u0026#34; }, \u0026#34;age\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;integer\u0026#34; }, \u0026#34;date\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; } } } } } # kibana操作： PUT index-name/type-name/_mapping { \u0026#34;type-name\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;field-name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } 发送PUT后返回：\n1 2 3 4 5 { \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;index\u0026#34;: \u0026#34;newlanguages\u0026#34; } kibana\u0008查询mapping：\n1 GET index-name/type-name/_mapping 此时在head插件的概览查看索引信息，可以看到mappings就是完全按照我们的json格式来定义的，这一步本质上，与rdbms中建表是同理的。\n使用别名 首先了解：如何在Elasticsearch里面使用索引别名。\n代码使用别名操作实际数据，可以做到几个好处：\n一个入口，可以无缝切换多个索引，不影响前端 分组多个索引，对数据\u0008进行自由分组，同一个别名可以同时整合多个索引中的数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 POST /_aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index_new_name\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias_name\u0026#34; } }, { \u0026#34;remove\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;index_old_name\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;alias_name\u0026#34; } } ] } 数据插入 doc的插入分两种类型：\n指定文档id插入 自动产生文档id插入 结合rdbms使用经验，与我们插入数据id自增与自定义是类似的。\n这里我们继续使用上面创建的newlanguagesindex，向其中java中插一条id=1的数据：\n1 2 3 4 5 6 7 localhost:9200/newlanguages/java/1 { \u0026#34;name\u0026#34;:\u0026#34;java\u0026#34;, \u0026#34;ranking\u0026#34;:\u0026#34;001\u0026#34;, \u0026#34;age\u0026#34;:30, \u0026#34;date\u0026#34;:\u0026#34;1979-01-01\u0026#34; } 返回说明生效：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1 } 改用POST，并去掉手动指定的id，让es自动帮助我们生成id：\n1 2 3 4 5 6 7 localhost:9200/newlanguages/java/ { \u0026#34;name\u0026#34;:\u0026#34;java_02\u0026#34;, \u0026#34;ranking\u0026#34;:\u0026#34;002\u0026#34;, \u0026#34;age\u0026#34;:31, \u0026#34;date\u0026#34;:\u0026#34;1989-01-01\u0026#34; } 返回如下信息说明生效：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;aMzB22QBUDCHWKlAjWAs\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 1, \u0026#34;_primary_term\u0026#34;: 1 } 可以看到id与之前的1是不同的，说明是不同的数据。head中的数据\u0008概览也能清晰地看到这条数据。\n数据修改 直接修改文档 脚本修改文档 同样使用Postman发送一个POST，指定文档id，\u0008url后紧跟一个_update关键词：\n1 2 3 4 5 6 localhost:/9200/newlanguages/java/1/_update { \u0026#34;doc\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;更新后的名字：java01\u0026#34; } } 提交后显示\u0008为udpated，\u0008说明数据已经更新完毕：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 2, \u0026#34;_primary_term\u0026#34;: 1 } 脚本的修改方式，区别于上方的参数部分，url格式一致，并且es支持了多种脚本语言，这里使用内置的painless，inline中指定脚本的逻辑，ctx表示es上下文，_source则表示本次操作的文档，比如对其age属性进行操作：\n1 2 3 4 5 6 7 localhost:/9200/newlanguages/java/1/_update { \u0026#34;script\u0026#34;:{ \u0026#34;lang\u0026#34;:\u0026#34;painless\u0026#34;, \u0026#34;inline\u0026#34;:\u0026#34;ctx._source.age += 2\u0026#34; } } 可以\u0008看到：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 3, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 3, \u0026#34;_primary_term\u0026#34;: 1 } 结果为updated，而version也变为了3。同样在head的界面中可以查看这条数据实时的情况。age由原先的30变为了32，我们的脚本成功执行。\n另一种写法是将参数值另外放到外层结构：\n1 2 3 4 5 6 7 8 9 { \u0026#34;script\u0026#34;:{ \u0026#34;lang\u0026#34;:\u0026#34;painless\u0026#34;, \u0026#34;inline\u0026#34;:\u0026#34;ctx._source.age = params.age\u0026#34;, \u0026#34;params\u0026#34;:{ \u0026#34;age\u0026#34;:38 } } } 请求后数据version又迭代了一次：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 4, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 4, \u0026#34;_primary_term\u0026#34;: 1 } 数据删除 删除分两个层级：\n删除文档 删除索引 删除一个文档，只需要指定文档id\u0008，并发送一个DELETE请求即可：\n1 2 localhost:/9200/newlanguages/java/1 DELETE 结果状态变更为deleted:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 5, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 2, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 5, \u0026#34;_primary_term\u0026#34;: 1 } 而删除一个index可以在head插件界面直接操作，选定某个索引的动作，点击删除，输入确认的删除操作，此时该索引关联的数据都\u0008会被清空。\u0008同样\u0008也可以发送DELETE请求，url为：\n1 2 localhost:/9200/indexname DELETE index-name 数据查询 分三种类型：\n简单查询 条件查询 聚合查询 使用index/type/doc_id\u0008即可简单直接地定位到一条数据，我们发送一个GET请求：\n1 localhost:9200/newlanguages/java/5 即可得到响应：\n1 2 3 4 5 6 7 8 9 10 11 12 13 { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;java5\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;005\u0026#34;, \u0026#34;age\u0026#34;: 51, \u0026#34;date\u0026#34;: \u0026#34;1959-01-01\u0026#34; } } 而条件查询则需要指定一些条件，请求改为POST，关键字改为_search，\u0008body中详细描述查询条件，比如查某index下所有数据：\n1 2 3 4 5 6 localhost:9200/newlanguages/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match_all\u0026#34;:{} } } 会获取到所有数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 { \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 3, \u0026#34;successful\u0026#34;: 3, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 6, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;java2\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;002\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;date\u0026#34;: \u0026#34;1929-01-01\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;java4\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;004\u0026#34;, \u0026#34;age\u0026#34;: 41, \u0026#34;date\u0026#34;: \u0026#34;1909-01-01\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;java5\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;005\u0026#34;, \u0026#34;age\u0026#34;: 51, \u0026#34;date\u0026#34;: \u0026#34;1959-01-01\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;aMzB22QBUDCHWKlAjWAs\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;java_02\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;002\u0026#34;, \u0026#34;age\u0026#34;: 31, \u0026#34;date\u0026#34;: \u0026#34;1989-01-01\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;001\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;date\u0026#34;: \u0026#34;1979-01-01\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;java3\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;003\u0026#34;, \u0026#34;age\u0026#34;: 31, \u0026#34;date\u0026#34;: \u0026#34;1949-01-01\u0026#34; } } ] } } 指定返回第一条数据：\n1 2 3 4 5 6 7 8 localhost:9200/newlanguages/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match_all\u0026#34;:{} }, \u0026#34;from\u0026#34;:1, \u0026#34;size\u0026#34;:1 } 指定关键词查询：\n1 2 3 4 5 6 7 8 localhost:9200/newlanguages/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;java3\u0026#34; } } } 可以对多条结果指定排序规则：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;java\u0026#34; } }, \u0026#34;sort\u0026#34;:[ { \u0026#34;date\u0026#34;:{ \u0026#34;order\u0026#34;:\u0026#34;desc\u0026#34; } } ] } 而聚合查询使用aggs作为关键词，比如这里对数据以age字段进行分组：\n1 2 3 4 5 6 7 8 9 { \u0026#34;aggs\u0026#34;:{ \u0026#34;group_by_age\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;age\u0026#34; } } } } 可以看到在聚合结果中，age为30、31的数据有两条：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \u0026#34;aggregations\u0026#34;: { \u0026#34;group_by_age\u0026#34;: { \u0026#34;doc_count_error_upper_bound\u0026#34;: 0, \u0026#34;sum_other_doc_count\u0026#34;: 0, \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: 30, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: 31, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: 41, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: 51, \u0026#34;doc_count\u0026#34;: 1 } ] } } 当然可以对多个字段同时进行聚合：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;aggs\u0026#34;:{ \u0026#34;group_by_age\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;age\u0026#34; } }, \u0026#34;group_by_date\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;date\u0026#34; } } } } 利用stats函数可以对某个字段进行统计计算：\n1 2 3 4 5 6 7 8 9 { \u0026#34;aggs\u0026#34;:{ \u0026#34;stats_by_age\u0026#34;:{ \u0026#34;stats\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;age\u0026#34; } } } } 聚合结果中对age维度的数量、最大、最小、均值、和进行了展示：\n1 2 3 4 5 6 7 8 9 \u0026#34;aggregations\u0026#34;: { \u0026#34;stats_by_age\u0026#34;: { \u0026#34;count\u0026#34;: 6, \u0026#34;min\u0026#34;: 30, \u0026#34;max\u0026#34;: 51, \u0026#34;avg\u0026#34;: 35.666666666666664, \u0026#34;sum\u0026#34;: 214 } } 高级查询 分类：\n子条件查询：特定字段查询所指特定值 Query context 在查询中，es首先判断文档是否满足条件，其次计算_score值，标识匹配程度（01是否匹配到02匹配得有多好） 常用查询： 全文本查询 针对文本类型数据 模糊匹配 习语匹配 多字段的匹配查询 字段级别查询 针对结构化数据，如数字、日期 Filter context 在查询中只判断文档\u0008是否满足条件，只有yes或no 复合条件查询：以一定逻辑组合子条件查询 \u0008固定分数查询 布尔查询 其他 例程：\n模糊匹配 1 2 3 4 5 6 7 { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;java\u0026#34; } } } 习语匹配 1 2 3 4 5 6 7 { \u0026#34;query\u0026#34;:{ \u0026#34;match_phrase\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;java\u0026#34; } } } 与模糊匹配的区别是，习语匹配不会将词分为多个，严格分词\u0026quot;name\u0026quot;:\u0026quot;java\u0026quot;。\n多字段匹配 1 2 3 4 5 6 7 8 9 10 { \u0026#34;query\u0026#34;:{ \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;java\u0026#34;, \u0026#34;fields\u0026#34;:[ \u0026#34;name\u0026#34;,\u0026#34;alias\u0026#34; ] } } } 语法查询 1 2 3 4 5 6 7 { \u0026#34;query\u0026#34;:{ \u0026#34;query_string\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;java and python\u0026#34; } } } 会同时返回带有java与python名字的数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 { \u0026#34;took\u0026#34;: 4, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 3, \u0026#34;successful\u0026#34;: 3, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;max_score\u0026#34;: 1.2039728, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;_score\u0026#34;: 1.2039728, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;007\u0026#34;, \u0026#34;age\u0026#34;: 77, \u0026#34;date\u0026#34;: \u0026#34;1999-01-01\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;pyp\u0026#34; } }, { \u0026#34;_index\u0026#34;: \u0026#34;newlanguages\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34;: 0.9808292, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;ranking\u0026#34;: \u0026#34;001\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;date\u0026#34;: \u0026#34;1979-01-01\u0026#34; } } ] } } 类似的也可以进行其他逻辑组合，如or：\n1 2 3 4 5 6 7 8 { \u0026#34;query\u0026#34;:{ \u0026#34;query_string\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;(java and python) or js\u0026#34;, \u0026#34;fields\u0026#34;:[\u0026#34;name\u0026#34;,\u0026#34;alias\u0026#34;] } } } 字段级别的查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \u0026#34;query\u0026#34;:{ \u0026#34;term\u0026#34;:{ \u0026#34;age\u0026#34;:31 } } } # 范围 { \u0026#34;query\u0026#34;:{ \u0026#34;range\u0026#34;:{ \u0026#34;age\u0026#34;:{ \u0026#34;gte\u0026#34;:20, \u0026#34;lte\u0026#34;:32 } } } } # 日期范围 { \u0026#34;query\u0026#34;:{ \u0026#34;range\u0026#34;:{ \u0026#34;date\u0026#34;:{ \u0026#34;gte\u0026#34;:\u0026#34;2000-01-01\u0026#34;, \u0026#34;lte\u0026#34;:\u0026#34;2010-01-01\u0026#34; } } } } filter 使用bool关键字标识： 1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;filter\u0026#34;:{ \u0026#34;term\u0026#34;:{ \u0026#34;age\u0026#34;:30 } } } } } es\u0008用filter专门用来做数据过滤，对数据也会进行缓存，所以速度也会更快一些（相比query）。\n固定分数查询 es针对每条数据进行_score评分，我们可以指定某一评分的数据（boost）： 1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;query\u0026#34;:{ \u0026#34;constant_score\u0026#34;:{ \u0026#34;filter\u0026#34;:{ \u0026#34;match\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;java\u0026#34; } }, \u0026#34;boost\u0026#34;:0.87546873 } } } 布尔查询 should代表或，must代表且，must_not代表非。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;should\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;python\u0026#34; } }, { \u0026#34;match\u0026#34;:{ \u0026#34;age\u0026#34;:31 } } ] } } } { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;must\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;python\u0026#34; } }, { \u0026#34;match\u0026#34;:{ \u0026#34;age\u0026#34;:77 } } ] } } } 在此基础上，我们还可以通过filter进行单字段条件的组合：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;should\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;python\u0026#34; } }, { \u0026#34;match\u0026#34;:{ \u0026#34;age\u0026#34;:31 } } ], \u0026#34;filter\u0026#34;:[ { \u0026#34;term\u0026#34;:{ \u0026#34;alias\u0026#34;:\u0026#34;pyp\u0026#34; } } ] } } } 条件非：\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;must_not\u0026#34;:{ \u0026#34;term\u0026#34;:{ \u0026#34;name\u0026#34;:\u0026#34;python\u0026#34; } } } } } 聚合 aggregations是我目前首先要搞清楚的操作（前人使用了该操作）。\n首先理解：类似于复杂SQL所能做到的一样，聚合之后的数据可以给到我们比较形象的分析结果，在API使用上，ES对聚合引入了两个抽象层：\n桶（Buckets） 满足特定条件的文档的集合 指标（Metrics） 对桶内的文档进行统计计算 本质上是对文档（ES将数据以文档为单位进行组织，类似于RDBMS中的表，doc\u0026lt;-\u0026gt;table）数据进行范围缩小、\u0008有用的数据计算。\n对概念简单理解后，使用API多次，基本没应用问题\u0008了。\nAPI 俗话说，文档在手，天下我有。到最后落实到编码环节上，参考API文档必不可少。\n参考Java API，根据自身使用的语言查看对应的API即可。\n这里创建一个springboot工程进行基本的es操作：esExercise。\n小结 每一步都可以深入，细化成多篇文档，但要记住这里的重点是快速上手，所以只要前面这几步走过一遍，目标就达成了：\n对ES\u0008基本作用的了解 应用安装 用户手册 API ","permalink":"https://redolog.github.io/posts/rd/storage/es/learn-from-scratch/","summary":"\u003cp\u003e短时间上手，ES实战。新手接触新东西，最重要的是理清学习路径，这里的目录，就是我个人的简化路径。深入还需要再多投入时间精力，慢慢研究才能做到深钻，本文目标是「快速上手」。\u003c/p\u003e","title":"快速上手Elasticsearch"},{"content":"软件架构基本模式，经典书籍。\n译作。\noreilly官网对software-architecture-patterns开放下载。\n软件架构模式 理解常用的架构模式，掌握适用的场景。\u0026ndash;Mark Richards\n目录 引言 1. 分层架构 模式描述 关键概念 模式案例 考量 模式分析 2. 事件驱动架构 中介拓扑 经纪拓扑 考量 模式分析 3. 微内核架构 模式描述 模式案例 考量 模式分析 4. 微服务架构 模式描述 模式拓扑 避免依赖以及协调机制 考量 模式分析 5. 基于空间的架构 模式描述 模式力学 考量 模式分析 内容 引言 对于开发者而言，面对一个无架构的应用项目是极为常见的。在实际开发中面对这样的情形，我们只能求助于一些常见的工业界的标准化架构设计，例如分层架构，但往往会简单地将源码分包。这样导致的结果往往是：一些职责不清晰的源码被分成了一个模块，内部的关系却是混乱的。通常这就是【大泥球混乱架构】。 缺乏良好架构的应用往往会有耦合、脆弱、不可变、无清晰的方向等问题。因而，在了解内部组件、模块的内在之前，我们很难去判定应用的架构特征。特征包括了以下问题，关于部署、维护的基本问题：这个架构可伸缩吗？应用性能如何？可变性如何？响应性能怎样？ 设计模式帮助我们定义这些基本特征以及应用的行为。比如，有些模式目的是为了提高伸缩性，而有些则是为了更加敏捷。了解多种设计模式的优缺点、特征对于满足我们项目中的商业需求、团队目标是有很大帮助的。 作为一名架构师，你必须去证明自己架构上抉择的有效性，尤其是在架构选型上。本书的目标就是帮助我们做更好的架构抉择。 第一章：分层架构 分层架构是最为常见的架构模式，也被称为多层体系架构。这种模式是J2EE应用常用的工业级标准，因而被很多架构师、设计师、开发者所熟知。分层模式紧密贴合了大多数公司中的传统IT通信关系以及组织结构，这使得大多数商业应用开发将其作为架构首选。\n模式描述 分层架构下的每个组件从水平上分层，每一层负责特定的职责，例如展示层、业务层。尽管模式中不强调层级的数量以及类型，但实践中一般包含这四层： 展示层 业务层 持久层 数据库层 在某些情况下，持久层会与业务层合并，尤其是一些SQL逻辑放在业务层处理的情况。因此，小的应用可能只有三层结构，大、复杂的应用则会有更多层。 每一层具有特定的角色、职责。比如，展示层负责处理UI交互以及浏览器的通信逻辑，而业务层则去处理请求过来的具体业务逻辑。架构中的每一层共同组成了围绕一个请求过来后业务处理的抽象实体。比如，展示层无需考虑如何拿到顾客的数据，它只需按既定格式将数据展示即可。类似的，业务层无需考虑格式化数据，也无需操心数据从哪里来。它只需要从持久层拿到数据，处理对应的业务逻辑（计算、整合数据），然后将数据传到展示层。 分层架构模式强大的一个特点：分离了组件的关注点。单一层中的组件，只关注该层级需要的逻辑。比如，展示层的组件只处理展示的逻辑，而业务层的组件只处理业务逻辑。这种组件分类的形式使得在我们的架构中构建有效的职责、角色变得更加容易，也使得我们使用了既定的组件接口以及范围的前提下，开发、测试、把控、维护应用更加得力。 关键概念 注意下图中，每一层被标注了关闭closed状态。这在分层架构中是很重要的概念，关闭的一层意思是：一个请求只能逐层流动，它必须从上层访问下层，不能越级。举例来说，页面上发出的一个请求，只能去到业务层，再去到持久层，最后才能访问数据。 所以为何不允许展示层直接访问持久层或者数据库层呢？毕竟，直接访问的性能是最佳的。这个问题的答案在于一个关键概念：层级隔离layers of isolation。\n概念的意思：单一层不会影响其他层的组件，即层内的组件变更被隔离了，改变仅限于本层内部。如果允许了展示层直接访问持久层，意味着SQL的改动会影响到业务层，还有展示层！！！就会造成多个组件间耦合，因为他们之间有过多依赖。这样的架构会有很高的改动成本。\n层级隔离也意味着，每一层都是独立工作的，不互相依赖，因而对其他层级内部不需要有了解。\n为了理解这种概念的重要性，可以想象一个重构任务：将JSP转换为JSF。假定展示层与义务层间的约定model是不变的，那业务层就不会受到展示层重构的影响。\n状态关闭closed促成了层级隔离，也分离了各层级的职责，但有些时候我们也需要将某个层状态打开open。比如，我们想要增加一个公用服务层，里面包含了一些原来业务层的封装（数据、字符串工具、日志工具、审核工具）。这种情况下增加一个服务层往往是种好想法，因为从架构上，它限制了服务层到业务层的访问，而不是展示层的限制。如果没有单独的层级，我们就做不到管控这种访问限制。\n在这个例子中，新的服务层处于业务层下，意味着从展示层是不能直接访问的。但这样带来一个问题，请求必须要经过服务层才能到达持久层，这完全是多余的一步。在分层架构中这是个老问题，通过创建开放的层级可以解决。\n如下图所示，服务层被标注为open状态，意味着请求可以直接穿过这层，访问其下面的层级。自然解决前面所提到的问题。\n利用open、closed状态帮助我们定义了架构层级的关系以及请求的流向，同时也为设计师、开发者提供了有用的信息，可有效理解架构中各层间的访问限制。如果各层级间 状态不记录，或者通信不做规范限制，后期就会难以测试、维护、部署。 模式案例 为了描述分层架构是如何跑起来的，我们现在想象一个查询客户信息的功能，如下图所示。 黑色箭头表示请求一直流转到数据库，拿到客户数据，红色箭头表示响应从数据库返回，一直到页面上，将数据展示出来。本例中，客户的信息同时包含了客户以及订单的信息。 UI层只负责接收请求以及展示数据信息。他不知道数据在哪里、数据如何查出来的、需要关联几张表才能拿到数据。一旦一个请求为某一客户创建，会同时创建一个客户的查询代理对象，这个对象的职责是：明确业务层哪一模块负责这个处理过程、如何转换为目标模块对象、目标所需数据（约定。下面的Customer Obj负责整合查询所需的信息。业务层调用了customer以及order的dao去持久层查想要的数据。这些模块轮流执行sql语句从库里拿到数据，再返给业务层。业务层的Customer Obj将数据整合完整，再一路返回给展示层的代理对象，进而展示在页面上。 从技术角度看，这个过程有多种实现方式。比如，在Java平台，可以用JSF作为表现层的组件。而业务层的bean可以被Spring管理或者EJB3。持久层的对象可以是一个POJO、mapper.xml文件、甚至JDBC查出的封装对象。如果是微软平台，展示层可以用ASP，使用C#调用业务方法，ADO查询数据。 考量 分层架构模式是一种稳定且通用的模式，为大多数应用架构开了个好头。但从专业角度，选用该架构后也有一些必须考虑到的点。 第一点需要考虑的就是：污水池反模式。这种反模式描述了当一个请求在没有逻辑处理的情况下逐层传递的情况。举例来说，假定展示层请求一个客户数据，请求到了业务层，在这一层没做任何操作，仅仅是调用持久层的方法\u0008，接着一条sql执行完毕，将结果再逐层返回。过程中没有任何整合、计算、转换的逻辑处理。 分层架构在某些少有情况下会遭遇污水池反模式。关键在于，要去分析有多少比例的请求是这种情形的。二八法则往往是鉴定\u0008架构质量的好的标准，即20%的请求做成了污水池反模式是比较常见的。但如果情况反过来，80%即大多数你的业务请求都做成这样，就属于问题案例，就要修改！就需要适当利用开闭(open closed)来调整这些业务代码，也要铭记，如果不分层隔离，将来做管控就会更难。 关于分层架构另一个关注点：即使我们对表现层与业务层做了部署隔离，分层架构往往会以应用为一个整体来进行设计。对于一些应用，这不是什么毛病，但他会对某些情况造成隐患，关于部署、健壮性、可靠性、性能、可拓展性这些方面。 模式分析 \u0008下面的表格包含对常见架构特点的打分以及分析。打分是基于对某特点的常见应用做自然倾向判断，包括模式的通用应用。在结尾的附录A可以查看与其他模式的比较。 整体敏捷性： 打分：低。 分析：敏捷性是指\u0008面对一直在变的环境所响应的快慢。尽管分层后隔离了层级特性，但它在修改时依然是笨重且耗时的，因为在大多数实现中，业务代码会经常出现层级耦合的情况。 部署容易度： 打分：低。 分析：这要取决于你实现该模式的方式，部署一些大应用时有可能会是一个大问题。某个很小的代码变更可能就会导致整个应用的重新部署，造成了无法排期、休息日执行的困难。也正因如此，该模式不属于持续交付的一环，进一步降低了该模式在部署方面的打分。 可测试性： 打分：高。 分析：由于该架构下的组件均只属于某一层，因而其他层是可以模拟或者彻底替换的，这提高了架构中的测试容易\u0008度。开发者可以模拟展示层组件以此测试业务层逻辑，同理也可以模拟业务层来测试\u0008展示层。 性能： 打分：低。 尽管某些分层架构性能不错，但从设计上说，其不是朝着性能这个方向努力的。因为所有请求都必须经过多层传递，这是\u0008低效的。 可拓展性： 打分：低。 分析：由于分层架构是\u0008面向高耦合、整体实现的，因而使用了\u0008分层后应用拓展性自然降低。我们可以通过将不同层分割在不同物理设备上或者将整个应用拷贝到多节点部署，以此拓展应用，但这样的粒度过大，拓展成本很高。 部署容易度： 打分：高。 分析：这里得分高，是因为这种模式是众所周知的容易部署的，因为其设计面向整体。在业务应用开发中，分层架构被广泛采用。公司内部的沟通方式与组织架构以及开发软件的方式之间的关系，在康威定律中阐述的最为彻底通透。 第二章：事件驱动架构 事件驱动模式是一种主流的用于设计高拓展应用的分布式异步模型。适用性也很高，可同时用于大应用与小应用。事件驱动架构从设计上就是高度解耦、单一事件处理（异步收取、处理事件）。 事件驱动架构模式包含了两个主要的拓扑模式： 调停者拓扑：Mediator Topology：通常在一个中心调停器中，用于协调某个事件的多个执行步骤 经纪人拓扑：Broker Topology：在不使用中心调停器时，串联多个事件 这两种拓扑的架构特性、实现都不一样，所以明白哪个更适用于我们项目很关键。 调停者拓扑 该拓扑模式面对多个事件以及多个步骤，并且需要一些级别下的协调时，就变得有用了。举例来说，买一个股票，首先要验证数据，接着检查交易合法性，将交易指派给经纪人，计算佣金，最后再把股票交给经纪人。这些步骤需要有一个协调机制，以此决定步骤执行的顺序，以及哪些序列执行，哪些步骤并行执行。 调停者拓扑模式下，有四种主要的架构组件类型： 事件队列 事件调整者 事件频道 事件处理器 事件的流程从客户端发送一个事件到事件队列开始运作，队列会将事件传送给调停，者。调停者接收到初始化的事件后，可以通过向事件频道发送异步的事件来达到协调事件的目的，发送过去后执行相应程序。事件处理器，监听事件频道，从调停者接收事件，并执行事件中特定的业务逻辑。 上图描述了事件驱动架构模型中，通用的调停者拓扑关系。 在一个事件驱动型架构内，有上百个事件队列也是正常的。\u0008架构模式\u0008并不指定事件队列的实现方式，可以是MQ，也可以是web service端，或者以上之间的组合。 在这种模式下有两个事件类型： 一个初始化事件： 调停者接收到的原始事件 一个处理中事件 调停者生成的事件，再将其穿传给事件处理组件 事件\u0008调停组件的职责：协调初始化事件内执行步骤。对初始化事件中的每个步骤，事件调停者会向其发送特定的处理事件到消息频道中，接着任务就被事件处理器所处理。PS：事件调停者并不对初始事件做任何逻辑处理，但它知道处理他们的步骤。 事件频道：负责将某特定步骤处理中的事件以异步的方式发送到事件处理器。频道可以用MQ也可以用消息主题消费模式，后者是更为常见的实现方式：因为多个处理器可对\u0008事件进行处理\u0008，每个基于接收到不同的处理中\u0008事件进行不同的任务操作。 事件处理器组件：负责应用业务逻辑的处理。处理器是独立、自包含、高解耦的组件，其在应用中执行特定的任务。事件处理器的处理粒度由小到大（fine-grained比如计算一个订单产生的营业税，coarse-grained或者处理一个保险索赔单），千万要记住：每个事件处理组件只处理单一的业务任务，并且不依赖其他的处理组件。 事件调停者可有多种实现方式，作为一名架构师，我们要清楚每种实现的原理与作用，以此来真正匹配满足需求。 最简单、最常见的实现诸如一些开源的集成组件，比如Spring Integra‐ tion, Apache Camel, or Mule ESB。这些方案的实现大多基于Java或者域定义语言。如果追求更加复杂的调节或者组织过程，你可以使用业务处理执行语言实现，其要依赖开源组件如Apache ODE的引擎运作。BPEL是一种标准的类XML语言，可以描述数据与执行的步骤，可以用于描述初始化事件。对于有复杂调节需求的大型应用（包括了人机交互产生的步骤），你可以使用BPM业务处理管家来实现，如jBPM。 对于调停者拓扑架构模式，理解需求与对应的技术实现，是决定该架构选型的关键！使用开源集成组件去处理相当复杂的业务流程会是失败的尝试，愚蠢程度可类比使用BPM去处理简单的路由逻辑。 为了理解调停者拓扑模式，可以想象你去到一家保险公司准备入保，这时你想要移动到下一步。在这个案例中，初始化的事件就可以叫做relocation重定位事件。其中的步骤已经在上面图中有描绘。对事件内的每一步，调停者都将产生一个处理事件，比如更换地址、重新计价。接着将这些处理中事件发送到事件频道，然后等待事件被对应处理器处理，比如客户流程、报价流程。在这个初始化事件被处理完之前，这些处理事件会持续进行这个过程。同一初始化事件内的不同处理事件是可以同时进行处理的。 经纪人拓扑 经纪人拓扑不同于调停者拓扑模式，因为其没有中心事件协调者，事件流顺着事件处理器以一种链式的方式进行分发，比如使用MQ的方式。当我们需要简单的事件处理并且不需要中心事件协调过程的时候，该拓扑模式就恰如其分。 在该拓扑模式内，有两种主要的架构组件： 一个经纪人组件: 可被中心化或者联邦化，内部包含了事件流所使用的所有事件频道。内部的事件频道可以用MQ、主题订阅，或前者任意组合。 一个事件处理器组件 如上图描绘的，没有中心的事件调停者，相反，每一个事件处理器负责处理一个事件，并且会发布一个新的事件来表明自己所做的任务。举例来说，一个用于平衡证券投资组合的事件处理器可能会收到一个初始化事件，名为股票拆分。基于这个初始化事件，该事件处理可能会对投资组合做重组，接着向经纪人组件发送一个新的事件，名为重组投资组合，接下来其就会分发为多个事件处理。PS：有时候当一个事件被发布，但并没有别的事件处理器来接收它。这在我们为未来功能做拓展时是很常见的。（暂时留用的意思 延续之前讲解调停者拓扑的习惯，我们假设还是在受保这里做操作，由于经纪人模式下没有中心的调停者来接收初始化事件，那这里的客户处理组件就直接接收一个初始化事件，然后发送一个的新的事件，表明其更改了顾客的地址。 在这个例子中，有两个事件处理器会关注更改顾客地址这个时间： 报价流程： 根据更改后的地址重新计算一个新的自动保险率，发布一个新的事件到频道中，同时标注其所做的操作 索赔流程： 收到同样一个更改地址的事件，但在这里，它的工作是更新保单，向频道发送一个更新保单的新事件 新发布的事件都会被另一个处理器接收然后处理，事件链条在处理完毕前会一直进行处理。 如上图，经纪人拓扑正是围绕业务功能处理所在的事件链条来工作。 为了更好地理解这种拓扑模式，可以想象其为接力赛跑。在接力赛中，运动员拿着接力棒跑一定距离，然后把棒子传给下一个选手，一直持续这个过程，直到终点。赛跑中，一旦某个运动员手里没了接力棒，他就跟这场赛跑没有关系了（已经完成了他的工作），这也正是经纪人拓扑的真谛：一旦一个事件处理器完成了事件处理，他就再也不会回到处理流程中了。 考量 事件驱动模式架构实现起来相对复杂，本质上是因为他异步分发的特质。当实现这模式时，你必须解决分布式架构的问题，比如远程流程的可用性、响应、经纪人或者调停者失败后重新连接的事件逻辑。 选用这种架构要考虑的一点：单一业务处理的原子性事务。因为事件处理器组件是高度解耦并且分布式的，所以维持同一工作单元内的的统一事务是很困难的。出于这点，当设计应用时，你必须不断思考哪些事件可以独立运行，然后据此去规划事件处理器的粒度。当你发现需要在一个整体中拆分事件处理，意思是，在一个不可分割的事务中你使用了多个独立的处理过程，这对你的设计可能就是一种错误。 或许事件驱动架构中一个最困难的方面在于事件处理器组件的创建、维护、治理。每个事件都会有一个特定的合同来制约它（比如数据值、数据格式）。在使用这种模式时，处理一些标准的数据格式是极为重要的，比如xml、json、java对象等，所以在最开始就要指定一个合约来规范这些方方面面。 模式分析 下面的列表包含了对事件驱动架构的评分、分析。同样评分是基于对该模式通用实现的考核。同样在附录A可以看到与其他模式的横评。\n整体敏捷性：\n评分：高。 分析：由于事件处理器组件是单一目的并且与其他组件高度解耦的，所以在不影响其他组件的情况下，可以做到快速隔离变化。 部署容易度：\n评分：高。 由于模式其内在的高度解耦特质，部署是很容易的。经纪人拓扑相比调停者拓扑又容易一下，因为事件调停者组件与事件处理器组件是耦合的，意味着处理器变动会造成事件调停者的变更。 可测试性：\n评分：低。 分析：单元测试无比艰难，需要一些定制的客户端或者工具来生成事件。由于异步的特性，也造成了测试的困难程度。 性能：\n评分：高。 分析：通俗来讲，异步的本质提升了性能表现，当然如果使用一些性能较差的消息组件也可能会造成低性能的实现。换句话讲，解耦能力、并行异步的操作要比消息的生产消费更加重要！ 可拓展性：\n评分：高。 分析：可拓展性也是由该模式的高度解耦、独立的事件处理器机制所造就的。每个事件处理器可以单独拓展，即允许细粒度的拓展性。 开发容易度：\n评分：低。 异步模型、合约的创建、未响应或失败响应背景下的错误处理，这些都增加了开发难度。 第三章：微内核架构 微内核架构模式（有时也称为插件架构模式）是一种为实现基于产品的应用的非常合适的模式。基于产品的应用是指：产品是打包的、可提供下载的，并且作为第三方产品有版本标注。然而，许多公司也会把内部的商业应用开发、发布出去，比如软件产品，会附带版本、发布日志、插件特性。这些都跟微内核架构天然匹配。 微内核架构允许你讲应用的特性以插件形式加入到核心应用中，在拓展功能的同时也做到了功能的分离与独立性。 模式描述 为内核架构模式包含来两种组件：\n核心系统： 通常会包含基础功能所需的组件，只需让系统运行即可。许多操作系统都实现了微内核架构，这也正是名字的由来。从商业应用的角度来看，核心系统部分通常被定义为通用的业务逻辑会包含特定的案例、规则、复杂的条件处理。 插件模块： 独立的模块，包含定制化的处理、附加功能，意味着是对核心系统的拓展、增强。通常插件都是与其他模块隔离的，当然也可以自己动手做一个依赖其他组件的。不管哪种设计，都要做到依赖最小化。 应用的逻辑在这两者间是隔离的，天然提供了拓展性、灵活性、功能隔离、自定义处理逻辑。下图描述了微内核架构基本的样子。\n核心系统需要明确哪些插件可用、如何使用他们。实现这种理念的一种办法是：插件注册。注册内容会包含每个插件模块的信息，比如名字、数据规约、远程访问协议的细节（取决于插件是如何连接核心系统的）。举例来说，一个计税软件的插件标记来某个高风险的税单审核项，那就对应会有一个审核人的注册服务存在、数据规约（输入、输出数据格式等）、格式。如果插件通过soap访问就应该包含wsdl。 插件可通过多种方式连接到核心系统，包括osgi、消息、web service、点对点绑定。连接的类型取决于你正构建的应用的类型（规模大小）以及你特定的需求（单点部署还是分布式）。架构本身不去制定这些细节，只说明了插件模块要保持独立。 插件与核心系统间的规约可以从标准规范到自定义范围选择。自定义规范往往会在第三方的开发环境下看到，在那里你通过插件无法改变规范内容。 这种情况下，我们可以在标准化规约与插件规约间增加一个适配器层，这样的话核心系统就无需对每个插件都做代码的改动。当准备创建标准的规范时（通常采用xml或者java中的map），要牢记，从一开始的版本策略一定要制定好。 模式案例 或许，对微内核架构最好诠释的一个案例就是：eclipse ide工具。下载eclipse后我们就拥有了一个强大的编辑器。同时可以自由添加插件，这时就有了定义的空间。浏览器则是另一个例子，一些查看器或者插件功能在基本的包中都是不存在的。 基于产品的软件案例太多了。也有一些大型商业应用。这次我们以购买保险为例，会讲到标准的索赔过程。 可以想象，大多数的保险索赔过程都包含了复杂的规则，其中会有对应的引擎来处理这些复杂业务。但随着规则越来越多，并且互相牵扯，最后就导致一个规则的变动就需要大量分析员、开发者、测试员来参与工作。而微内核架构就解决了其中很多问题。 上图就说明了一个索赔处理核心系统的工作情况。它首先具备了基本的业务逻辑处理能力，而没有定制化过程。然后，每一个插件模块都对基础功能做了自定义，有状态标示。在本案例中，插件的代码都是独立的，改动不影响核心系统以及其他插件。 考量 微内核架构一个很棒的点：它可以内置、包含在另一个架构模式下。举例来说，在应用中某个易变区域，产生了某个问题，如果可以用微内核模式解决，你就会发现，实现时不能完全改造成这种模式。这种情况，我们就可以在分层架构中，来嵌入微内核架构。同理，事件处理的组件也同样可以用微内核架构应用到其他地方。 微内核架构为演进设计、增量开发提供了很好的支持。你可以先开发核心的基础系统，随着应用的成长演进，就可以以插件的形式丰富功能。 对基于产品的应用，微内核架构可以是你考虑的首选模式，特别是那些随着时间会有附加功能、自定义推送用户的模式。如果慢慢地你发现这种架构满足不了你的需求，也可以将其重构到更符合需求的架构设计之上。 模式分析 下面的表格包含了对微内核架构的特点的评分、分析。评分是基于对该模式的通用实现所做的自然倾向分析。同理，附录a可以看到与其他模式的横评。\n整体敏捷性：\n评分：高。 分析：整体的敏捷性是指对一直改变的环境所具备的响应能力。在微内核架构中，变更会随着插件模块跟着低耦合的节奏大举隔离开。因而，核心系统模块会表现出很强的稳定性，因为变更都放到了插件区域，没有影响。 部署容易度：\n评分：高。 分析：要取决于模式是如何实现的，插件模块可以在运行时（热部署）动态添加到核心系统，将部署宕机时间最小化。 可测试性：\n评分：高。 分析：插件模块可以分开测试，同时功能的应用也可以在对核心系统无改动的前提下做到。 性能：\n评分：高。 分析：设计意图上，微内核架构并没有朝着性能方向发展，但整体而言，大多实现了微内核架构的应用性能都是优秀的，因为我们可以自定义或者合理化应用功能。jboss web server就是一个很好的例子：你可以把用不到的功能拿掉，把高开销的功能也拿掉，例如：远程访问、消息、捕获消费内存。 可拓展性：\n评分：低。 分析：由于基于产品的应用实现微内核架构后通常体积是很小的，他们都作为一个很小的单元运行，因而拓展性不大。当然这也取决于你实现的方式，也可以自己来做一些插件层面的拓展，不过意义不大。 开发容易度：\n评分：低。 分析：微内核架构要求大量设计理念、合约管控的支撑，这就造成了实现成本的增高。其中，合约版本化、内部插件注册机制、插件粒度、插件连接可选的宽泛度都对复杂性有所作用。 第四章：微服务架构模式 微服务正快速成为整体设计、面向服务架构之外的高可用选择。由于这种架构模式还在演进，工业界对它还有很多困惑，同时对如何实现也需要摸索。报告的本部分内容会向读者阐述它的关键概念以及使用它的取舍所需要的必备知识。 模式描述 不管你选择了哪种拓扑模式或实现风格，有一些核心的理念是必须要遵守的。首先就是隔离部署的元件。 如上图所描绘，微服务架构内的所有组件的部署都是隔离的，这就促成了更方便的部署，因为更有效率、合理、流线型生产，也提高了可拓展性，并且大量应用与组件之间是高度解耦的。\n或许理解这种架构，更重要的是理解几个字：服务组件。不要在微服务架构内去考虑服务，而要单独的思考服务组件这回事，这个组件从粒度上有可能是一个单一模块也可能是应用的一大块内容。服务组件包含了一至多个模块，其中他们或许是一个函数，也或许是包含了许多业务逻辑的应用。在合理的层面去设计微服务是一大难点。其中的细节会在服务组件协调子部分进行探讨。\n另一个关键概念是：分布式架构，也就是说所有该架构内的组件都是高度解耦的，同时通信访问使用RPC协议（JMS,AMQP,REST,SOAP,RMI\u0026hellip;）。分布式的天然属性早就了其高度可拓展并且部署上的特点。\n关于微服务另一个亮点是：它并不是出于解决某个问题诞生的，而是从另一个架构中的问题演化来的。微服务架构从以下两种架构中演进而来：\n使用了分层架构的巨石型应用； 使用了面向服务架构的分布式应用； 从巨石型应用得到的启发，源于持续交付的开发过程，持续迭代的生产线连接了开发、部署生产。而巨石型应用往往都是紧耦合的组件，内部都是可拆分部署的单元，这就造成了一整个应用过于庞杂、难以改动、测试、部署的困境。所以在很多IT部门造成了一个月部署一次的情况。前面所说的这些因素使得每次部署应用时，让应用变得极为脆弱。微服务架构通过拆分一个大应用为多个可隔离的小应用解决了这些问题。\n另一个演进的路径则是从SOA面相服务的架构产生的问题而来。尽管SOA非常强大，其提供了非并行的抽象层次、异种连接、服务协调治理、面向业务目标的承诺实现。虽然它复杂、成本高、无所不在、难以理解以及实现，但往往可以解决大多数应用的问题。而微服务解决了他的复杂性，通过简化服务的概念，消除了一些协调机制，并且也简化了与其他服务组件的连接以及访问。\n模式拓扑 尽管微服务架构可有多种实现方式，但其中三种脱颖而出了： RESTful API； 适用于网站向外暴露一些小的、自有的独立服务，方式是通过应用程序接口。 图片描述了这种情况，可以看到，这些服务组件都是很小粒度的，内部会包含了特定业务逻辑的处理模块，并且与其他rest服务是独立的。在这种拓扑模式下，这些很细粒度的服务都是通过RESTful接口访问的，在部署上会分开为独立的web api层。这种模式下的案例会包括Yahoo、Google、Amazon的一些云服务web接口。 基于REST的应用； 与前者不同，基于Rest的应用的api接口是从用户界面访问的，不仅仅是一个简洁的API层。如图所示的那样，用户界面这一层作为一个独立的web服务部署到服务器，通过远程访问的协作匹配对应需要的业务组件。这里的服务组件比前者的往往要复杂、庞大、粗粒度，往往也代表了整个业务应用的一部分，而非单一行为的服务。这种拓扑模式对小型或者中型的业务应用（复杂度相对低）还是较为适用常见的。 中心化消息拓扑； 而这种拓扑模式与前者较为类似，不过访问不要通过REST接口，而是用一个轻量级的中心化消息组件（ActiveMQ）。在学习这种拓扑模式时，能够不将其与面向服务的架构或者SOA-Lite所混淆，是极为关键的。这里的消息中间件不做任何协调、转换、复杂路由的工作，只是作为远程访问的传输路径存在。 中心化消息拓扑模式往往是在大型应用或者对传输层有更高控制需求的应用中出现。好处是利用了队列机制、异步消息、监控、错误处理、更好的负载均衡、拓展性。单点失败或者性能瓶颈往往都是在中心化的那个消息中介上，可以通过集群来解决此类问题。 避免依赖与调度（协调机制） 微服务下的一大主要难点在于：如何为服务组件选择合适的粒度层次。如果粒度过大则无法享受到微服务的好处：部署、拓展、测试、解耦。而过细的粒度则会导致服务调度问题，也就是说，这样会使得我们精干的微服务架构朝着SOA发展，过重，内部引入复杂性、困惑、成本、冗余。 如果你发现从用户界面到API层需要做服务调度，那很有可能就是粒度过细了。同理，如果对单一请求你需要做内部服务通信，那极有可能服务组件是过于细粒度了，亦或是从业务功能的角度没有分区到位。 内部服务的通信，会强制产生组件间没必要的耦合，这种情况可以用共享库表来解决。比如，如果一个服务组件负责处理订单，用到了客户信息，就可以去数据库查询必要的数据，而不是调用客户服务组件。 共享库表可以处理需要的信息，但共享功能如何？如果某个服务组件用到了另一个服务组件的功能，有些时候可以将公用的功能复制过去，但这样破坏了DRY原则。这种做法是在微服务架构内常用的一种折中方案，干掉了服务组件的耦合，而追求了代码与部署上的独立。有些代码重复了，但相对少。 如果你发现当前服务组件的粒度仍然无法避免组件的调度，很有可能这种架构不适用于你当前的项目。微服务有着天然的分布式的特性，所以很难去维护跨服务组件中的整体事务特性。这种情况需要借助事务补偿框架，来做到回滚一致，这样就给我们相对简洁的架构增加了很大的复杂性。 考量 微服务解决了很多巨石应用或者SOA下的问题。因为主要的业务组件被拆分为了更小、更独立的模块，部署也可以隔离，这增加了应用的健壮性、拓展性、对持续交付更好的支持程度。 另一个好处是，可以做到实时生产部署，这样就没必要像之前那样发布一次就兴师动众了。因为变化都被隔离到了某个特定的服务组件内部，所以部署只涉及相关的小组件。如果你只有一个服务组件的实例，可以针对用户界面应用做一些定制化，去检测热部署，并且将用户重定向到一个错误页面或者等待页面。或者，也可以部署时替换实例，这样也做到了部署周期内的持续可用性。（分层架构内很难实现） 最后一点需要考虑的，是微服务架构天然具备分布式特点，这样就会有一些与事件驱动架构共同的毛病存在，比如合约的创建、维护性、管理性、远程访问可用性、远程访问校验问题。 模式分析 下面的表格包括了对微服务架构通用特点的评分、分析。\n整体敏捷性：\n评分：高。 分析：由于部署是分开的组件单元，每个服务组件被隔离了，这就促成了更快速简介的部署过程。同时也是解耦的，也易于做变更。 部署容易度：\n评分：高。 分析：部署的特点是由于细粒度的规划以及远程服务的独立所造就的。每个服务隔离部署，也能做到随时随地更新上线。部署时的风险也降低了，即使某个实例down掉也只影响内部的一小部分功能。 可测试度：\n评分：高。 分析：由于业务功能也被隔离开来，测试范围就变小，测试的目标更加明确。回归测试也更加可用、易用。由于服务组件解耦的特点，也帮助测试链路缩短，应对变化更便捷。 性能：\n评分：低。 分析：尽管你的实现可能性能很赞，但这个架构设计不是趋于性能的，因为必须做到分布式。 可拓展性：\n评分：高。 分析：由于应用被拆分到了多个可部署的组件，每个服务组件都可以单独拓展，应用的拓展更加自如。举例来说，由于股票交易后台的用户量不多，所以管理后台可以不变动，而只需拓展交易前台的服务，这样就可以满足性能要求。 部署容易度：\n评分：高。 分析：功能被拆分为单独的服务组件，部署也因此变得更加容易，体量更小、独立的范围。某个服务组件的部署也不会影响其他组件的正常运行，部署团队间就无需有多余的协调工作。 第五章：基于空间的架构 大多数web应用遵循了大致相同的请求流：一个来自浏览器的请求先到达web服务器，接着到达应用服务器，最后到达数据库服务器。对很多小型应用来说，这种架构运行良好，但随着用户数增多瓶颈也就出现了，压力会逐层表现出来。应对这种问题一种通俗的解决方案是拓展web服务器，很多时候这种低成本的方案也会奏效。但，面对大量用户的负载，仅仅拓展web应用是不够的，压力还是会传递到服务层级。最后会导致一个三角拓扑，即web容器最容易拓展，数据库服务最难以拓展。 面对大量用户高并发的访问，数据库就必须解决并发处理事务的问题。尽管很多缓存技术、数据拓展产品可以帮助解决问题，但应用处理并发负载一直是一个遗留难题。 基于空间的架构就是为了解决拓展以及并发难题的。同样也适用于变化性高、不可预测的并发量的情况。从架构上解决极端的并发问题要远远优于拓展应用服务、在不可拓展的架构内改进缓存技术。 模式描述 基于空间的架构（有时候也指的是云架构模式）将限制应用拓展的因素最小化了。这种架构的名字来源于元组空间，即分布式共享内存。通过移除了中心数据库的限制以及使用复制内存中的数据表格达到了高拓展性。应用数据放在内存中，通过正在活动的处理组件进行复制操作。处理组件可以随着用户负载的增多减少而做到动态开启、关闭，因而做到了可变情况的拓展。也正由于去掉了中心化数据库，数据库的性能瓶颈也就没了，拓展性几乎没有上限。 大多数适用该模式的应用通常就是标准的网站，他们从浏览器接收请求并且执行一些操作。一个拍卖竞标网站就是个很好的例子。这种网站不断地从浏览器收到用户竞标的请求，当为某个物品竞标后，后台就要记录一个时间戳，并且为这个物品更新竞标的信息，最后将这个信息返回给浏览器。 有两类组件是实现了该架构模式： 处理原件（组件）； 包含了应用的组件（或者应用组件的一部分），这包括了基于web的组件以及后端业务逻辑组件。处理原件的内容会随着应用类型而改变，小的web应用往往会以单独的处理原件部署，而大的应用会根据功能拆分为多个处理原件。处理原件内通常包含了应用模块，同时伴随着内存数据块、可选的异步持久化存储（防止failover）。还会有一个复制引擎，通过虚拟化中间件来复制改变的数据（改变来自于其他活动的处理原件）。 虚拟化中间件； 负责家政、通信问题。包含了控制多路数据同步、请求的组件。在其内部有：消息块、数据块、处理块、部署管家。这些组件在下一部分会进行细节阐述，可以自定义也可以购买第三方产品。 如下图描绘了基于空间架构的基本样子，以及他的相关组件。 模式力学 下图基本说明了处理原件内部包含了应用模块、内存数据、可选的异步持久化存储（防failover）、数据复制引擎。 虚拟化中间件往往就是架构中的管理器，他要负责管理请求、session、数据复制、分布式请求处理、处理原件部署。内部有四大架构组件： 消息块 管理请求以及session，他会决定哪个处理原件来处理这个请求，并且把请求推过去。消息块的复杂度可以从简单的循环算法到判断下个可用算法的程度，后者会根据请求被哪个处理原件处理着。 数据块 数据块是该模式下最重要的组件了。他要与数据复制引擎交互，当数据开始更新时，来管理数据的复制操作。由于前面的消息块可以将请求推到任何一个可用的处理原件，那对应的处理原件就必须保证拥有内存中数据块中的数据。图中说明了数据复制是同步操作，事实上他是以异步并行的方式进行的，非常迅速，完成数据同步只需要微妙ms级别。 处理块 处理快组件是可选的，管理分布式请求处理，当有多个处理原件活动于应用的部分时，他就需要工作了。如果一个请求需要协调，比如一个订单处理原件与客户处理原件做交互，这就是处理快协调工作的内容了。 部署管家 部署管家基于加载条件来管控动态的处理原件的开闭。它需要持续监控响应时间、用户负载，当负载增加时就开启新的处理原件，同理负载下降就关闭。应用内想要做到高可用的拓展性，部署关键是一个极为重要的组件。 考量 基于空间的架构模式是一种实现起来复杂、高开销的模式。对于小型应用，如果负载是变动的，就适用于这种模式。（例如社媒网站、拍卖竞标网站）但是对于传统的大型关系型数据库应用来说，他们有着大量操作的数据，这种情况是不适用的。 尽管这种模式不需要中心化的数据存储（往往用于初始化内存数据块、异步持久化数据更新），但实践中通常会创建隔离的分区，用于隔离易变数据、常用的事务型数据，以此降低每个处理原件使用的内存。 架构的另一个名字：基于云的架构，处理原件（以及虚拟化中间件）并不一定要放在云服务或者PaaS上，它可以就部署在本地的服务商，所以不要被名字误导。 从实现的角度来看，你可以通过很多第三方的产品来实现，例如：GemFire, JavaSpaces, GigaSpaces, IBM Object Grid, nCache, and Oracle Coherence。这些实现从开销与拓展性（特别是指数据复制次数）变化特别大，作为一名架构师，你应该首先确定你具体的目标以及需求。 模式分析 同理，下面还是有对该架构模式的评分、分析。附录A可以看到横评。 整体敏捷性： 评分：高。 分析：处理原件（应用的部署实例）可以被很快的收放，应用面对变化的用户负载具备了很好的应对能力。使用了这种架构的应用往往对代码变动有着很好的响应能力，原因是其较小的应用体积以及动态变化的特性。 部署容易度： 评分：高。 分析：尽管该架构模式不是解耦、分布式的，但他们是动态的，以及基于服务的云工具，使得应用可以很容易推送到服务器，简化了部署过程。 可测试性： 评分：低。 分析：在测试环境做到高用户负载成本很高，使得很难去测试应用的负载拓展性。 性能： 评分：高。 分析：高性能是通过内存数据访问、缓存机制做到的。 拓展性： 评分：高。 分析：高拓展性能来源于其不依赖中心化数据库的特性，因此从根源直接干掉了影响拓展的因素。 开发容易度： 评分：低。 分析：复杂的缓存以及内存数据产品使得这一模式难以实现，通常也是对相关工具、产品的不熟悉。进一步讲，想要合理使用这种架构模式，就必须做更多投入。 附录A：模式分析总结 上图对各项指标、各种模式做了小结。这里会对你决定使用哪种架构模式提供一定帮助。比如，如果你架构首要关注的事拓展性，那可以看下这个图表并且细研究下事件驱动模式、微服务模式、基于空间的架构模式。同理，如果你选择了分层架构，你也可以了解到：部署、性能、拓展性可能会成为瓶颈。 这里只是提供了一些基本的了解，对于选择架构还需要研读更多内容。针对你的环境必须做深度分析，包括基础支持、开发者技能栈、项目预算、项目期限、应用规模等。选择一个对的架构是很重要的，因为一旦选好开始了开发，后期就很难去改架构了。 关于作者 Mark Richards是一名架构、设计、实现方面的老司机，参与实现了微服务、SOA、J2EE中的分布式系统等。从1983年就进入了软件工业界，在应用、整合、企业架构方面都有如虎一般的操作。1999-2003期间他担任了New England Java Users Group的主席。也是多个技术书籍、视频的作者，包括了《So ware Architecture Fundamen‐ tals (O’Reilly video)》、《Enterprise Messaging (O’Reilly video)》、《Java Message Service, 2nd Edition (O’Reilly)》、《97 Things Every Software Architect Should Know (O’Reilly)》等作品。他拥有计算机科学与技术的硕士学位，以及来自IBM, Sun, The Open Group, and BEA的多个架构师、开发者认证。他也是No Fluff Just Stuff (NFJS) Symposium系列的常用会议演讲者，在全世界对不同的企业相关的技术话题上演讲过不下一百次。不工作的时候，他喜欢去White Mountains or along the Appalachian Trail远足。 Ref 软件架构模式 Pines_Cheng 2017年06月04日发布 ","permalink":"https://redolog.github.io/posts/reading/software-architecture-patterns/","summary":"\u003cp\u003e软件架构基本模式，经典书籍。\u003c/p\u003e","title":"译：Software Architecture Patterns"},{"content":"后端大部分时候在玩儿MySQL（或者其他存储），而针对MySQL我们碰到过各种各样的问题，在此一并整理记录。\n背景信息 假设我们的问题都出在t_biz表上，这是他的表定义：\n1 2 3 4 5 6 7 8 CREATE TABLE `t_biz` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `code` varchar(50) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;编码\u0026#39;, `name` varchar(255) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;名称\u0026#39;, `status` tinyint(2) NOT NULL DEFAULT \u0026#39;1\u0026#39; COMMENT \u0026#39;状态,1可用 0不可用\u0026#39;, PRIMARY KEY (`id`), KEY `idx_code` (`code`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 Terms： SQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 -- 每个问题对应`MySQL`版本可能不一样，在看问题前需要看下版本信息： show variables like \u0026#39;%version%\u0026#39;; -- 查看服务端字符集设置： SHOW VARIABLES LIKE \u0026#39;character_set_server\u0026#39;; -- 查看校验集： SHOW VARIABLES LIKE \u0026#39;collation_server\u0026#39;; -- 服务器解码请求时使用的字符集 SHOW VARIABLES LIKE \u0026#39;character_set_client\u0026#39;; -- 服务器处理请求时会把请求字符串从 character_set_client 转为 character_set_connection SHOW VARIABLES LIKE \u0026#39;character_set_connection\u0026#39;; -- 服务器向客户端返回数据时使用的字符集 SHOW VARIABLES LIKE \u0026#39;character_set_results\u0026#39;; -- 统一设置上述三个character_set：字符集 指的是某个字符范围的编码规则 SET NAMES \u0026#39;utf8mb4\u0026#39;; -- 查看校验集：比较规则是针对某个字符集中的字符比较大小的一种规则。 SHOW COLLATION LIKE \u0026#39;utf8\\_%\u0026#39;; 类型 uint8 : 0 to 255 uint16 : 0 to 65535 uint32 : 0 to 4294967295 uint64 : 0 to 18446744073709551615 int8 : -128 to 127 int16 : -32768 to 32767 int32 : -2147483648 to 2147483647 int64 : -9223372036854775808 to 9223372036854775807 查询数字不符合预期，数字字符串转型并溢出 MySQL版本5.7.26-29-log。\n过程 我们的sql： select * from t_biz where code=43050172683600000019。 发现查出了code为43050172683600000214的数据。 字段类型为varchar。 分析解决 查询条件使用\u0026rsquo;\u0026lsquo;包裹变量即用字符串匹配时没有问题。 说明此时在数据库层面发生了cast类型转换，而且此时的查询条件超出了int64 long的长度。判断此时的隐式转型出现了截断。 验证： select cast(43050172683600000019 as signed integer),cast(43050172683600000214 as signed integer)from dual;，结果均为9223372036854775807； select cast(43050172683600000019 as unsigned integer),cast(43050172683600000214 as unsigned integer)from dual;，结果均为18446744073709551615； MySQL本身是强schema的存储系统，因此指定了字段类型时，查询要确保类型一致。 启发 对类型的边界值要敏感。\nvarchar字段应用端判断不相等，写入时报Duplicate entry MySQL版本5.6.16-log。\n过程 针对code增加uniq索引； 1 create unique index uniq_code on t_biz (code); 插入数据； 1 INSERT INTO t_biz (code, name, status) VALUES (\u0026#39;a\u0026#39;, \u0026#39;aName\u0026#39;, 1); 插入第二条数据； 此时我们java端认为 'a'.equals('A') == false，也就是说这是条新的数据。 1 INSERT INTO t_biz (code, name, status) VALUES (\u0026#39;A\u0026#39;, \u0026#39;AName\u0026#39;, 1); 报错； [23000][1062] Duplicate entry \u0026lsquo;A\u0026rsquo; for key \u0026lsquo;uniq_code\u0026rsquo;。 分析解决 首先我们在对应写数据的代码处打了断点debug。 确认了字符串的真实值。 建表时指定了CHARSET=utf8mb4，对应的校验集collation默认为utf8mb4_general_ci。 官网相关文档：5.6/en/charset-general 说明了校验集的作用之一是：Compare strings using a variety of collations.，而ci全称是case ignore。既然是忽略大小写的，那 'a'=='A' 就说得通了！ 根据业务指定需要的collation，程序中与存储设置逻辑保持一致。 启发 对字符集、校验集要敏感。\n用户输入自己的名字搜不到结果，入参输入有误 发生在2017年，排查几分钟，最后结果不是个技术问题。但是有点意思。\n过程 背景； 用户名我们暂且叫小国吧，我们的t_biz存储了用户的信息，提供一个基本的查询功能，通过名字，查到自己的数据。 提供线索、入参； 用户输入了小囯。 解决 小囯 != 小国。\n就这么简单。但是对我们的启示之一：索要线索、入参时一定要保证准确性，否则线索失真，排查成本就变高。\n启发 对入参要敏感。\n","permalink":"https://redolog.github.io/posts/rd/troubleshoot/mysql/biz/","summary":"\u003cp\u003e后端大部分时候在玩儿\u003ccode\u003eMySQL\u003c/code\u003e（或者其他存储），而针对\u003ccode\u003eMySQL\u003c/code\u003e我们碰到过各种各样的问题，在此一并整理记录。\u003c/p\u003e","title":"MySQL业务应用问题汇总"},{"content":"通用常见缩写。\nBusiness PO:Purchase Order\nSO:Shipping Order\nHRBP:HR BUSINESS PARTNER\nOKR:Objectives and Key Results\nCFR:Conversation Feedback Recognition\nSMART:Specific Measurable Assignable Realistic Time-related\nSTAR:Situation Task Action Result\nCAPA:\nST:Sell Through 通常指厂商销售给一级二级代理商\nSO:Sell Out 指直接销售给用户\nLTV:Lifetime Value\ncac:customer acquisition cost\ngmv:gross mechandise volume\ngtv:gross transaction volume\nAKA: Also Known As\nCFR:Conversation Feedback Recognition\nSOP:Standard Operating Procedure\nNPS:Net Promoter Score 净推荐值\nAdvertisement CPT:Cost Per Transaction 按成交量计费 CPC:Cost Per Click 以每点击一次计费 CPA:Cost Per Action 每次行动成本，计价方式是指按广告投放实际效果，即按回应的有效问卷或订单来计费，而不限广告投放量 DSP:Demand Side Platform 广告主需求方 CS computer science SSO:Single Sign-On SPOF:Single Point Of Failure TOGAF:The Open Group Architecture Framework CBO:cost based optimizations AOT:Ahead-of-time LCE:Launch Coordination Engineering KISS:Keep it Simple and Stupid LVS:Linux Virtual Server CMP:composed method pattern Ad-hoc: for this 1-5-10:1分钟发现故障-5分钟定位原因-10分钟处理问题 TGI:Target Group Index CDC:Change Data Capture AKSK:AccessKey 和 SecretKey BVT:Build Verification Test RDD:Resilient Distributed Dataset HPC:high performance computing bff:backend for front NTP:Network time protocal 2pc:2 phase commit 2pl:2 phase locking non repeatable read:read skew MPP:massively parallel processing SSTable:Sorted Strings Table SEDA:Staged Event Driven Architecture DIP:dependency inversion principle CAS:Central Authentication Service RAID:Redundant Array of Inexpensive Disks NAS:Network-attached Storage sata:Serial Advanced Technology Attachment rpm:Revolutions per minute: rev/min, r/min, or with the notation min−1 HDD:Hard Disk Drive SSD:solid-state drives PCIe or PCI-e:PCI Express:Peripheral Component Interconnect Express NVMe:Non-Volatile Memory Express XA:eXtended Architecture NCA:Network Computing Architecture VPC:Virtual Private Cloud mutex:Mutual Exclusion SICP:Structure and Interpretation of Computer Programs ACL:Anticorruption Layer PS:PartnerShip LSP:Language Server Protocal https://microsoft.github.io/language-server-protocol/specification JNLP:Java Network Launch Protocol CQRS:Command and Query Responsibility Segregation TLAB:Thread Local Allocation Buffer MMU:Memory Management Unit NPC N：Network Delay，网络延迟 P：Process Pause，进程暂停（GC） C：Clock Drift，时钟漂移 MTBF:Mean Time Between Failure 平均故障间隔 MTTR:Mean Time To Repair 故障的平均恢复时间 CPE:Cycles Per Element CFQ:Completely Fair Queuing JMH:Java Microbenchmark Harness oops:Ordinary Object Pointers SPI:Service Provider Interface rrt:round trip time FST:Finite-State-Transducer GPX:GPS eXchange Format RESP:REdis Serialization Protocol NVM:Non-Volatile Memory ACL:Access Control List LTS:Long Term Stable RC:release candidate ABI:Application Binary Interface DPDK:Data Plane Development Kit JDWP:Java Debug Wire Protocol RDB:Redis DataBase AOF:AppendOnlyFile KISS:Keep it Simple and Stupid RK:Rabin-Karp BF:Brute Force BFS:Breadth-First-Search DFS:Depth-First-Search MRR:Multi-Range Read https://dev.mysql.com/doc/refman/5.6/en/mrr-optimization.html CE:Computer Engineering BBT:Balanced Binary Tree BST:Binary Search Tree AVL:AVL 是大学教授 G.M. Adelson-Velsky 和 E.M. Landis 名称的缩写，他们提出的平衡二叉树的概念，为了纪念他们，将 平衡二叉树 称为 AVL树 BF:Balance Factor：左子树高度-右子树高度 CA:Certificate Authority GA:generally available KNN:k-nearest neighbours ocr:optical character recognition bst:binary search tree SHA:secure hash algorithm rt:ResponseTime CR:Code Review CL:Change List SSD:Solid State Drive(Disk) HDD:Hard Disk Drive(spinning disk | spinny disk) TBD:To Be Determined RISC:A reduced instruction set computer CISC:A complex instruction set computer LSN:log sequence number mcs:John Mellor-Crummey + Michael Scott clh:Craig Landin and Hagersten NUMA:Non-Uniform Memory Access SMP:Symmetric Multi-Processor SCM:Software Configuration Management NUSI:non-unique secondary indexes ICP:Index Condition Pushdown Optimization QOS:Quality Of Service LSM:Log-Structured Merge-Tree oltp:Online transaction processing olap:Online analytical processing rc:Run Commands/Run Control wbs:work breakdown structure \u0026lt;管理\u0026gt; 工作分解结构 SLA:service level agreement RESP:REdis Serialization Protocol SDS:Simple Dynamic String APM:Application Performance Monitoring TLB:Translation Lookaside Buffer 快表 PTE:Page Table Entry 页表 OAP:Observability Analysis Platform CRD:custom resource definition NAT:Network Address Translation TSDB:Time Series Database CQRS:Command Query Responsibility Segregation WebRTC:Web Real-Time Communication pip:Package Installer for Python CNCF:Cloud Native Computing Foundation MTU:maximum transmission unit NIC:network interface controller DMA:Direct Memory Access WAL:write ahead logging AOF:Append Only File SRE:Site Reliability Engineering cat:concatenate IaaS:Infrastructure as a Service IBM:international business machines proc:process information pseudo-filesystem JCP EC:Executive Committee JSR:Java Specification Request JCP:Java Community Process jinfo:JVM Configuration info jhat:JVM Heap Analysis Tool jmap:JVM Memory Map jstat:JVM statistics Monitoring jps:JVM Process Status Tool Visualgc:Visual Garbage Collection Monitoring Tool mat:Memory Analyzer Tool Jhat:Java Heap Analyzer Tool GAV:GroupId ArtifactctId Version: Maven坐标，是用来唯一标识jar包 SIT:System Integration Testing MVCC:Multi-Version Concurrency Control RMI:Remote Method Invocation AWT:Abstract Window Toolkit EDT:Event Dispatch Thread CSP:Communicating sequential processes TLS:Transport Layer Security IPC:Inter-Process Communication WAN:Wide Area Network EDID:Extended Display Identification Data EXIF:Exchangeable image file format FYI:For Your Information RAML:RESTful API Modeling Language NACK:Negative Acknowledgement AMQP:Advanced Message Queuing Protocol DDD:Domain-Driven Design ESB:Enterprise Service Bus AWS:Amazon Web Services PDCA:Plan Do Check Act ETL:Extract, Transform and Load ELK:ES LOGSTASH KIBANA CSRF:Cross-site request forgery JMX:Java Management Extensions mvnw:Maven Wrapper RDS:Relational Database Service ECS:Elastic Compute Service rc:run command | run configure KVM:Kernel-based Virtual Machine VMM:Virtual Machine Monitor HDFS:Hadoop Distributed File System NAS:Network Attached Storage AWT:Abstract Window Toolkit EXIF:Exchangeable Image File BLOB:binary large object ISAM:Indexed Sequential Access Method LRU:Least Recently Used RDDs:Resilient distributed datasets AMQP:Advanced Message Queuing Protocol lte:Less Than Equals gte:Greater Than Equals UUID:Universally Unique IDentifier PEP:Python Enhancement Proposals rc:runlevel control war:Web application ARchive jar:Java ARchive SMB:Server Message Block:服务器消息块 HLOOKUP:Horizental Lookup VLOOKUP:Vertical Lookup TFTP:Trivial File Transfer Protocol vsftpd:very secure ftp daemon MTU:Maximum Transmission Unit SIT:System Integration Testing：系统内部集成测试 UAT:User Acceptance Testing：用户验收测试 GDPR:General Data Protection Regulation MCU:Microcontroller Unit:微控制单元 SLA:service level agreement CAP:Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性） BASE:Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性） cdr:could-er:Contents of the Decrement part of the Register car:Contents of the Address part of the Register cons:construct SCRUM: TPS:Transaction Per Second QPS:Request Per Second Feature-driven development:FDD（功能驱动式开发） OSS:Object Storage Service built-in function:BIF TIOBE:The Importance Of Being Earnest sarg:searchable argument RDBMS:Relational Database Management System EJB:Enterprise Java Beans DBC:DataBase Commander posm:Point of Sales Materials_辅助销售材料，例如吊旗，跳跳卡，冰箱贴，地贴店招，横幅，海报，宣传单张，价格牌，瓶颈标签，吊牌等，促销纪念品、现场销售派送品、免费试用品等，堆头，促销台等 SFA:Sales Force Automation mtu:Maximum Transmission Unit nfs:Network File System:网络文件系统 dfs:Depth-First-Search:深度优先搜索 mq:message queue rpc:remote procedure call cd:continuous delivery ci:continuous integration RESTful:Representational State Transfer JMM:Java Memory Model MESI:Modified修改 Enclusive独享 Shared共享 Invalid无效 grep:global search regular expression(RE) and print out the line PaaS:platform as a service EDA:event-driven architecture DRY:Don\u0026rsquo;t Repeat Yourself BPM:Business process manager BPEL:business process execution language DSL:domain-specific language pwd:print working directory TDD:Test Driven Development LF:Line-Feed：换行 CRLF:Carriage-Return Line-Feed：回车换行 MVP:minimal viable product RHEL:Red Hat Enterprise Linux STAR:Situation Task Action Result \u0026lt;hr /\u0026gt;:horizontal rule \u0026lt;br /\u0026gt;:blank rule SIFT:software implemented fault tolerance (computing system) VFS:Virtual File System FHS:Filesystem Hierarchy Standard cwd:current work dirctory DDoS:Distributed Denial of Service NFS:Network File System MTU:Maximum Transmission Unit CSRF:Cross-site request forgery PWA:Progressive Web App SFC:single-file components RPM:RPM Package Manager_Red Hat Package Manager FHS:Filesystem Hierarchy Standard GNU:GNU is Not Unix(recursive abbreviation) GPL:GNU General Public License VFS:Virtual Filesystem JVMTI:Java Virtual Machine Tool Interface PSR:PHP Standard Recommendations REPL:R(read)、E(evaluate)、P(print)、L(loop) CAS:Compare and Swap LNMP:Linux+Nginx+MySQL+PHP POSIX:Portable Operating System Interface of Unix IEEE:Institute of Electrical and Electronics Engineers JMS：Java Message Service JNDI：Java Naming and Directory Interface SPI：Service Provider Interface MOM：Message Oriented Middleware BPM：Business Process Management SKU:Stock Keeping Unit SPU:Standard Product Unit REST:Representational State Transfer SOAP:Simple Object Access Protocol BOM:Browser Object Model POM:Project Object Model PSS:Purchase Sales Stock WMS:Warehouse Management System P2P:Peer To Peer RBAC:Role Based Access Control Servlet:Service Applet JSP:Java Server Page EL:Expression Language JSTL:JSP Standard Tag Lib Servlet:Service Applet POJO：Plain Old Java Object HQL : Hibernate Query Language CRUD：Create Retrieve Update Delete ACID：Atomicity-Consistency-Isolation-Durability DI：Dependency Injection IOC：Inversion Of Control AOP：Aspect Oriented Programming OSGI：Open Service Gateway Initiative JNDI：Java Naming and Directory Interface TCP:Transmission Control Protocol UDP:User Datagram Protocol UML:Unified Modeling Language Dom:Document Object Model BOM：Browser Object Model JSON:JavaScript Object Notation AJAX:Asynchronous JavaScript and XML XML:eXtensible Markup Language XSD:XML Schema Definition DTD:Document Type Definition xmlns:XML Name Space DQL:Database Query Language DML:Database Manipulation Language DDL:Database Definition Language TPL:Transaction Process Language DCL:Database Control Language CCL:Cursor Control Language ADO:ActiveX Data Object ORM:Object Relation Mapping JPA:Java Persistence API API:Application Programming Interface J2SE:Java Standard Edition J2EE:Java Enterprise Edition MIME:Multi_purpose Internet Mail Extension URL:Unified Resource Locator URI:Unified Resource Identifier SDK:Software Development Kit AOP：Aspect Oriented Programming OGNL:Object Graph Navigation Language DAO:Data Access Object SQL DDL: Data Definition Language CREATE ALTER DROP RENAME TRUNCATE COMMENT DML: Data Manipulation Language SELECT UPDATE INSERT DELETE MERGE CALL EXPLAIN PLAN LOCK TABLE DCL: Database Control Language GRANT REVOKE TCL: Transaction Control Language COMMIT ROLLBACK SAVEPOINT SET TRANSACTION CR IMHO: In My Humble Opinion LGTM: Looks Good To Me PTAL: Please Take A Look SGTM: Sounds Good To Me TBD: To Be Done TBR: To Be Reviewed TL;DR: Too Long;Didn\u0026rsquo;t Read WDYT: What Do You Think ","permalink":"https://redolog.github.io/posts/rd/abbr/common/","summary":"\u003cp\u003e通用常见缩写。\u003c/p\u003e","title":"通用常见缩写"},{"content":"这个系列是我16年的时候开始写的笔记，主要记录自己的想法。\n支撑000224 2025-02-12 00:00 追求更大的增长空间、更深入的业务壁垒、更稳定的关系。\n如 支撑000180 所说，我在 2021.03.27 那天离开了待了将近四年的广州（正式工作约四年）来到了北京。当时选择了走出自己的舒适区，选择了拥抱可能性与确定性。\n到今天为止，我在北京也待了将近四年，待了小米、贝壳找房两家公司。当然，也遇到了一些足以改变我人生轨迹的人。\n回过头来看，来北京值吗？\n是非常值的。 而且我判断我来北京有点晚，这也是我落后于同龄优秀的人最大的原因。\n来北京的成长：\n更专业职业的工作习惯（跨部门沟通协作、own项目） 大厂视野 系统思考能力 带人能力 汇报意识与能力 皮实坚韧的心态 业务判断能力 经过这几年的成长，我判断我当前正处于非常关键的人生节点，需要再次走出自己的舒适区。因此我选择了离开北京。\n下一站，我追求的是更大的增长空间、更深入的业务壁垒，以及更稳定的个人关系。\n未来依然充满不确定性，但我还是想试一试。\n支撑000223 2022-06-03 14:52 造神的本质是造假。\n学霸不是神，大公司也不是神，美女也不是神。\n学霸也是人，同样需要经历人性的弱点、挑战。 大公司也是公司，同样有各种职场问题、资本本性。 美女也是人，同样内在可能丑陋。 不要被神化迷惑。\n支撑000222 2022-04-08 19:28 事情陷入僵局时，就需要转机。\n转机不能期望自动出现，我们需要做一些尝试、付出一些成本，而尝试大概率要跳出原思路。\n支撑000221 2022-03-22 19:28 抽象：\n共性 层次 规律 支撑000220 2022-03-21 16:49 面向变化设计。\n变化是一定会存在的，如果在可预见的时间预期内有大概率的变化发生，设计之处就应该面向变化出方案，这种情况下无关过度设计，属于适度。\n变化有哪些：\n业务含义能否满足、能否混合抽象？ 多语言、异构技术栈是否在本期需要考虑日后的重构、迭代便利性？ 领域范围是否相对准确？ 是否有通过CR可避免的错误代码？ 模块、服务边界是否合适？ 领域限界上下文是否得当？ 迭代频率是否一致？（源码阶段是否同频） 部署资源是否同规模？（运行时阶段是否同频） 服务间的交互频率是否得当？（调用阶段是否同频） 支撑000219 2022-02-07 14:41 邓宁克鲁格效应（达克效应 | Dunning-Kruger effect）：愚蠢的人总认为自己很聪明，而智者却知道自己的无知。\n支撑000218 2022-01-22 23:33 软件系统架构：\n基础架构 技术组件选型 层次结构划分 业务架构 需求分析：针对稳定点形成系统核心能力；针对变化点做好开放设计； 概要设计：划分子系统（系统、模块、领域、接口）；明确接口协议； 详细设计：子系统功能细化；明确需求、现状、实现方案（数据结构、算法）、流程、领域； 支撑000217 2022-01-22 21:37 Life is a game. Job is a trade. Choice is a compromise.\n支撑000216 2022-01-20 22:32 区分不同维度，梳理各自的复杂点。\n比如DDD思想，一个基本思路是：\n纵向梳理系统流程、结构，如数据入驻、下单、分货，具体分哪些步骤 横向梳理业务模型（限界上下文、领域实体），如入驻聚合、订单实体 上述过程与画一个时序图类似（横轴表示时间序列，纵轴表示参与的组件以及对应操作）。\n如果系统更加复杂，上述维度可适当增多，即一般是x/y轴，如普通的时序图一般只关注时间维度上各组件、参与者做了什么，但系统变得更加复杂后，可增加z轴。\n各维度职责分离后，复杂度也被分离，架构梳理就变得清晰。\n支撑000215 2022-01-04 21:25 通过总结类的资料，可以快速了解一个人、一件事、一个组织的历史。\n本质上这个问题是信息如何快速获取，而存储系统一般是建立索引，像MySQL中B+索引结构中record_type==1的节点，其实就是大纲，而record_type==2的节点则保存了具体的索引细项，查找，必定是先看大纲再看细项。\n先大纲，再细项。\n支撑000214 2021-12-14 15:25 复杂!=杂乱。\n复杂应当是问题属性，杂乱是错误的方案属性。\n支撑000213 2021-12-07 21:28 做事切忌只把自己当成局部成员，要有全局意识。\n此为owner意识。\n支撑000212 2021-11-08 12:26 静下心，更专注。\n注意力不集中是我从小的一个问题，至今仍然影响我的水平发挥。\n支撑000211 2021-12-06 12:59 代码分两类：\n技术 业务逻辑 要想在业务组（通常中的现实）有产出、成长，必须先搞定第二类。\n支撑000210 2021-11-08 12:22 在资源调度方面，线程想的是尽早抢占，早占早用，系统想的是尽量公平排队，人人有份。\n线程没有调度逻辑，只适合干具体的活儿，抢占假如失败，你承担的成本更高。而系统则有效率方法，排队其实是效率更高的方案。\n做人，要做系统，不要光做线程。\n支撑000209 2021-10-18 20:38 生物体内充满激素。\n激素决定了我们的感性体验，但是我们应该控制、利用好激素，而不是被激素控制。\n支撑000208 2021-10-18 20:37 没有男人女人，只有资本家跟打工人。压榨源于利益冲突，性别本事不存在冲突。\n支撑000207 2021-09-08 17:08 需要的东西，预算范围内买最贵的。\n支撑000206 2021-08-18 17:35 不只解决一个问题，要解决一类问题。\n归纳归类，是通用、批量的解决问题的思路。效率比解决问题高，产出多。\n支撑000205 2021-08-04 13:10 退一步评估价值。\n抽烟危害大吗？看跟什么项目比，退一步讲，抽烟的危害，比黄赌毒的危害可小多了，所以抽烟不一定是坏事。\n黄赌毒的危害也是依次递增的，所以单拿一项，无法评估价值。退一步，需要有项目、情况间的比较。\n支撑000204 2021-08-03 22:16 虎父无犬子：上一代牛逼，下一代就差不到哪里。\n上一代是大部分人的一个水平下限。\n支撑000203 2021-08-03 22:14 小问题大家都会解决，小问题组合起来的大问题就不是大家都能解决的了。\n这句话最开始出自高中物理老师。与我最近花钱买的一份经历不谋而合了。\n当你不把问题复杂化时，其实你是可以解决掉的，这时候碰到诱惑是极有可能抵挡得住的，因为你还没有被吸引进去。一旦你的意志开始松动，小问题就转化为了复杂的大问题，而这其中的诱惑慢慢隐藏了起来，你开始踏入了黑暗，此时的诱惑你看不清，更加无法抵挡。\n所以解决问题的正确思路是，将大问题拆解为小问题，逐一击破。同时也要提高面对复杂问题（诱惑）时的定力。\n支撑000202 2021-07-10 09:08 对了解的领域，一般人都知道追求确定性，因为确定性带来安全感。\n对不了解的领域，一般人其实都在盲目追求不确定性，因为被带歪了，第一反应都是觉得不确定性带给自己利益，实则毫无益处。\n支撑000201 2021-07-04 19:27 主动沟通不是对别人的期望，而是对自己的要求。\n昨天跟乔总聊，我们互相讲了点对方的缺点，也聊了一些共同的朋友相处的事。发现其他朋友发现我的缺点的时候并不会选择主动找我沟通，而是选择远离、排斥我。而我发现了别的朋友的缺点的时候，以前我也不会主动去沟通（做过尝试，被拒绝了）。\n工作中主动沟通有益于进度推进，而关系中主动沟通，则有益于问题的修复与关系的长久维系。\n发现了问题要主动点去找人沟通，而不是憋着。\n支撑000200 2021-06-28 23:00 摘抄：这意味着，你既懂得理解别人的需要，也知道照顾自己的情绪，既愿意付出又有底线，既能够坦荡地靠近，又能够在正确的时间做出取舍，这一切是真的需要学习的。\n结合最近的生活体验，我感觉承受着不小的工作压力的人，把生活照顾好难度会高一些，甚至就是这样，错过了一些人跟事。可惜但是也理所应当。\n所以饿的时候、困的时候、累的时候、烦的时候，别做关键的决定，也别去扩散自己的坏情绪。照顾好自己的身体与情绪，再去做事、爱人。\n支撑000199 2021-06-09 13:31 精力管理\u0026gt;时间管理。\n支撑000198 2021-06-06 09:08 追求中位数，不追求最大数。\n支撑000114中提到了求其上得其中，这两点并不矛盾，我们最终想要的其实就是中。\n大部分人能力不足水平有限，非要强求自己去做力所不能及的事，最后只会事倍功半，不如开始就做自己做的到的事。\n支撑000197 2021-06-05 18:29 人可以精于计算，但不可精于算计。\n计算是对面临的各种目标（身体健康、情绪稳定、家庭幸福、被动收入）、条件做权衡。\n算计是在面临钱权性的诱惑时忽视、背叛人心。\n支撑000196 2021-06-05 18:27 把身体搞好。 把业务搞好。 把关系搞好。\n支撑000195 2021-05-28 08:46 提升运气值的途径之一：做好事、做好人。\n运气这个词汇，带有玄学、不可证的特点，但是背后一定有逻辑机制。从逻辑上无法找到精确的办法可以提高我们的运气（如果可以的话那我早就发财了）。但是持续做好事传播善意一定可以让自己变得更好，也可以提高别人对自己的印象分。\n或许关键时候就有回报（其实不求回报），也就提高了运气值。\n支撑000194 2021-05-15 08:49 什么阶段解决什么阶段的问题。\n不要在当下解决未来才能解决的问题（可以思考、准备，不需要立刻解决），也不要在未来解决当下错过的问题。\n现在考虑未来，是为了未来做准备，等到未来解决问题的时候，现在的准备是有用的。\n未来考虑现在，只是为了review，不是为了replay。\n支撑000193 2021-05-11 09:16 交朋友，交的是平时的舒服与理念的认同，而不是一时的心动。\n去年分别买了一双耐克与阿迪的鞋，耐克的鞋最开始特别喜欢，后来越穿越发现气垫进水的问题，不舒服，而阿迪是初见不太满意，现在越穿越舒服。\n以后买鞋买阿迪不买耐克。\n支撑000192 2021-05-06 20:38 把人当人，就已经是对人足够的尊重了。\n很多时候的打标签，比如对某某群体有什么偏见，其实就是没首先把人当人看。\n对小孩儿也是，把小孩子当成独立个体、独立的人，在内在认同上就已经对ta有了足够的尊重。\n支撑000191 2021-04-30 21:08 能困住我的，只能是我自己的心态。\n以前我觉得一个东西重要，那ta就会制约我，现在我觉得这个东西不重要了，那我就自由了。\n而这个东西是否重要，取决于我的能力跟心态。\n支撑000190 2021-04-20 13:09 没退路，没保底，没实力，就不能赌。\n不是说不能赌，而是赌之前要把前面这几个补足。\n支撑000189 2021-04-20 13:08 代码工程，没有最烂，只有更烂。\n生活、人也是，没有最烂，只有更烂。\n支撑000188 2021-04-18 13:38 三元悖论。三个条件只能满足两个。\n支撑000187 2021-04-17 10:51 我不追求完美，我追求真实、真诚。\n支撑000186 2021-04-17 10:47 我们假设时间是一种宝贵的资源，在这样的前提下，做事的质量要远远比数量重要。\n谈恋爱多的人不一定获得了有效反馈，获得了有效反馈又不一定让人得到提高。\n声称自己谈次数多的人没搞明白手段跟目标的意思，得到提高才是目标，而谈只是手段，谈得多不多跟效果好不好是两码事。\n可以谈得少，但是一定要有质量。\n支撑000185 2021-04-16 20:16 降低期望。\n在没有充足的情报的前提下，期望一般是略高的。获取足够的情报，有助于我们摆正期望。\n支撑000184 2021-04-10 20:38 进度、关系状态，要么进一步，要么退一步。\n停在原地是一个项目、关系中最糟糕的状态，要主动争取进一步或者退一步。\n退了一步的时候，其实也是进步。\n支撑000183 2021-03-31 22:54 生活不能replay，但是可以review。\n从现在回到过去：就是replay。\n从现在看过去：总结经验并且做好当下为未来助力，就是review。\n支撑000182 2021.03.30 08:37 生活中有很多去魅的事发生，而去掉魅力之后，其实你会找到ta真正迷人的地方。\n之所以要去魅，只是因为你初期对ta的认识或者期望有偏差，而后面你可以探索到的真正的魅力，会更加迷人。\n所以要大胆去魅，大胆去追求这个世界更迷人的地方。\n支撑000181 2021.03.22 23:13 真心是比现金更宝贵的东西，要省着用，要用对地方。\n支撑000180 2021.03.22 22:22 追求确定性与可能性。\n自2020.09.28头一回在京味一品吃了一口炸酱面，我就扬言我一定会去北京。因为这个炸酱面能让我分泌多巴胺【大概就是快哭的程度】，这是确定性。像我喜欢骑车，也是因为确定性，因为骑车能让我分泌内啡肽【更加持久】。\n后来我花了大概五十天冷静，怕自己一时冲动，但是2020.11.19，也就是五十天后，我还是觉得我得认真对待去北京这件事。因为我觉得去北京会有更多的可能性：有更多的老同学以及朋友，有更多自己喜欢吃的（最主要是在广东没有人能跟我一直吃面，也确实没找到那么好吃的面），有更好的互联网工作机会。而这三点，社交圈子、美食、互联网，随便拿出一点，都让我难以忘怀（难受到烧心）。\n但是后来我也没实际做什么（可能性）。直到过年（有一些外部推力），我投了一下北京的岗位，结果拿到了好几个offer（确定性），其中当然有我心仪的。人在差不多的时候被推到了更前的位置，这时候就不会回头了。\n所以，去北京，起于想追求确定性与可能性，并且过程以一种确定并且有可能的节奏进行，终于什么，我不知道，但是我想试试。\n我追求的是，确定性与可能性。\n支撑000179 2021.03.22 22:21 股票投资、找队友、个人成长，就这么几点：\n看准 重仓 拿住 支撑000178 2021.03.17 12:35 投入总会有产出。\n越是基础性的设施建设，越需要大的投入，想要把事情做成，往往需要投入到基础建设上。\n往基础的地方投入，短期受损长期受益。需要格局与定力。\n支撑000177 2021.02.19 23:01 风险共担。\n投资、找人，要更关注风险。\n支撑000176 2021.02.17 21:00 我要懂珍惜。\n我应该珍惜现在拥有的物质以及精神力量。相比之下，精神力量更加宝贵。\n爱太珍贵了。不珍惜不行啊。\n支撑000175 2021.02.13 23:31 彼此承担后果才是真正的负责任。\n几年前就觉得：我如果可以承担某一个选择带来的后果，那么我对这件事以及对我自己，就是负责任的。\n现在发现：除了要自己心里走查一遍，还需要与队友确认，是否认同这个逻辑以及是否可以承担这个选择带来的后果。\n这样，才是对整件事对对方也是负责的。\n支撑000174 2021.02.13 23:25 照顾队友感受。持续沟通解决问题。\n这是我2020年到现在与人加强连接后得出的有效方法论。\n队友可以是亲人、朋友、同事、战友，越是亲密的人，越要照顾对方的感受，并用行动改善彼此的感受，其中就包括表达自己的需求、感受。\n而持续这个过程，则演绎了沟通解决问题的模型。\n支撑000173 2021.02.13 23:14 关于发财，算是很多人口头上的一个梦想、目标。但实际上做到的人寥寥无几。做到的人，就已经不算是口头梦想，而是天时地利人和都兼备了。\n昨天与老爸比较深入的聊了会儿天，给我讲了一些他20-40岁的一些所见所闻所经历。发现他现在能挣一千万的老同学，在二十年前的时候起码可以挣十万。\n这种人对于目标从来不是放在口头上，而是付诸行动，而且用结果说话、用实力说话。\n而且强者恒强。如果我现在26岁了赚不到十万块，那我二十年后一定赚不到一千万。反之，则有可能。\n支撑000172 2021.02.10 15:19 分清变与不变。\n软件架构、模式设计上，针对变与不变的部分要做出识别、拆分。做不好，代码就容易乱，设计就会不够整洁、优雅。\n支撑000171 2021.02.07 22:58 未建立制度的情况下，印象分\u0026gt;体系分。\n作为组织的leader应该完善制度，因为制度下的体系分更加科学，更能体现客观表现。\n作为组织内被考核的成员，应该关注团队此刻更加偏重的目标。\n支撑000170 2021.01.31 10:30 亏钱要趁早。\n人生是一场无限游戏，越早越快往前推进进度，越早习得经验。\n支撑000169 2021.01.24 11:31 利益关系，比盟友关系更靠谱。\n简单来说，通过合同约定的交易，比口头协商的联盟，更能有效实现双赢。\n利益，是中性词。追求利益是人类目前一直以来的追求。做到这个过程的高效，则体现了人类的智慧。\n合同约定比人更加靠谱。合同才是承诺，口头承诺不算。\n支撑000168 2021.01.24 11:23 人一旦拥有了钱权性这种硬通货，恶就会成倍放大出来。\n所以有人会说创业、炒股都会把你的优点缺点放大，是一样的逻辑。\n但是这句话还有后半句，智人是有进步学习能力的，发现了问题，有纠正的能力。\n所以人之初性本恶不是问题，发现了问题视而不见才是真的问题。\n支撑000167 2021.01.21 13:21 只有真正付出成本，才有入门的可能。\nI learned several lessons by losing money.\n从教训中吸取经验，教训就是成本，经验就是收获。前期畏于付出，后期就难有这部分对应的成长。\n支撑000166 2021.01.05 22:20 定位清楚，守住边界。\n摘抄自KnowYourself：\n明确对一段关系的期待 处理好「性吸引力」在关系中的角色 设置边界，寻求共识 采取合适的方法应对外部挑战 一段好的关系的特征：\nEqual 平等 Reciprocal 互惠 Respect 尊重 2020年，很幸运遇到了（或者开始沟通这类话题）能在情商方面指导自己的朋友。对朋友、关系的认识又进了一步。\n支撑000165 2020.08.15 00:02 现代化工作核心是协作，协作核心是沟通，沟通核心是信息互通。\n信息互通，向上多汇报多做所见即所得的事，向下多筛选归类多做易于凝聚团队的事。\n支撑000164 2020.08.14 23:56 世界是复杂的，需求是复杂的，复杂需求需要多方面满足。\n单场景的实现，无法满足所有场景需求。\n支撑000163 2020.07.03 08:03 变态需求，变态解决。\n切忌用复杂方案解决复杂问题。\n支撑000162 2020.06.02 23:43 做对的事 \u0026gt; 把事情做对 \u0026gt; 做多的事 \u0026gt; 不做\n支撑000161 2020.05.24 23:00 理论学习支持你去理解技术原理，实战过程帮助你掌握工程实现细节。\n支撑000160 2020.05.22 23:18 不耽误。\n很多事同时进行，是不耽误的。\n举例说明\n你学了Java，想学Golang，这不耽误，二者是补充关系，不是非Java就Golang的替代关系（这里不探讨后端技术发展的晋升路线）。 你有理想，也要面对生活，这不耽误，理想跟生活在我看来是可以兼得的。 工作不顺心了，碰到了困难，能解决的就尽力解决，解决不了的，到最后一关，你还可以想：this is just a job ，天塌不下来，而你在成长。碰到不顺心的人，能合作的就好好合作，合作不了的，到最后，很多路人只不过你认识世界理解世界的一个样本而已。所以为了这些，不必过度劳心。事情最后可能不能做到一百分，但是经历一定是可以反馈到经验上的，所以整个过程是一定会有收益的。\n这些事，谁都不耽误谁。\n支撑000159 2020.05.22 23:18 做成事，既要有理想与能力，也要有手腕。\n支撑000158 2020.05.21 23:50 人跟人之间有了利益关系，一切就都变得复杂了起来。复杂是这个世界的本质，带来了挑战跟机遇。\n如何应对复杂性，将决定创造的价值大小、高低。\n支撑000157 2020.05.08 23:53 秩序产生效率，制度维护秩序。\n设计核心流程的制度，至关重要。\n支撑000156 2020.05.03 23:49 宗教、等级制度是一种政治工具。同理，饭圈、娱乐工业是一种赚钱模式。\n在前者环境内，要争取不做低等级参与者。在后者环境内，要争取不做买单者。\n只有高阶参与者才能有的赚，低阶参与者只能被PUA。\n支撑000155 2020.05.02 17:47 买得起的是需求，买不起的是欲望。\n需求可控，欲望不可控，欲望要么消耗团队成本，要么效果不好。\n要提高做需求的能力，减少欲望。\n支撑000154 2020.05.01 23:59 分清目标与手段（实现方式）。\n像SMART法则，是用来定目标的，而不是定手段的。比如，我要考中山大学的硕士，细化下来，就是目标。根据目标，再定手段。\n从我观察的有限样本来看，前期阶段大家都不太会混淆目标与手段，往往进入了中间阶段，进入了状态，头脑中对手段的定位不够坚定了，调整不够弹性了。\n目标与手段都可以应变调整，但不可混淆。\n支撑000153 2020.05.01 13:15 做成一件事，需要多方面的支撑。\n比如工作，有完全各方面都满意的Offer吗？可能会有，但是概率太小，我视为没有。\n那我想要追求的目标，就是达成，达成我的职业发展、持续输出、挣点钱。\n为了达成目标，首先我要接受现实上的限制，并不是与自己达成和解，而是接受。其次，就是要找到别的方面的乐趣。\n一件事，有无聊无趣甚至恶俗的部分，也必然有好的、有趣的、值得品味的部分。\n两方面再拆分细致一些，就是多方面的支撑。\n支撑000152 2020.02.20 22:00 信心来源于实力。\n小时候经常受到别人的鼓励：要对自己有信心！\n这当然也利好于状态。但这种信心是虚无缥缈的。真正的信心只来自事实与逻辑。\n比我差比我牛的人都能给我信心，因为都能帮助我更加看清事实与自己。\n支撑000151 2019.12.31 22:12 经历+总结=经验。\nPMO出金句，但我琢磨了下，这玩意儿必须还得实践，不实践都是假的，所以我给补一句。\n经历+总结+实践=经验。\n支撑000150 2019.12.20 23:50 世上无难事只要肯放弃。\n对于不适合自己的东西，趁早放弃，不必煎熬（承受困难）。找个适合自己的，才是正道。\n比如我花了挺久才琢磨明白，我是不爱跑步、不适合跑步，并不是我不自律，我喜欢骑车，骑车照样锻炼身体，追着风还开心。\n支撑000149 2019.11.16 23:09 做人做事的通用能力在跑赢大盘上至关重要。\n这一点是看了小叶先生跟58沈剑的观点后所得的启发。\n作为软件研发工程师这个岗位，态度、格局、积极性、项目管理、表达能力，这些是解决通用、特别问题的基础能力。也是我之前提过的「套路=基本素质」，所以在提高技术的同时，提高这些基础素养也要重视。\n本页面所有观点不仅适用于职业发展，也适用于生活的方方面面。\n支撑000148 2019.11.08 19:49 接手了未测试过的别人的代码，要全部扔掉。\n未测试过的别人的代码=坑，理解或者测试成本都远高于自己重写。\n支撑000147 2019.11.08 19:46 CBD陷阱。\n支撑000131有提过CBD不代表水平高。而对于CBD搬砖的一般白领来讲，如果没有必须得是CBD才能提供的条件，那CBD对于个人的成本则更高：\n通勤拥挤 餐饮费略高 容易有「自己牛逼」的幻觉 支撑000146 2019.11.03 13:20 套路只是基本素质。\n基本素质的意思就是，你要掌握这项技能，但它不能是核心能力。比如与人协作的过程，会使用各种工具，IM跟Jira就是两种场景，不会用就是不懂套路。\n也要警惕把自己套路进去（纸上谈兵）。\n支撑000145 2019.11.03 00:18 二线城市就是二线城市，跟一线城市的差距是无法抹平的。\n但人选择过活的城市时，要清楚哪里的核心价值更适合自己。\n支撑000144 2019.11.03 00:16 别人的东西不直接跟我有关系。\n人都是利益导向的，我对你没有好处，你就不会靠近我。\n利益很重要，追求利益也很重要。获得利益，就是获取力量。没有力量的弱者无法变得更强。\n支撑000143 2019.11.03 00:03 人的样子是一开始在选择赛道的时候就已经决定了，影响人的环境比目前的状态更适合做判断依据。\n有些人「油」，那他年轻的时候就已经「油」了。\n支撑000142 2019.10.28 22:41 越练越强。\n以健身为例，只要动作不错，练越多就会越强。如何保证动作正确？花点钱问问教练即可。但是健身（其他事也是）最大的难点却是大部分人无法坚持。\n所以「练」难的是坚持。只要坚持，就会变强。\n支撑000141 2019.10.20 10:13 变通能力极为关键。\n只要有多方沟通、对接的情况，人就是占主导地位的，进度能否推进，除了看做事方法论是否稳健，还要看随机应变的能力。\n支撑000140 2019.10.20 10:10 编码技术与带人能力确实是两个门路。\n但反过来说，一个徒弟带不上去，有可能是师傅的问题，也有可能是徒弟的问题。我们只能探讨这个徒弟是值得挖掘的，接下来是配一个有带人能力的师傅。这一点在面试中需要考虑进去，因为前面挖了坑，有时候后面是补不起来的。\n支撑000139 2019.10.19 11:11 感觉与事实都重要。\n人活着，感觉跟事实在很多时候是一回事。\n支撑000138 2019.09.22 17:45 既要能写好编码细节，也要对要做的事、应用有较全局的认识。\n内部系统处于新老桥接的阶段，我处理到的部分充满了不顺利的味道。先发出一些疑问：\n为什么不是基于旧系统进行改造？而是要完全更新换代（毕竟使用的语言\u0008、技术栈还是一致的）？ 为什么新系统有部分功能上线一年了竟没有投产？ 为什么新系统在设计时不全面考虑业务线完整的流程？方案没有全覆盖？ 为什么在过去跟我说要避免各种奇技淫巧的资深工程师在目前负责的工程中也有使用奇技淫巧？ 做的应用量级去到了中型级别，人员之间的沟通至关重要，负责人应当经常对一线的工程师阐明应用的目标、开发的规范性，由上到下的沟通比一人独写对团队的整体产出要更为重要，而开发者之间的工程性沟通也要紧密、清晰。\nCode Review对于工期紧张、团队战斗力还不够统一的情况下，极容易变成一种形式，即审核正确，这样的敏捷要不得。\n支撑000137 2019.09.22 17:41 精力、效率是决定出活的关键，而时间不是。\n住郊区即使有地铁，时间上也是增加了成本，这部分的时间我觉得可以有两种补救策略：\n管理时间：安排通勤期间的学习 管理精力：利用通勤补觉 即使有了第一条，也要铭记原则：要保持工作时间内的效率是第一要务。\n支撑000136 2019.09.15 22:39 关心自己最为关键。\n关键阶段，多为自己考虑、行动，是负责的表现。\n换个角度讲，做好自己，才能不拖累（帮助）别人。\n支撑000135 2019.09.15 22:32 思路决定出路。\n思路如果不合适，一周或者更多的工作量就浪费了，当然可以有所止损，但整体上，这个事就是亏的。\n工程师的职业精神要求我们体现专业性，更多的人（包括现在的我）会犯错误，\u0008觉得动手更为要紧，从而少了思考，前置的深入思考实际可以为我们降低后期的成本。\n应该先做设计，评估，最后才是动手。\n支撑000134 2019.09.15 22:28 好了伤疤忘了疼。\n我又度过了本年度的一劫，痛了三周多，现在恢复完毕，确实舒服了，感觉上自我也复现不了当时的痛感，但我可以记录下来，提醒自己有过这种痛。\n进一步也有更实际的行动：饮食规律、清淡；注意补充水果蔬菜；健身；保持良好的心态。\n记下来，不会疼，但伤疤要规避。\n支撑000133 2019.07.30 22:37 物理学家研究星球运转的规律，但却改变不了这些规律。\n规则只能先通晓，再在其合理合适的边界内进行自己的工作。\n支撑000132 2019.07.30 22:33 信息只流通于圈定的范围。\n组织的结构决定了物理分布、工作方式、信息流通形式以及范围。\n相关信息仅限于相关人员之间流通。\n支撑000131 2019.07.19 19:03 CBD工作、会讲英语的国人不一定素质高。\n照以前我印象中的判断，这么个\u0008环境中生产的东西，一定是高级以及高效的。实际我在环境中浸染后，发现完全相反。系统低效、人也是千奇百怪。\n做判断所依赖的材料，不能死板。所谓的学习能力，就是判断能力。表面的高端，背后完全可以是低端的。\n支撑000130 2019.07.19 18:57 @\u0008周剑：一个棘手或者难以解决的问题，可能可以解，可能无解。但是解法肯定是理性分析，一点点解决。情绪化肯定不是，消极肯定不是。\n@大花花奶奶：#真话难听#\n企业不是你的对立面；你的对立面是市场上那些和你一样能干好用愿意加班，而工资和你一样甚至更低的人。 员工不要以自己主观感情去看待企业，企业也不要给自己加一些“情怀”“大家庭”的戏。 一个有“自我负责”意识的成年人，埋怨的情绪只是无畏的内耗，他/她需要考虑的是：如何离开不满的现状。 支撑000129 2019.06.28 12:05 见不同的人，处理更多的事，经历多了才能沉淀覆盖率足够高的方案。\n你只要活着，只要每天做点事，见不同的人，就会发现，总有你一时间忘掉或者压根没见过的人性、行事思维与习惯。\n面对这些，进度还是要继续推动下去的，那就吸收这些好的坏的案例，提升自我以后面对这种情况的处理能力。\n支撑000128 2019.06.28 11:59 面对已经发生的坏事、故障，先聚焦解决问题，再回头复盘提升。\n人不像机械、程序，人做的事有很大灵活性，需要临场发挥，是个自编程、实时编程的场景，而机器，没有神经，本质上是\u0008编好的码按照既定规则重复执行。\n后者\u0008已经是目前人类追求增量生产力的主要途径，提高工程能力就是其中的一大关键。\n前者则需要人来解决，只有人才有思考对话能力。\n当时就该集中精力解决问题，事后再\u0008\u0008尝试去坏点，这样时间效率最高。\n支撑000127 2019.04.30 23:42 支撑000115中提到的：Done is better than perfect。\n有两个事实启发：\n下班与同事等公交时有俩老外过来问路，我用蹩脚口语指了商场，继而觉得自己当年的口语是不是白学了，因为课程首先教的是发音、情景用语，而我当时派上用场的，却是初中时已经会说的几句简单口语。国内英语教育，我一直也觉得，最大的问题就是无用，派不上用场。沿着这个思路，那想办法用上是不是比考证要靠谱多了。 现公司创始团队做事的风格、效果，确实有我值得学习反思的地方，为什么同一个项目我评估并做出来是比较悲观的，而他们可以更快做出来，心中疑虑似乎少很多？ 多做，一方面积累经验能力，一方面减少心中的疑虑。\n支撑000126 2019.04.01 13:33 好的心态利于完成度的提高。\n看《人件》《人月神话》《软件构建之法》，首先可以预先（未参与项目前）了解到项目运作的真实情况，其中的难点有所说明，解决方案也有所探讨。\n客观上的难点是无法避免的，那解决方案就至关重要。在人这一面，调整心态是有必要的，会利于进度的推进。反之，则会带来负面作用。\n支撑000125 2019.03.27 13:56 选择的优先级：预期\u0026gt;\u0008当下的利益。\n支撑000124 2018.12.21 23:46 \u0008自省。\n\u0008类似于自律这种话题，喊口号的成本真是比行动要低太多了。\n所以这种事，关键在于要用对方法、建立机制。\n谁不知道自省对自己有好处？\n支撑000123 2018.12.21 23:39 你想要去菜市场买个西瓜，结果到了地方看到一群人在买白菜，于是你买了白菜回去了。\n这是个惯有事实逻辑。在我看来，做事的思路不应该是这样的。\n你忘了自己来菜市场干嘛了吗？你是要买西瓜的。当然在生活中，买菜这么小的一件事，完全可以随心情、随菜果行情来实时调整。\n但面对人生比较严肃的选择时，大多数时候我们会忘掉，自己来这儿是干嘛的，忘了自己想要的是西瓜不是白菜。\n所以别忘掉自己要买西瓜，买个西瓜再回家。\n支撑000122 2018.12.15 22:55 做事，\u0008要有完整思路。\n可以达到目标，可以优化，可以做。\n做人，要有基本原则\u0008。\n有底线，真诚。\n支撑000121 2018.11.24 17:10 机制优于机器，机器优于人工。\nAI算一种抽象出的机制，当然他比较消耗算力，但解耦了一部分生产力。\n典型的机制就是，一家公司的制度、模式、运作成员。\n人工的典型特点就是，堆量，效率无法得到指数提高。\n效率差距大概相当于资产与工资的区别。\n支撑000120 2018.11.15 19:54 存索引不要直拿数据，记策略不要背密码。\n很多人都忘了或者压根没记得，IT是InformationTechnology的缩写，那么信息技术，核心也就是信息了。我们如今看到各种各样的产品，究其根本，也都是对信息做不同程度、形态的处理，前后端都是如此。（只不过对流程、逻辑的提炼也相当消耗计算、存储能力，所以这些事都有很大价值）\n那我们做基本的业务开发的人，对一些业务场景基本都能有还算完整的解决思路，应用到生活也完全可以。\n举个例子，记单词有一些方式方法，那么生活、工作同样也有，就对信息、数据的处理上，这里只拿出来一点：对数据进行合理的索引。\n我的一个习惯：该记录到云笔记的就写过去，该存到git仓库就push上去，该提炼数据生成策略的就定过去。\n支撑000119 2018.11.10 01:20 约定大于配置，水平高低大于工种差异。\n前者大于后者的原因，就是事实。事实上，前者就是强于后者。\n当然，我也不是第一次强调，在不同人的认知背景下，对一个定义的理解显然也会有差异，那么我这里的观点，自然是基于我的认知。强，也源于个人的信念、信仰，通俗来讲，你信什么，世界就是什么。\n目前我当然信水平高的。\n支撑000118 2018.07.21 18:15 良好的生活习惯，可以少得很多习惯病。比如高血压、视力问题。\n这次到爱尔做手术，是今年第二台大手术，做的时候不痛，但晚上回来就痛了四五个小时，并且流泪流鼻涕，用了整一包纸。\u0008过程中会有些紧张，做完又会有几个小时痛苦期，这时候就会后悔：为什么要自己选择遭罪？跟上次手术是一样的。\n但过了这个阶段就不同了，良好的身体状态告诉我：这种该用手术改良的状况，应当听从理性的建议。\n良好的习惯，重在维持。\n支撑000117 2018.07.15 21:25 抓重点，抓得住重点。\n「既要低头拉车，也要抬头看路」，为什么不是「既要抬头看路，也要低头拉车」呢？\n因为倒过来的结果，有可能事情没人去干，光打了嘴炮了。而低头拉车，则是很多项目的现状，你拿了工资，不干活是不可能的，那问题在哪里？这么多项目，为什么难做好？在时间、精力、业务之间，有什么是难以推动的？其中一点必然是管理，PM作为带队，管理能力在有挑战的工作中会体现得更加清晰。\n所以大多数项目问题，是管理问题。错误地把有限的人力都放在了错误的地方，比如纠结细节。结果就是，进度不达标，质量跟不上。\n怎么解决？找个靠谱leader，组个靠谱团队，不要瞎干。\n要抓得住重点。\n支撑000116 2018.07.01 09:52 用脚投票，我，看脚的数据进行决策。（撇开运气不谈）\n今年进度正式进入了后50%，进度的推进分两派：\n嘴派 脚派 前者就是光喊口号不干，后者就是不喊但实干。我属于哪派呢？\n我当然是该喊则喊，该干就干。避免这个「程度」讨论界限\u0008不清晰导致话题谈不下去，我在「推进度」这件事上，\u0008站脚派。\n对大多数人，多数时候，也都是实干在为自己出活儿。\n警惕嘴派分子，多用脚投票。\n支撑000115 2018.06.27 23:53 Done is better than perfect.\n强调的是做事的思路：要有轻重缓急，能分清重点。比如时间紧迫上线要紧，当然就是完成编码最要紧，而不是细节调优。\nKnowing the thing is better than knowing the name of the thing.\n强调的是投入的重要性：很多事不投入时间精力，做不成。以及，不要随便吹牛。事情是做好的，不是吹好的。\n支撑000114 2018.06.12 19:30 单论对自身的要求，那就应该尽可能高。\n这里的「尽可能高」，展开讲，就是合适与否的问题，这是个大问题，我们缩着不展开。\n《孙子兵法》云:“求其上,得其中;求其中,得其下,求其下,必败”。\n为什么如此？\n因为大多数人大多数事，都不是顺便就能干好的。\n支撑000113 2018.06.11 23:24 输入决定输出。\nIOIO，不仅仅是个管道流，本意是输入输出，计算机世界充满了这种模型，而起源，也是现实模型。\n想要输出一样\u0008价值，必然要先摄取，无非是摄取的地方不同。\n所以别偷懒，没输入，谈什么输出。\n支撑000112 2018.06.11 22:03 基于事实的逻辑\u0026gt;自我感动式的成功学。\n或者说，实力\u0026gt;无知热情。\n之前说过，很多事，首先是个能力问题，能力不够，后面再这么扯也是扯淡。\n所以一些\u0008相对低门槛的行业，鸡汤、企业文化（假的）是可以相应提高生产输出的，比如典型的发廊，因为能力基本达标了。而IT中细分的领域，大部分都不能这么搞，当然IT行业内的运营、销售岗位除外，意思是，开发、技术类岗位，\u0008发力的重点并不应该是这些地方（忽悠），而应该是提高研发能力、效率。\n锤科的罗老师（我之前的偶像之一），我现在一点不看好他们所做，因为他们没好好干工程，净扯些市营、概念型的蛋。发错力了。结合我之前所说：力要用对方向。方向错了，就是做无用功。\n那么在顺丰这次本就是一次「成功」的营销事件上，多少人犯了事实逻辑认知错误呢？应该是\u0008多数人，因为基本都没有这位跟我同岁的博主具备的专业知识。\n不过，借由这种事，来启发自己：「只要肯干，就没有走不通的路」一定就是错的。在创业这种成本\u0008\u0008风险都高的事上，个人不建议犯这种错误，读读「人件」便有所体会。\n支撑000111 2018.05.30 00:11 实力产生自信，自信不能伪装。\n真正有实力的人散发出的自信就是他的气质，没实力，怎么装也不行。\n实力不够的情况下，也不用装，当务之急应该是去提高自己的实力。\n实力不够，也要尽量自信，认可自己当前的能力上限，去发展，不要局限。\n支撑000110 2018.05.12 20:39 匹配即合适。\n找工作谈匹配，谈对象\u0008谈匹配，经验与能力也要匹配。\n昨天面的一哥们儿（这次我是面试官！！！），工作年限与能力断层较大，原因就是：学习方式、过程出了问题。\u0008你结果的不匹配，总是由于你前置输入的不匹配。\n做匹配当下时机、能力的事，出匹配的结果。\n支撑000109 2018.04.28 11:09 知其然，知其所以然。\n说实话，昨天那位年轻（估计跟我差不多大）的面试官把我虐残了，我个人还是比较欣赏这样的面试官的，因为认同他面试的思路：我想要看看你是否具备知识基础、问题解决能力。即，他会问很多底层原理、过程分析的问题。\n我这块儿当然是不足的，能怎么办？下来赶紧补。\n知其然，知其所以然。\n支撑000108 2018.04.19 21:39 支撑，首先要支撑你的生命，身体是革命的本钱，这也是我多次强调过的，当然，有些病可能由不得你（医生说病因不明，治好就好了），但还是要牢记，要保持一个好的生活工作习惯，不要让自己步入亚健康状态。\n2018.04.10 14：30我如约到了王医生门诊室，在护士间等待四十分钟，15：10进入手术室，过一二分钟，床上就位，一个男医生，一个阿姨护士长，一个妹子护士（95年），开始了我这次手术。手术很顺利，因为局麻原因我也没觉得多疼，但术前因为紧张血压偏高，不过无妨大碍。术后的这一周多时间我过得不容易，也更加觉得不生病不动刀是如此珍贵。\n手术做到一半，那个护士长阿姨还说要给我介绍女朋友，我当时真的是哭笑不得。挺过来就真的挺过来了，现在整个人的精神状态恢复了，室友说我现在又准备开挂了，嘛，又开始研究代码了。\n被同一个护士抽了两次血，被同一个医生做了两次外科手术（上次是2018.01.27）。谢谢你们，让我又重新活蹦乱跳更加帅气了。\n咱们这是一种缘分，但以后希望还是少见面orz。\n支撑000107 2018.03.25 11:39 算力matters，运行时间并不。\n在##支撑000007提到过的功效问题中，功=F*L。问题核心：能不能真的用上功？力能不能用对地方？很多时候我们在评估一件事值不值得做、到底要怎么达成目标、如何找伙伴等现实问题上，往往就是没分清力的功效问题。\n比如一颗CPU，我们往往会看重其运行时间，误将这个指标作为功效的考核（类比绩效考核的概念，okr），而事实上的功效是由算力决定的。\n所以你能分清算力，而不要混淆运行时间？理论理解后，执行能跟上？\n支撑000106 2018.03.24 17:01 舒服。\n人都喜欢令自己舒服的东西。\n父母为什么会是大多数人的依靠呢？生，其实对孩子来说是没什么感受的，而养就不同，半岁的婴儿，生活是完全靠养他的人，憋尿、饿，都需要这个人来让自己脱离不舒服的状态，久而久之，生理、心理上对这个人形成了依赖。突然来了个陌生人照顾他，他自然会不舒服，就要哭。\n成年后，如果像我这样的情况，读了一些东西、出去认识了很多志趣相投的朋友，可能跟父母从观念上更多的是沟壑而不是连接，从这点上，就是不舒服的感受。知道自己要追求的目标跟父母感情不冲突。\n所以，交朋友，价值观还是蛮重要的指标。\n无非就是追求生理、心理上能够令自己舒服的人、事。\n支撑000105 2018.03.22 20:04 抛异常一定可以溯源。\n程序都是人写出来的，方法间是一层层调用栈，报错贴异常栈，简单的情况几秒钟就可以解决问题，复杂的情况可能需要从服务器用jstack或者jProfiler入手，一步步推导过来，总能找到原因。\n久而久之，这样的思路与操作成为了你工作的方法论、经验。\n抛开写代码，大多数结果都可溯源。做人做事都是。\n既然可以被聪明人溯源，那就不要随便干傻事。我是那个聪明人。\n支撑000104 2018.02.23 19:08 爱八卦的人，终将走进八卦的泥潭。\n一般人都爱八卦，因为八卦不费神，又能获得表面上的「爽感」。知乎：人是如何废掉的！\n但对于真正的朋友间，八卦的尺度就要把控了。我自己不愿意透露一些自己的隐私，相应的，对方肯定也有，那我应该尊重人家，默契使然，我就不该或者少八卦。\n你忍不住？那你终将陷入无法自拔。\n支撑000103 2018.02.23 11:41 屏幕障目。\n古有一叶障目，今有程序员显示器障目。\n干一个岗位时间久了，往好了发展那就是「专精」，往差了发展那就是「障目」。\n结合## 支撑000101，技能都是可以培养的，同理心也完全可以生长再成长。运用→反馈→改进。\n已知「障目」不可取，那我就试着改进一下。现在要向HR这些精英学习职场思路。\n支撑000102 2018.02.23 11:39 会做选择题，会做选择，两码事。\n读书考试时经常做选择题，有逻辑也有技巧。现实中的选择，有逻辑也有运气。\n逻辑可以训练，运气不能培养。\n支撑000101 2018.02.22 01:09 屁股决定脑袋。\n在其位，谋其政。\n同理心、换位思考，说起来谁也知道这些词，但含义可能就是错的，比如现在网上对「直男」「女权」很多人认识直接是错的，所以于我来说，首先要有对的认知，其次就是在实际中应用。\n不在其位，也可以谋其政。\n支撑000100 2018.02.17 15:13 推荐tiny女友楠子的一篇公号文章：在爸妈面前，讲什么自我。\n文章关键字：亲子关系、家庭教育、关系平等、认知交流。\n我从上中学开始与爹妈间爆发了较大冲突，物质与精神上都是。上大学我爸送我去机场，忘了因为说啥闹地一脸不开心。到现在我过年回次家，经常因为一些小事我爸会生气，开始两天我还没啥反应乖乖听着，但一直这样也会受不了。现状就是这样。\n中学一好友当时给我提议多沟通，但多年后才明白，他所处的家庭环境与我截然不同，不是说沟通本身有错，而是说方案在不同背景下可行性会有天翻地覆的差别。\n楠子的文章观点我个人倾向很喜欢，标题也是一项硬道理：在爸妈面前，讲什么自我。父母所处的年代、工作环境、生存经历与2018年的我截然不同，首先我会感恩，我也相信爸妈是爱我的，只不过在行为层面我意识上不认同，而我认为追求自由是优先级很高的一件事，我会尽量服从（毕竟回家几天而已），更重要的，我会尽力去提高自己的能力。\n回家再一次感受到了来自家人、老朋友观念上与我的冲突，这是难以改变的一大堆人所组成的环境。矛盾哪里只存在于亲子间。\n要学会应用同理心。\n支撑000099 2018.02.14 23:13 「要」也是为了「给」。\n也就是中国一个成语：量入为出。计算机有个词：IO。\n干工作、创业、做事，以效益为导向我认为是对的，既然要有产出，那么必定会有投入。但如果不在这条思路上，只考虑投入，只想着吸收资源，那么价值也还是发挥不出来，因为逻辑上不成立。\n甚至，人际关系上也是这样。你跟我要东西可以，啥都不给我就是不地道。\n支撑000098 2018.01.27 12:54 多熬夜不一定会成事，但一定有损于你的身体，关键还是提高日常工作的效率。\n26号跟医生约好了手术，27号中午进手术室，很小的手术，单子上写的是「颈部血管瘤」，长了快两年的一个疙瘩，医生说这次比较及时，切除后就没啥影响了，但是嘱咐我以后要注意生活习惯，比如别熬夜。\n两年前确实是熬夜比较严重，经常吃宵夜，觉得自己年轻，但其实这种行为相当不理智，熬夜图什么呢？破坏了良好的生活习惯，降低了身体素质，即使有规律锻炼身体也是有损的，在做这些事前，能不能确保日常的工作效率？当然能！那就切掉不必要的熬夜，睡觉与吃饭都要在自己能力范围内争取质量。\n自律，理智，首先对自己的身体负责。\n支撑000097 2018.01.25 19:54 最近长了一次口腔溃疡，突发感想：人生病的时候比不生病脆弱那么一些，然后才更加能感受到平日里「健康」的舒服与珍贵。\n去年生命很重的时候一度会觉得女性的不容易，月度必然遭受的痛苦我们男性就天然地躲过去了，除了侥幸，更应该对她们致敬，也应该给每个人更多关怀。\n我活了二十多年，与人的交往、做事的成败都是这样一个轮回。\n所以平时就对这些看似是essentials的事物多用心吧。\n支撑000096 2017.11.27 23:08 公共场所需要利他意识\n利他：首先不妨碍他人，其次同理心意识。\n在公共场所与家里不同，在外往往可以体现一个人真正的素质：\n身体素质 心理素质 文化修养 只要不是恶意妨碍他人，我就不会讨厌这样的个体。能做到礼貌对人，交流就会很顺畅不别扭。\n公共场所去利他，其实是利己。\n支撑000095 2017.11.11 13:53 我认为应当的样子：给的能力\u0026gt;\u0026gt;要的能力。\n\u0026gt;\u0026gt;：远远大于。\nJava程序员都知道JavaBean这个规范，普通的字段想要成为属性，就需要提供setter或getter，当然大多数时候二者是同时存在的。从程序的设计与使用上讲，我们都觉得这件事顺理成章，拿到null必然知道是方法调用者没有调setter。\n但是到了生活中，这种逻辑运行起来，从我的眼睛里，看到的都是bug。\n即现实中的样子：要的能力\u0026gt;\u0026gt;给的能力。\n举几个例子：\n为什么背了好几天单词，英语考试成绩却还是不行呢？ 为什么加班这么多，工资却还是这么点儿呢？ 为什么我又胖了？ ta为什么抛弃了我？ 先给，再要。\n支撑000094 2017.11.09 00:11 想占便宜，就是被占便宜的时候。\n学习的时候想走捷径；工作的时候想偷懒；买东西的时候想省钱。\n人之初，都是懒。但是经过多年的栽培，你就不应该随着无知的性子走。\n想成事，就永远别想着占任何便宜。\n支撑000093 2017.10.30 00:15 找合适的人，做合适的事\n交朋友要看合不合适，观念、经济实力 做事要看适不适合，能不能做到、爱不爱做 买东西也得挑个合适，需不需要、买不买得起 话粗理不粗：「交配交的人，做爱做的事」。\n支撑000092 2017.10.22 23:13 追求自由，先分清自由是什么：\n物质自由，一两百人天就够独立生活了 思想自由，多读书多实践，多坚持自己的思路 财务自由，够肉翻，够干大部分事 财务自由不可强求，强求也没用，大多数（所有）人认命最为合理。\n我坚持第二条十年就够了。我不会放弃已有的自由。\n支撑000091 2017.10.21 09:47 谈判，想成，就要有足够的筹码。\n「追龙」里面，泰国方换了将军，洛哥派跛豪去泰国谈判，想争取到货源，见面之后的谈判场景比较爽快，泰国新的将军嫌价格太低，玫瑰把报价高的人头摆到了桌面上，跛豪被按到桌子上，玫瑰放出将军儿子的录音。\n那段录音就是足够的筹码，货源敲定。\n下一步的问题就是，如何拿到足够的筹码。\n支撑000090 2017.10.11 23:13 借钱，借别人的要感谢，借出去的该催就催。\n有个词叫「主观能动性」，就是说人不能死板，因为人生不是做题，没有丝毫不差的模板让你照抄，具体的场景出合适的方案。\n我借给你钱，多的，那你肯定是我认可的朋友，不存在催的情况，少的，那你我就是有共事的时候，适当需要催一下。\n你在我落难的时候借了我五千，我以后会尽力报答你。我在你兜里临时没钱借了十块，你能还我就一定会要。\n「你我」说的都是我，教育别人无益，管好自己就很牛逼。\n支撑000089 2017.09.22 20:30 言行一致。\n「大时代」中的丁蟹是个枭雄，敢想敢做，而且有一技之长，光这点来看，世间大部分人都已经不如他了，当然他有个致命的毛病，是非观混乱。\n我想说的是，他能够严格执行自己的意志，这是值得模仿的特点。\n当然言行一致也要算成本，很多时候迫于最优路径的影响，我们的行动也要适用于所处的场景。\n两个关键：言行一致，因地制宜。\n支撑000088 2017.09.01 00:37 学到东西+赚到钱，这是比较好的结果，也应当是我的追求。\n我自己认为像「得到」这种知识学习产品需求是很明确的（团队自己清楚），罗振宇这个团队相当精悍，既赢得了自己用户的认可也赚到了钱，不像锤科，罗永浩团队也精悍，但是他进入了制造业，碰到了工程难题，这种难题没有硬本事破解不了。\n「得到」让用户觉得自己学到了东西，所以他们的产品卖的是一种感觉。这个团队值得我学习的，就是手法跟结果，赚到钱。\n而我指的「学到东西」并不是这样一种感觉，而是我自己通过投入适当的时间、金钱、精力成本获得的能力，比如实际学会了工程编码。\n所以我看「得到」的成功，并不鄙视罗振宇团队，相反，他们值得我尊敬，更值得我去学习，他们的用户表现的品质其实也没必要去鄙视，正如同我的一些朋友瞧不上这种人的感觉一样，这些都是人性的一部分，既然自诩聪明人，那为何不去赚这部分钱呢？\n所以，我要去学也会学到一些东西，正如这篇文字的观点，我也会去赚到钱。\n支撑000087 2017.09.01 00:22 水平高低之争，有意义，门派挤兑无意义。\n争论也分好与坏，真的高手之间是真的在切磋，图的是一个分享与进步，而低级选手往往都分不清重点是什么，大多数时候也只是无意义的站队（屁股决定脑袋）。\n不是不能争，而是要争对了地方。\n我图的是水平。为了水平上升我才能屈能伸。\n支撑000086 2017.08.10 19:39 「Life is hard , right ? WRONG ! Life is easy , YOU suck .」糟糕的是人，不是生活。\n刚吃饭刷微博看到这句话。很明显，这是大多数人平时的状态，当然，这些人在某种程度上都是弱者，因为比惨这种完全不合乎逻辑的行为并不是一位优秀成年人该干的事。\n别人比惨我当然管不着，我也不想管（因为我知道说几句话并不能对IQ、悟性有什么帮助），我只能做到两件事：\n管住自己，做些更实际的事，而不是在social network上打一些低级嘴炮 借鉴，能够挖掘出一些价值，比如一个思路就是，服务都是面向普通人的，换言之，这是一个需求的入口 当然，我不比惨，因为我混得还行\n支撑000085 2017.08.09 19:52 雕琢才出真作品。\n按我的理解，「自然」不是天生的，好的作品（人造跟大自然）都是经过处理的，比如艺术作品、软件产品、山水。\n也可以说，不存在「自然」一说。既然如此，转基因的问题就不存在了，争论也其实完全是胡闹，跟愚昧的人没必要浪费自己的时间。（只是个例子）\n人也是作品，大程度上还是自己出品的，我不信「自然」，我信磨炼。\n支撑000084 2017.07.21 00:12 经济账+野蛮的行动力。\n不管多穷多富，能算好经济账就称得上牛逼，当然，经济学这种东西操作起来门槛不是一般的高，所以需要花费心力、时间去打磨，但一定值得付出。\n有了亮堂的方向，剩下把人类自带的冲动释放到应有的行动中，能成多大的事我不知道，但一定不是俗人了。\n说白了，既要脑子好使，也要能干肯干。\n支撑000083 2017.06.24 09:51 认识→了解→再了解，从自己入手。\n大多数人最终都是平庸的，想要不平庸或者走在这条路上必定是要支付比常人更多的代价，能不能做成是一回事，有没有机会做成又是另一回事，对于我这么一个非各种二代的平民想要成事就必须这么去尝试。\n平庸的原因其一就是不了解自己，不了解社会跟自己直接丧失了选择与晋升的能力。\n推荐阅读下和菜头发的一篇文字：高考填报志愿私人指南。这个有智慧的老大哥何止是写了高考填志愿的指南，对于常人来说，这种思路适用于工作、生活各个方面，当然前提是你认同他的观念。\n从观念这一层，说白了就是价值观，说三观正不正容易引发客不客观的无意义争论，那我就说我非常喜欢菜头叔的三观，相当主观了，这篇文字里很多东西与我之前几年的经历都扣上了，所谓感同身受。\n我去年底才选择了广州作为工作的城市（之前要去深圳，上海也待过），虽说当年CS专业有所坎坷，但并没有阻挡我成为一名Java开发，学习经过了几年我也知道了自己需要看什么书需要什么样的生活习惯，这些对我都是一种刻意的选择。\n当然代价是有的，甚至有些朋友觉得这是额外的代价，但是我愿意并且有这个能力支付，总是想着获取并不愿意支付的人我不喜欢，我的简书我自己决定说喜不喜欢哪些人，就这么耿直。\n合适的代价支付，其实对于平民是非常棒的一种了解自己的渠道，我还想了解（塑造）更好的自己。\n不扯了，起床去加班了。\n支撑000082 2017.06.01 05:29 尽可能多地去接触可能的大牛。\n从上学到工作，再到换工作，这个过程比较有趣的一点（特别是IT行业）就是，你可能一直在换环境，更直接点儿，你的领导从水平上可能会有大的变化。以我自身为例，学东西的广度与深度都开始打开了。\n昨天一大哥跟我说的一句话，虽然我心底里持悲观态度（理智性不选择这种方向），但还是记到了心里：你现在跟他有差距不代表你将来在这个领域就比他差，还是看你够不够用心。\n支撑000081 2017.05.26 00:29 机缘巧合跟一位互联网老司机（待过大小公司，参与创业三次）聊了一下，他的一些经历带来的教训我也比较认同：\n铺张浪费有两种，一种是突然暴富，一种是虚荣心驱使，都不可取 干技术要踏实，做产品要深入到受众，做UI可以学点儿前端 干工作不定说要有多大热情，起码拿得下自己的任务，够负责，临时出事儿也能扛得住，下班后完全排斥自己做的事说明很有可能你不适合干这个 公司找合适的人要提前把一些话说清楚，个人选合适的岗位要清楚自己的定位 从「从业八九十年的人」身上确实可以反映出一些自己目前做不到的东西，不过可喜的就是人的状态终于从「向大十岁的前辈学习」转为了「可能五年内就能超越大十岁的前辈」。\n支撑000080 2017.05.20 00:40 投入时间空间，产出对应的价值\n一个观察：大部分不干IT的朋友使用电脑仅限于office、视音频 一个体会：程序员下班后的兴趣投入对发展的影响不小 所以「种瓜得瓜，种豆得豆」，有时间都拿来看剧可能会「精通剧情推理」，但成不了编剧 凑巧前两天看到周楷雯说能花钱买的设备跟知识就要买，是一个道理 不一定对 支撑000079 2017.05.18 00:03 别受限于当下所处的环境。\n过去的一年中，以住所为基准，我已经换过了三次环境，这个过程一时难以描述，但我无疑是获得了空前的进步的，在实际操作跟心得体会上都练得多也想得够多。一个很有意思的地方：不同的环境下你看到的现象、接触到的人和事首先有视觉上的不同，背后的不同则是一股无形影响着自己的「力」。\n想到我初中数学老师说过的一句话：「别尝试改变环境，要学会适应环境」。这句话对错难辨，因为这种嘴炮上的东西永远没有绝对的对错之分。\n对的一点是，学着适应可以让自己更快更好地融入到群体，在一个团队协作的大环境下明显是对KPI这种东西有利的。错的一点则是，单纯体会到表面上的道理只能让你成为一个极为普通的人，最终也只是随了大流，压根没有找到自己真正能够做到的设定，因为没有环顾四周，也没有认识自己。\n怎么办呢？二者其实完全不矛盾。首先第一点是要做到的，做一个靠谱的人先，其次要时刻审视环境与自己的关系，最好永远别被牵着鼻子走，这股「力」能不能起正向作用其实恰恰决定于自己。\n环境对人起着太大的作用了，从小的学校、家庭教育，到大学再到工作，以及你最核心的朋友圈子，从想法到行动，但换个思路，自己对环境有着同样的反作用力。\n能意识到局限还远不够，更要紧的，是你要真的从坑里跳出来。\n人总是会越来越好的。\n玩过悠悠球吗？球不但可以转得溜，还可以玩各种花式。\n支撑000078 2017.04.25 10:54 昨天下午的感受：\n东西好不好，或者说「判断」本身也需要强大的能力。不要瞎否定自己，更不要自负。产品做出来我个人觉得商业价值最重要，单纯堆需求没有价值。 领导能力是存在的，有用的。 表达展示能力来源于思路。单纯的形式会产生更大的用户价值，以形式倒推可以形成思路。 还有前两天看到瓜哥一句话↓↓↓ 支撑000077 支撑000076 2017.02.27 23:41 个人的种种选择，犯的错，不要甩锅给时间。\n支撑000075 2017.01.07 23:59 找原因。\n我发现，大多数人并不是败给了现状，而是输给了形成现状的原因。\neg：概率上讲专科生一定是弱于本科生的，二本的一定不如一本的，普通一本拼不过211、985。但这些弱势的人有一个共同的特点，他们不是同级别的佼佼者，狭隘一点说，可能他们都不爱读书，学习生活习惯都比较差。\n找到原因，不要困于原因。\n支撑000074 2016.12.25 20:53 再用力点。\n这两天已经看到一些年末总结的文字了，对自身我当然也有判断，但同时我更加意识到这些优秀的人同我不在一个阶段上，向优秀的人学习是必须的，但多思考自身的具体情况来做出对应的路线更加重要。\n今年一直很用力，碰壁也不少，最近学技术的体会是：不同的事情需要往特定的方向动脑筋，例如写代码跟写文章是不同的事。\n总之，明年再用力点。\n支撑000073 2016.12.09 00:23 意识到入坑就用力跳出来。\n下午想到高中时候学英语那种状态，状态其实也是效率的基础，坏的情况或者表现很差往往也是状态出错了，不要迟疑，拼尽全力跳出怪圈，成本预算可以提高到平时的两倍。\n这不是调整，这是意识。\n支撑000072 2016.12.06 22:39 多观察，多回馈到自身。\n核心在于自身，但是焦点在于周围，所以多观察自己的同时需要借鉴周围的环境，看到别人可能无利的举动多想想自己，因为终究能不能成事要靠自身的本领。\n当然，对待别人的方式无论何时都应当尽可能体面，这也是本领。\n支撑000071 2016.02.01 23:09 结构与细节都需要足够了解，这是我应该继续提升的方向。\n作死可以，思考可以，粗心不可以。\n支撑000070 支撑000069 2016.11.16 00:17 「勿忘初心」压根不需要说出来，很多事都是。\n最近一直状态很差劲，今天索性请假，下午画了画会员原型，吃完饭拿起衣服就奔向了电影院，俩小时看完了《驴得水》，电影上映前就准备去，无奈工作不由人。\n看完后觉得开心麻花这个团队真是牛逼，靠谱的人才做得出靠谱的作品，简单说这个话剧电影涉及了性、爱情、教育、体制、生活、美丑、责任这些我个人感兴趣的点，框架不乱、故事整洁、演技有张力，总之十分推荐，我觉得目前国产电影这算是最好的之一了。\n文字感悟来自电影，也来自近期工作经历，提醒自己要对目标负责，对人负责，好好做事，细心做人。\n支撑000068 支撑000067 2016.10.09 23:50 如果非要站队的话，我站强者这头。\n“不行的人就是不行，不管什么时候也不管做了什么。”这是我爸在一个月前跟我说的一句话，他很少认同我的观点，更很少鼓励我支持我，因为我们家是典型的打压式教育环境，这一点怪不了我爸，但那一次他就用这句话鼓励了我。\n他举的例子很简单，厂里工人请假需要征得我爸的同意，而这其中懒的人不在少数，一部分人的借口就是：我老了，干不动了，身体不行了。\n我爸继续跟我说：实际上，他年轻的时候一直是这个状态，干了一辈子也没出息。\n再反过来看我爸的现任老板（他的发小），十五年前人们说开厂太难了，五年前人们说更难了，现在这些人就是那些颓废的打工者，而我爸的老板在严峻的商业环境里把自己的厂不断扩充，他一直没说这事儿干不成他不想干，所以你明白了么？关键在于人！\n张靓颖这个事儿也可以用这个逻辑来看，感恩父母当然是我们作为子女应当要做的事，所以人际关系大多数时候需要软处理，但这不代表我们可以失去理性、原则、能力，所以有能力有智慧的人在做大多数决定时要强于常人，因此，我相信张靓颖（简单看下她的微博，再看看下面劝她改主意的ID，很容易判定那些人大部分都是loser）。\n当然，我不站队，不喜欢也没必要，这只是本文的假设而已，该干嘛干嘛去。\n支撑000066 2016.02.01 23:09 来自微博ID科学家种太阳的一句话：很多事情的真相，对于我们这些旁观者来说，并不那么重要。最重要的是，我们从中能有什么收获。\n这句话本身源于对公众事件中吃瓜群体行为的判定，但放在我今天的文字中也有些许切题，核心就是：时刻将自我的提升放在第一位，与自己无关无用无效的事要尽量避免。\n结合我之前的经历，我确实犯了太多这样的错误，所以请自己铭记：夏虫不可语冰，要做于自己有利的事。\n支撑000065 2016.09.25 12:46 为自己找正确的理由，而不是无用的借口。\n无用的借口诸如「英语太难了，我记不住单词」或者「这个事不可能我做不到」，本质上就是懒，没有获得意志与远见的情况下跟随潜意识随波逐流，读过书不应这么短视，只看得到眼前的蝇头小利而已。\n而正确的理由则不同，核心就四个字：有用、有效。世界的不堪是由人造成的，而想要更好的世界与生活就需要抗争，就需要努力，这个过程需要有实操能力与思维模式做支撑。\n最近两件事我为自己找了正确的理由。\n支撑000064 2016.09.24 11:20 拥抱开放的环境，抵制封闭的自己。\n苹果不是封闭，高效闭环带来了大量开发者与制造合作商，如此共同成就了伟大的Apple，这是开放；阿里的淘宝拥抱了大量了商家，为其提供完善的电商服务，拥抱开放成就了阿里；秋叶团队，这个团队不容小觑，现在威力还没爆炸，但终有一天会指数增长，只是因为开放，聚集了大量优秀的知识输出者，开放的模式终究会有更大的效应。\n反观自己，大学是一个较封闭的环境，某种程度上我有很大的盲点，意识到了这点，我可以更准确的以开放的心态自我成长。\n我铁定是太嫩了，但不会一直嫩。\n支撑000063 2016.09.19 00:54 认真对待承诺，科学规划进度。\n知易行难这种道理也是听起来特简单，真正想做再到完成这个过程却是漫长或者无尽的，现实世界中大多数人确实有形形色色的想法，但能够付诸行动开始第一步的人甚少，开始后能够完成整件事的人甚少。\n我习惯于用锤子便签写to-do-list，刚刚完成了一个月前规划的一篇文字，五个字，花了五天的时间完成，当然我认为质量还不高，但我庆幸我完成了。\n便签里的to-do也是一种承诺，干。\n支撑000062 2016.09.11 23:53 时间与钱不可兼得。\n任何时候时间都是很贵的一种商品，对大多数人来说，于时间面前，人类是被动的，但通过天赋+正确投入，少数人可以突破一点屏障，例如花自己赚的钱来买时间。\n在时间面前我一点也不能贪心，对钱可以。\n支撑000061 2016.09.04 15:31 适应环境是在社会中生存的必备技能。\n这一观点与「改变世界」并不矛盾，在人生前一半的路上，「改变世界」是我的目标，「被世界改变」即适应环境是必需的手段，对成功的渴望与坚守原则只需要在心中牢记。\n能力问题在这件事上实际上是背后的驱动力，在没有拥有「改变世界」的实力之前，我就该学聪明、长本事、适应环境、建立自己的光环，避免像一年前成为学校的半牺牲品。\n心中不必抗拒这种不适感，实则「适应」就该是make it better的最好表现。\n支撑000060 2016.09.01 23:39 状态是最重要的。\n姑且把一段项目分为**「开始前」「进行中」「结束后」**三个阶段，用一个因素来表示什么最关键？以前我觉得无解。\n项目可以是做一张图，可以是设计一套PPT，也可以是写一篇文章，更可以是学英语健身这样的事。\n我尝试过很多并且一直在坚持其中某些项目，之前一些朋友询问我学英语的事，在分享过一些方法或者技巧后发现其实最大的关键在于他们本人，而我并起不到什么作用，我也一直没确定最关键的应该是方法还是坚持。\n但我现在有了答案：状态，这个词可以是决定你日后成就的关键。\n堆砌时间没有意义，状态才是最重要的，精力应该放在这上面。\n支撑000059 2016.08.28 00:50 在大多数等待我们将要执行的事情上，第一个问题应当是能力问题。\n已知的情况有经验引路，未知的情况需要尝试，但行动前需要逻辑清晰，对事实的判断产出合理的观点，能力判断不要视而不见。（人类总是不肯相信自己）\n以借钱为例，判定我目前需要外部支持后，（救急不救穷，我很急）我需要缩小对象范围，第一个指标是能力，第二个指标才是对方意愿。（关系、价值）\n提高自我能力是第一要务。\n支撑000058 2016.02.01 23:09 选择产品方向即业务内容，目测+亲手测。\n关于方向选择的强调来源于敦发财关于产品自学的观点，经过了大几个月我基本摸清了可以实际用来上手的方法，即目测+亲手测，实际上这是一个筛选过程，目测已经可以毙掉大多数品类，亲手测则用于过滤更细致的业务内容。\n多重筛选其实是为了避免勉强自己的情况，比如今天中午自己做了「扬州炒饭」，用了「下厨房」与「香哈菜谱」这两个移动App。\n我自己开始做饭从四月份体验到了现在，（别看我是个糙汉子，我会自己做着吃）原本感觉我不会是这一类的用户，表现应该为没什么业务敏感力，但实则不是，这就导致一些我原本以为不敏感的品类在深入理解后其实是可以继续做的，而骑行类则相反。\n今天是周末，下午出去骑了35公里，回来太晚导致没写菜谱产品垂直电商的体验文，不过今天娱乐最大也不是罪过，关于体验报告的目标导向我也必须在近期整理出来，与很多产品作者的思路或许有些不同，因为我不求全能有相应细分的收获就行，深度选择。这个系列说这么多废话才是罪过！\n方向选择使用多重筛选手段，目标导向思维行文。\n支撑000057 2016.08.20 01:15 做事应当有理有据。\n人经常会被自己麻痹或者欺骗，反而愿意相信一些明知不可行的方案，一方面是情绪作祟另一方面是思维没有摆脱这样障碍的能力，但都可以通过不断学习练习去修正。\n努力提高自己找依据信依据的能力。\n支撑000056 2016.08.14 18:27 追求「劳逸结合」的路数，在完成任务这件事上「结果」比「过程」重要。\n每个人都有不同的活法，充分认识到自我的局限才不浪费精力，因此这一条就是针对我前几个月状态提出来的应对措施，涉及到作息的改变，需要慢慢调，加入了跑步，一周内达到早睡早起的效果。\n专注产生效率。\n昨天买了瓶昆仑山，¥8.8！！！ 中午吃了饺子顺路买了块西瓜，¥16.8！！！ 龙哥你要一心向善！！！\n支撑000055 2016.08.08 09:09 珍惜我的时间。 为什么不写成管理时间？因为这是我个人的规划并不是工作，过分地将工作渗透到日常生活对我对世界都没太大好处，最终可能会诞生一个平庸的上班族但那不是我所求的。\n直接砍掉无用的时间投入，难点在于没有判断标准，需要自己建立； 正经事跟放松都需要注重效率，即像疯子一样学习，像狗一样玩耍； 交朋友看缘分吧，跟我能说到一块去的人真心没有多少，先积累自己；\n珍惜时间，我去做听力。\n支撑000054 2016.07.22 22:48 珍惜童真，追求有趣。\n人总是会对自己喜欢或者感兴趣的东西愿意付出自己的力量与时间，我虽然年轻但已经有类似的经历因而可以心底里感受到这样的指引，但这么好的模式也需要自我学习来坚守。\n希望我会一直童真，我必将是一个有趣的人。\n支撑000053 2016.07.12 20:39 掌握基本手法，保持稳定心态，执行。\n这是一套可行的方法论，以开车为例，刚开始上手的时候发现心慌车不稳，等到师傅将所需注意的点都逐步掌握并加以适当练习后，发现开车很容易，只不过不要得意。\n类似的理念适用于不同领域，内化后就是个人的本事。\n支撑000052 2016.07.06 18:56 形而上的东西不具备操作性并不代表不需要。\n俞敏洪、马云、史玉柱这些人算是我中学时期的精神偶像，这种崇拜自然有些许年少无知的成分，但现在我敢说了，我依然崇拜这几个大神，没有无知的成分，因为我明确的知道了他们处在怎样一个高度，站在公司或者社会的层面上，他们的价值确实是极大的。管理是科学，领导是艺术，男人征服了世界，女人征服了男人，你说哪个更牛？\n他们并不是吹牛逼，很多成年人觉得这些人过于虚无，是大骗子，原因是这些成年人成熟了，但是不够成功，嗯，年薪几十万确实是穷。\n支撑000051 2016.06.17 23:51 方法的妙用。\n今天来逛了逛简书，发现真是哈，大学生或者刚入职人群是他们家的核心用户，励志、方法论的文章颇受欢迎，其中看到关于“行动”“方法”这些字眼，我内心深有体会，方法这个法宝不仅仅适用于学习工作，在任何领域都需要这样一个成分，比如骑车，你的姿势，呼吸，装备等等其实都是特定领域的方法。\n所以适当的时候用正确有效的方法。\n支撑000050 2016.06.15 23:42 时刻记住自己的出发点。\n多年前我选择来南方，不为啥，两个字\u0026quot;文化\u0026quot;，后来赶上了互联网这个大潮，恰逢我二十来岁胡思乱想于是乎算是上了这条贼船，每逢三五个月对自己的规划就更加细致甚至方向都不停改动，今天猫儿跟我说准备下一站北京链家，我内心那杆秤被他的几句话拨动了两个来回，北京两个字意味着离家特别近，我想回去，但是我还是要去深圳，因为我要明白我的出发点，要去感受到深圳甚至香港的文化才算是一个节点。\n最近收入过低，甚至跟朋友家里拿了很多钱，恰恰是要静下心来投资自己，提升内在的能量，今晚这个心动也算是诱惑当前坚守本心吧。 这个系列这篇太啰嗦了，不忘初心。\n支撑000049 2016.06.03 19:33 与人为善，做事认真，集体认知。\n沟通的流程中，我首先应该表明自己友善有益的态度，暴露出自己人性中的缺陷并不是我所想的，我已经在改善并且一定可以做到更少的漏洞，理智的克制缺陷，利用科学的方法加以同理心的运用去推动整个团体的利益进程。 对不起，我爱你们。\n支撑000048 2016.06.03 19:23 基于多维的数据产出结论。\n单点或者一条线上的数据或者现象并不是不能够相信，只不过这样的结论毕竟还是可信度不够，最好就是基于足够的数据来分析案例，在理智的项目中需要类似的技巧或者手段，在感性的关系处理或者体现情商的部分也可以这样做，因为人作为主体所做的决定都来源于自我的认知，而多维数据是服务于认知的。\n支撑000047 2016.05.23 02:11 提醒自己别被技巧带错方向。\n技巧的学习同样需要消耗时间精力，有时候甚至会‘入不敷出’，这样的困境正是我想要避免的，由此，我根据产出比来区分技巧与技术，两种能力我都要提高，但是付出相应的时间足矣，别被带到沟里。\n支撑000046 2016.05.12 00:11 请专业的人，做正确的事。\n方法论或者逻辑层面的东西压根都不需要对人对事，不分行业同步适用。我在学校上课发现了老师很多的问题，其中一点就是他们一般都是某个方面的专家，但是在讲课这一点上，大多数我的老师都不专业，可悲的在于他们也意识不到这一点，即使我善意的提醒他们，他们也丝毫没有改变的动力，貌似这是教育的一个痛点。\n事情难做是事实，可大幅改善的一个解决方法就是：请专业的人，更加专业的做正确的事。\n支撑000045 2016.05.12 00:07 做事前别急着动手。\n昨天教个同学做视频，我用了iMovie这个软件，目的是为了极致降低操作难度，过程中就得知关键并不在于操作层面，没有思路急着上手这才是现状。 以结果导向的活动最好有一个比较清晰的思路，paper writing管这个叫提纲，写作文里面叫三段式，骑车是路线规划，生活学习工作中如此典型的一个场景因为一些人的蠢而搞得如此狼狈，我不喜欢这样的作品，更不喜欢这样的人。\n集中脑力提供战略，简洁高效健壮的框架，踏实细致的行动力，初步的版本出水后细细斟酌，后期投入一定时间，产出比杠杠滴。做事前别急着动手，动脑！\n支撑000044 2016.04.08 09:22 生命周期的管理。\n一个不大不小的project无疑是需要对其生命周期进行科学管理的，甚至是一条plan都需要进行流程化处理，生命周期说长就长，三五年，说短就短，一个决断。科学化是必须要走的路。\n支撑000043 2016.03.31 20:31 收手打的一手好牌。\n打仗的时候不能仅靠果敢的冲锋，打牌的时候不能仅靠三条A，打人的时候不能叫太多人。即使是单挑，力度跟准度也要恰到好处，打到要害并且打不死是最好的，解恨又解救。记住咱毕竟要打一手好牌，收手是一智能feature。\n支撑000041 2016.02.01 23:09 温柔对待世界是我的追求。 表面上很多人认为我不够‘温柔’，我现在对这部分的评价判断为‘大家标准太不一致’，温柔是我的追求，对不同观点求同存异，对不同世界的人包容大方，对相互的质疑保留可有的弹性，我认为的温柔恰恰是一种动态变化，是能力的体现。 你才不够温柔。 受‘包子铺一号客服’对我的劝告，我言语上要减少攻击性。\n支撑000040 2016.03.24 20:33 放大机会成本。 你要知道我还很年轻，一旦很多事按照‘保险’的路数走必死无疑，之所加了‘’就是因为我并不认为这种plan可以做到保险的效果，而恰恰我的plan是冲着保险且理智的方向出发的，放大机会成本是必须要做出的表面的决断，成本适当上升，但是机会却可以指数式攀高。 boy，你跟我说你想要那个plum，但其实你只是渴了一点而已，而且养尊处优的毛病太多，差评并且给我出去！\n支撑000039 寻求突破。\n这是我的一个危机，在既定时间内没有突破就意味着降级重来，而一直存在的大概八年的一个问题就是视野跟思路问题，一直尝试不停学习。 即使是天才儿童，也须经历学习模仿超越，直至形成独立人格。\n支撑000038 投入会看产出。\n即使人本身没有发觉，潜意识也会自动形成一种产出判断式的决定，当然前提是拥有了相对独立的人格，这是一种高级玩法，吃饭问题都解决不好还是建议一步一个脚印。 例子：为何这么多学霸过不了六级？\n支撑000037 我要鱼，你要熊掌。\n起初鱼跟熊掌只是个选择问题，现在已经衍生出了正负相斥的属性，我必须放弃改变对方的磁性，磁铁已经是大自然完美的造物了，我只需找到另一块磁铁。 求同存异。\n支撑000036 看大不信小。\n做产品需要有全局意识，搞活动需要有对应视野，学习也要四两拨千斤，连追女友都要斗智斗勇，聪明才智以及视野并不能轻易获得，但在有能力后一定要坚强的相信自己，因为只有自己是静态对象掌控全类。 看大不信小，哥看的是后台，你靠的是没毕业的直觉。\n支撑000035 支撑000034 支撑000033 Write less,do more. It\u0026rsquo;s almost impossible for me to handle with hundreds of problems one time just with my own strength.In that case,I must take some measures to optimize the system I am using.What I can do is that doing things efficiently with limited sources.\nPractice everyday:\nParticipant:DragonSong Day:33 March 3\nIntrospection:\nI should take all of my actions intensively. I need more patience. I need to calm down. Plans for tomorrow:\nAndroid project getting started Database learning Listening sentences Oral English Practice Pictures capturing 支撑000032 A firm goal with solid action. There will always be a variety of problems in our life,waiting for us to solve.We sometimes shake inside.But we\u0026rsquo;re gonna confront it and beat it.We are leading a great life with a firm goal and solid action.\nPractice everyday:\nParticipant:DragonSong Day:32 March 2\nIntrospection:\nI should take all of my actions intensively. I need more patience. Running will be a good habit of exercise. Plans for tomorrow:\nListening sentences Thinking in Java:ten pages Android project getting started Oral English Practice 支撑000031 In a scientific way. I am supposed to make all of my decisions in a scientific way as firm as possible.At the root of thinking,science is the best one waiting us.Health and art are a kind of science.\nPractice everyday:\nParticipant:DragonSong Day:31 March 1\nIntrospection:\nI should take my steps faster and more efficient. A good lifestyle includes a good sleep (between 11:30 and 7:00)and a good habit of learning.(Java and English) During the day ,I am supposed to focus on Java and Compiler. Eat more and exercise more. Plans for tomorrow:\nListening sentences Brief strokes Thinking in Java:ten pages Oral English Practice: 51talk course and practice with my new partner. New words learning 支撑000029 调节理解力。\n力度一旦失控便会造成诺大的消极影响，而对于理论以及人情的理解力则更加重要，不仅仅控制力度，更要调节相关的方向，模式。\n支撑000028 现实与抽象。\n抽象虽看起来并不可触摸，但抽象也来自现实，例如OOP，设计模式，架构。思维其实只是现实的一个维度世界，互相调用。\n支撑000027 听从内心的声音。\n大部分人最终会是凡人，当然随着教育等因素的推进会有很多顺流而上的人，比如我是这一批，过程中别人的话只能是建议的层级，自己做主是关键。\n支撑000026 快于恐惧。\n人的恐惧有多种来源：已经恐惧时要积极应对，从容出方案规避消极影响；还没恐惧时最好有预判能力或者危机意识，提前干掉并不是没可能，天下武功，唯快不破。\n支撑000025 勇敢尝试。\n一路上之所以会是连续的，其主要的成分还是失败，当连续到成功比较多的时候，说明我进步到了一定阶段。而这中间“胶水”的角色则是：勇敢尝试，无所谓成败的尝试，这是痛，要忍过。\n支撑000024 自己选择。\n从出生每个人已经背负了很多被选择的所谓天生的品质，这些我们已无力改变，能做到放其光芒最好。真正的高权限是，我们有着自己选择的权利，自己选择方向，自己选择好友，自己选择活法。珍惜自己选择，选择。\n支撑000023 越过山丘。\n明知前面有座山需要跨过去，甚至是无尽的山脉，尽管很难我们还是踏上了这条征途，走之前，壮士，请带上你的装备，做好充足的心理准备，做好应有的生存技能训练。越过山丘，需要自知，需要对大世界的渴望，需要武功，不仅仅是勇气。\n感觉这么写药丸\n支撑000022 逻辑入门：顺序与分类。\n以前听过一句：小孩子看对错，成年人看输赢；我的版本是：小孩子走情绪，成年人走逻辑，逻辑这门课太深，理清顺序与分类已然可以解决大多生活与工作问题。\n支撑000021 控制欲望之放大。\n从小受束缚教育的人一直被教育人需要控制自己的欲望，而这个控制专指抑制缩小，比如《人仙之恋》里面的灭人欲，灭情欲。而事实是：很多人的问题并不在于欲望过强失控了，而在于欲望太弱。\n支撑000019 坚守自己。\n我以为我只是读个大学是为了寻找自己，突然意识到，其实这是一出生就带有的使命，几年前一句Jobs的follow your heart一直在琢磨，不就是寻找自己，继而坚守自己吗？物理世界与精神世界本就是一体的，我可以设计修建一套房子，我也可以构建一个App，我也可以写文章写音乐，但溯其根本，heart才是根本，认知是驱动力。你不自知，你不懂。\n支撑000018 物欲需要恰到好处。\n平庸的一部分原因就在于物欲没有得到适当的释放，我向来觉得啥都要学，理科文科一起抓，理性感性都要培养，灭欲不可取，但是控制利用是必要的，物欲。\n支撑000017 规则就是底线。\n我意识到了管理也是科学的，加之以前自己与班主任的讨论中认为规则是必须存在的，一直以来我都是很看重规则的，底线必须遵守，大多数终将没有原则，能力问题，规则就是原则就是底线。\n支撑000016 认真专注产生效率。\n深入学习任何一门课程都需要精神与肉体的高度集中，效率是新产能的突破点，我需要专注。\n支撑000015 行动力与动力，动力与行动力。\n动力是初速度，行动力是加速度，没动力没开端，动力对于此时的我更重要，不相信20岁不堪不敢不干的人40岁会有什么大成就，当然，后发育的情况除外。\n支撑000014 追求理性与感性。\n这俩也都需要不断的深挖，有人说太理性不好，不好意思，这人功底太差，两个都尽可能的上高度，类比：商业决策中的风险与成本控制，理性中的感性，魄力与智慧。\n支撑000013 追求科学。\n科学是最合理的，人文跟社会科学都是，两者必须并存，两个都要学习，两个都要实践，科学的做产品，科学的做人。\n支撑000012 任何时候聪明比努力要更为关键。\n因为聪明是基础，两者都不能缺，但聪明是基础。聪明也许也分为不同副本，存在升级裂变的过程，与努力的无缝搭配也是步步推进的前提，所以，聪明且努力着！反例：可怜之人必有可恨之处；蠢人多作恶。\n支撑000011 人与人的区别甚至跟人与动物的区别是一样的。\n在这个层面上，我们已经可以完全摒弃互相理解的追求了，只需要找到自己的同类，与同类一起创造奇迹与价值。\n支撑000010 掌握度。\n力度，速度，准确度。无论是机械工程还是人文关怀，度的规则都必须遵守，到个人努力的角度，这仨的掌控力需要同时提升，respectively＞＞＞\n支撑000009 辨别信息真假确实是一种能力。\n但放在大众身上，大部分人还远远没到需要辨别真假的程度，一方面接受的信息量太少，一方面信息的受众是信息主体，注定要顺着这个方向走。真真假假。\n支撑000008 经济学原理之降低成本。\n众所周知，我经济学的几门课都挂掉了，但是我学到了一些精髓，例如降低成本的玩法。也是这两天吃饭有感，也是一直以来的传统技巧，但很多人并想不到有更深入的玩法，包子哥有言：视野不够。滴滴这种就是想利用闲置的能源与人力，烧钱的过程顺便培养移动支付的使用习惯，顺便还要干其他事，高度整合，欧。\n支撑000007 只要不犯原则性错误，一个方向深挖总会有收获。\n一件已经开始的事，要么坚持到底，要么趁早放弃，召唤挖宝藏的图。加以高中物理所学做功的原理：功=F*L，即相应方向上的力的产出，所以战略上要尽量避免力的浪费，在正确的方向上用力才会有功，于此，时间会给我们不错的答复。\n支撑000006 2016.02.05 21:38 语言以及肢体表达能力有着非同凡响的爆发力与持久力！ 这话与一个人是否内向，是否有社交恐惧症毫无关系，很多产品经理都不爱瞎聊天，但是表达能力绝对是一流的，逻辑清晰，落落大方，葵花宝典。好多能力欠缺的人是轻视这个能力的，或者说他们轻视一切他们无法获得的能力，也正是由于这个稀缺的属性造就了其可以创造的价值体量。 我现在的问题在于：注意导向的listener，产出定制化的内容以及方式；注意目的性，收收锋芒；注意保持冷静。\n支撑000005 2016.02.04 23:18 哪部分不重要？ 不得不说，哪部分都不能少，哪部分都重要，这正是精髓所在，多的是累赘，少了缺个神，所有的都是精华。在project中自然有轻重缓急，相应的leader就是那个掌舵的人，但，哪块肉都要煮好！\n支撑000004 2016.02.04 23:15 三观初步形成了闭环。 世界观，世界如何构成，地球如何运转；价值观，价值的分类，价值的创造，价值的取舍；人生观，自己如何做事，如何运用价值观做判断，如何运用世界观探索三观。我看人第一看价值观，共处看人生观，能力由其世界观决定。\n支撑000003 2016.02.04 23:11 专注做少也需要能力。 正如能不能与做不做是两回事一样，有能力后才可以有所选择，有能力后才有意识与感知，有能力后行动力才会有质的飞跃。能力是我所求，做少是世界所求。\n支撑000002 2016.02.01 23:09 每个人选择不同，我尽可能去尊重大家，但并不是所有人都值得我去尊重，骨灰级玩家跟骨灰完全是两种现象的存在，我敬重骨灰级玩家，但是骨灰只是踩过去的一种连垫脚石都谈不上的环境变量。 骨灰的价值挖掘出一个：我这么努力自然有一个较宏大的目标或者方向，但你若想感受我的具体点，我会说，我比骨灰强点儿就成了。\n支撑000001 2016/02/01 23:04 有些时候很优秀的人告诉我你要赶紧跑，不然永远追不到前面那块石头； 有些时候比较不优秀的人告诉我你停下来休息会儿吧，不然可能会累死； 我当然知道要怎么跑，快不快慢不慢其实不需要这么多人来指点，这事儿我自己心无旁骛的认真去做就好，听多了反而都是噪音。\n","permalink":"https://redolog.github.io/posts/note/brace-series/","summary":"这个系列是我16年的时候开始写的笔记，主要记录自己的想法。 支撑000224 2025-02-12 00:00 追求更大的增长空间、更深入的业务壁垒、更稳定的关系。 如 支撑00","title":"生-支撑"}]